{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__) # 현재 설치된 PyTorch 버전을 출력합니다. bgf\n",
    "print(torch.cuda.is_available()) # CUDA(병렬 컴퓨팅 플랫폼)가 사용 가능한지 확인하고 결과를 출력합니다.\n",
    "# torch.cuda.is_available() 함수는 PyTorch에서 CUDA(Compute Unified Device Architecture)를 사용할 수 있는지를 확인하는 방법\n",
    "# PyTorch와 같은 딥러닝 프레임워크에서 CUDA를 활용하면 모델 학습 및 추론 속도를 크게 향상가능\n",
    "# True: CUDA가 사용 가능함. 즉, NVIDIA GPU가 설치되어 있고, 적절한 드라이버와 CUDA Toolkit이 설치되어 있습니다.\n",
    "# False: CUDA가 사용 불가능함. 즉, GPU가 없거나 드라이버가 제대로 설치되지 않았습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tensor\n",
    "\n",
    "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
    "\n",
    "### Tensor 만드는 법\n",
    "\n",
    "\n",
    "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등임.\n",
    "\n",
    "주요 속성들\n",
    "- dtype: 데이터 타입\n",
    "- device: gpu에 있는지, cpu에 있는지\n",
    "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-D Tensor\n",
    "scalar = torch.tensor(5.0) # 0차원tensor(scalar)생성\n",
    "number = torch.tensor(1.0)\n",
    "print(scalar)  # tensor(5.)\n",
    "print(number) # tensor(1.)\n",
    "\n",
    "vector = torch.tensor([1.0, 2.0, 3.0]) # 1차원tensor(vector)생성\n",
    "tuple_vector = torch.tensor((1, 2, 3))\n",
    "\n",
    "print(vector)  # tensor([1., 2., 3.])\n",
    "print(tuple_vector) # tensor([1, 2, 3,])\n",
    "\n",
    "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "matrix2 = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
    "print(matrix.shape)\n",
    "print(matrix2.shape)\n",
    "# tensor([[1., 2.],\n",
    "#         [3., 4.]])\n",
    "\n",
    "matrix2 = [[1, 2, 3], [3, 4, 5]]\n",
    "lst = [matrix2, matrix2, matrix2, matrix2]\n",
    "\n",
    "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
    "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0]), ....\n",
    "print(tensor_3d)\n",
    "print(tensor_3d_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.tensor의 주요 속성들\n",
    "\n",
    "- tensor.shape\n",
    "- tensor.size()\n",
    "- tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.shape)    # torch.Size([3])\n",
    "print(matrix.size())   # torch.Size([2, 2])\n",
    "print(tensor_3d.shape) # torch.Size([2, 2, 1])\n",
    "print(tensor_3d.dtype) # torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector.dtype)    # torch.float32\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(int_tensor.dtype)  # torch.int32  # in32 : 32자리 이진수공간 할당?\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
    "print(tensor_gpu.device)  # cuda:0 or cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.tensor 만드는 방법\n",
    "\n",
    "- torch.tensor(data)\n",
    "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
    "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
    "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
    "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴.\n",
    "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬.\n",
    "- 이외에도 많이 쓰이는 함수들\n",
    "  * torch.arange: range() 함수와 매우 비슷하다.\n",
    "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tensor = torch.tensor([1, 2, 3])\n",
    "tuple_tensor = torch.tensor((4, 5, 6))\n",
    "\n",
    "zeros = torch.zeros((2,3))\n",
    "zeros1 = torch.zeros(2,3)\n",
    "zeros2 = torch.zeros(((0, 0, 0)))\n",
    "zeros3 = torch.zeros((([0, 0, 0])))\n",
    "zeros4 = torch.zeros((1, 2))\n",
    "ones = torch.ones((2, 3))\n",
    "rand = torch.rand((2, 3))\n",
    "eye = torch.eye(3)  # 3x3 Identity matrix\n",
    "\n",
    "print(zeros.shape)\n",
    "print(zeros1.shape)\n",
    "print(zeros2.shape)\n",
    "print(zeros3.shape)\n",
    "print(zeros4.shape)\n",
    "\n",
    "print(zeros)\n",
    "print(ones)\n",
    "print(ones)\n",
    "print(rand)\n",
    "print(eye)\n",
    "\n",
    "normal = torch.randn((2, 3))  # Normal distribution\n",
    "\n",
    "arange_tensor = torch.arange(start=0, end=10, step=2) # range(0, 10, 2)\n",
    "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
    "print(arange_tensor)\n",
    "print(linspace_tensor)\n",
    "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor((((2,3))))) # 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=((((((((((2,3)))))))))) # 실험\n",
    "\n",
    "while not isinstance(a[0],int):\n",
    "    assert isinstance(a, (tuple,list)), \"not a list or tuple\"\n",
    "    a = a[0]\n",
    "print(a[0],a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here # 미완성\n",
    "class pytorch():\n",
    "    def nested_list(shape, value=0):\n",
    "        \n",
    "        while not isinstance(a[0],int):\n",
    "            assert isinstance(a, (tuple,list)), \"not a list or tuple\"\n",
    "            a = a[0]\n",
    "        return \n",
    "    def random_nested_list():\n",
    "        return \n",
    "    def torch_zeros(self, *arg):\n",
    "        return torch.tensor()\n",
    "    def torch_ones(self,):\n",
    "        return torch.tensor()\n",
    "    def torch_rand(self,):\n",
    "        return torch.tensor()\n",
    "    def torch_arange(self, end=None, start=None, step=1):\n",
    "        return torch.tensor()\n",
    "    def torch_lnspace(self, end=None, start=None, step=1):\n",
    "        return torch.tensor()\n",
    "    @staticmethod\n",
    "    def torch_eye(x):\n",
    "        return torch.tensor([[0 if i != j else 1 for j in range(x)] for i in range(x)])\n",
    "    def __str__():\n",
    "        return torch.tensor()\n",
    "\n",
    "py = pytorch()\n",
    "# py_zeros = py.torch_zeros((2,3))\n",
    "# print(*py_zeros, sep='\\n')\n",
    "# py_ones = py.torch_ones((2,3))\n",
    "# print(*py_ones, sep='\\n')\n",
    "# py_rand = py.torch_rand((2,3))\n",
    "# print(py_rand, sep='\\n')\n",
    "py_eye = py.torch_eye(10)\n",
    "print(py_eye)\n",
    "# py_arange = py.torch_arange(start=0 ,end=10, step=2)\n",
    "# print(*py_arange, sep='\\n')\n",
    "# py_lnspace = py.torch_lnspace(0, 1, 5)\n",
    "# print(*py_lnspace, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "import random\n",
    "\n",
    "def nested_list(shape, value = 0):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [value for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
    "\n",
    "def random_nested_list(shape, sample_from, *args):\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [sample_from(*args) for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [random_nested_list(shape[1:], sample_from, *args) for _ in range(l)]\n",
    "'''\n",
    "def random_nested_list(shape):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [random.random() for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
    "'''\n",
    "def randomn_nested_list(shape):\n",
    "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
    "    \"\"\"\n",
    "    if len(shape) == 1:\n",
    "        l = shape[0]\n",
    "        return [random.gauss(0, 1) for _ in range(l)]\n",
    "    else:\n",
    "        l = shape[0]\n",
    "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
    "\n",
    "def zeros(shape):\n",
    "    return torch.tensor(nested_list(shape, value = 0))\n",
    "\n",
    "def ones(shape):\n",
    "    return torch.tensor(nested_list(shape, value = 1))\n",
    "\n",
    "def rand(shape):\n",
    "    return torch.tensor(random_nested_list(shape, random.random))\n",
    "\n",
    "def randn(shape):\n",
    "    return torch.tensor(random_nested_list(shape, random.gauss, 0, 1))\n",
    "def eyes(n):\n",
    "    return torch.tensor([[0 if i != j else 1 for j in range(n)] for i in range(n)])\n",
    "\n",
    "print(nested_list((2, 3, 4), value = 0))\n",
    "print(zeros((2,3,4)) == torch.zeros((2,3,4)))\n",
    "print(ones((2,3,4)))\n",
    "print(rand((2,3,4)))\n",
    "print(randn((2,3,4)))\n",
    "# print(eyes(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.tensor끼리의 연산\n",
    "\n",
    "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨.\n",
    "엘레멘트 와이드?로 행렬의 차원이 같으면 원소간 연산이 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
    "'''\n",
    "1 2  5 6\n",
    "3 4  7 8\n",
    "\n",
    "1 3\n",
    "2 4\n",
    "'''\n",
    "'''\n",
    "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
    "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
    "'''\n",
    "\n",
    "matmul = torch.matmul(matrix_a, matrix_b)\n",
    "\n",
    "# tensor([[19, 22],\n",
    "#         [43, 50]])\n",
    "\n",
    "elem_mul = matrix_a * matrix_b\n",
    "# tensor([[ 5, 12],\n",
    "#         [21, 32]])\n",
    "\n",
    "transposed = torch.transpose(matrix_a, 0, 1)\n",
    "# 주로 딥러닝이나 배열 연산에서 사용되며, 다차원 배열 또는 텐서의 축을 바꿔주는 역할\n",
    "# tensor([[1, 3],\n",
    "#         [2, 4]])\n",
    "\n",
    "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "add = a + b  # tensor([5., 7., 9.])\n",
    "sub = a - b  # tensor([-3., -3., -3.])\n",
    "\n",
    "mul = a * b  # tensor([ 4., 10., 18.])\n",
    "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
    "\n",
    "exp = a ** 2  # tensor([1., 4., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
    "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
    "'''\n",
    "1 2  5 6\n",
    "3 4  7 8\n",
    "\n",
    "1 3\n",
    "2 4\n",
    "'''\n",
    "'''\n",
    "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
    "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
    "'''\n",
    "\n",
    "matmul = torch.matmul(matrix_a, matrix_b)\n",
    "\n",
    "# tensor([[19, 22],\n",
    "#         [43, 50]])\n",
    "\n",
    "elem_mul = matrix_a * matrix_b\n",
    "# tensor([[ 5, 12],\n",
    "#         [21, 32]])\n",
    "\n",
    "transposed = torch.transpose(matrix_a, 0, 1)\n",
    "# 주로 딥러닝이나 배열 연산에서 사용되며, 다차원 배열 또는 텐서의 축을 바꿔주는 역할\n",
    "# tensor([[1, 3],\n",
    "#         [2, 4]])\n",
    "\n",
    "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
    "\n",
    "브로드캐스팅 규칙:\n",
    "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
    "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
    "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
    "\n",
    "예를 들어서,\n",
    "\n",
    "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape (1, 3) / (3, 1)\n",
    "# dim = 0\n",
    "# 3 / 3\n",
    "# dim = 1\n",
    "# 3 / 3\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])    # Shape: (2, 3,)\n",
    "b = torch.tensor([1, 2, 3])                 # Shape: (3,)\n",
    "b = torch.tensor([[1, 2, 3]])               # Shape: (1, 3,)\n",
    "b = torch.tensor([[1, 2, 3], [1, 2, 3]])    # Shape: (2, 3,)\n",
    "\n",
    "broadcast_add = a + b  # Shape: (2, 3)\n",
    "# tensor([[2, 4, 6],\n",
    "#         [5, 7, 9]])\n",
    "\n",
    "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
    "b = torch.tensor([4, 5, 6])        # Shape: (3,)\n",
    "b = torch.tensor([[4, 5, 6]])        # Shape: (1, 3,)\n",
    "b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])        # Shape: (3, 3,)\n",
    "a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])  # Shape: (3, 3)\n",
    "a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\n",
    "# To make shapes compatible:\n",
    "# a: (3, 1) -> (3, 3)\n",
    "# b: (3,)   -> (1, 3) -> (3, 3)\n",
    "\n",
    "broadcast_mul = a * b  # Shape: (3, 3)\n",
    "# tensor([[ 4,  5,  6],\n",
    "#         [ 8, 10, 12],\n",
    "#         [12, 15, 18]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 외 tensor operation들\n",
    "\n",
    "- Slicing / Indexing\n",
    "- Reshaping\n",
    "- Concatenation / Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing / indexing\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Basic indexing\n",
    "element = tensor[1, 2]  # tensor(6)\n",
    "\n",
    "# Slicing\n",
    "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
    "                            #         [5, 6],\n",
    "                            #         [8, 9]])\n",
    "\n",
    "# Advanced indexing with masks\n",
    "mask = tensor > 5\n",
    "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping\n",
    "\n",
    "tensor = torch.arange(0, 12)\n",
    "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
    "                                   #         [ 4,  5,  6,  7],\n",
    "                                   #         [ 8,  9, 10, 11]])\n",
    "\n",
    "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
    "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
    "\n",
    "# tensor.permute\n",
    "tensor = torch.randn(2, 3, 4)\n",
    "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
    "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Concatenate along existing dimension\n",
    "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Stack along a new dimension\n",
    "stack = torch.stack((a, b), dim=0)\n",
    "# tensor([[1, 2, 3],\n",
    "#         [4, 5, 6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수학적 함수들\n",
    "\n",
    "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
    "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
    "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([-1.0, -2.0, 3.0])\n",
    "\n",
    "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
    "# sqrt_a = torch.sqrt(a)\n",
    "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
    "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
    "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
    "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2,3)\n",
    "\n",
    "sum_all = torch.sum(tensor)          # tensor(10)\n",
    "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6, 8]) (3,)\n",
    "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([6, 12]) (2,)\n",
    "\n",
    "mean_all = torch.mean(tensor.float(), dim = 1)  # tensor(2.5000)\n",
    "print(mean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([2, 2, 2])\n",
    "\n",
    "greater = a > b  # tensor([False, False, True])\n",
    "equal = a == b   # tensor([False, True, False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch로 다시 해 보는 선형회귀\n",
    "\n",
    "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
    "\n",
    "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
    "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
    "\n",
    "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의로 데이터를 한번 만들어 보자.\n",
    "# True parameters\n",
    "true_w = 2.0  # 실제 가중치\n",
    "true_b = 1.0  # 실제 바이어스\n",
    "\n",
    "# Generate data\n",
    "X = torch.randn(100, 1) * 10  # 100 samples, single feature # 평균이 0이고 표준편차가 10인 정규 분포에서 샘플링\n",
    "y = true_w * X + true_b + torch.randn(100, 1) * 0.2  # 노이즈를 추가하여 y 값을 생성\n",
    "# w,b 2개의 파라미터 텐서 2개 생성\n",
    "\n",
    "# y값 생성 과정\n",
    "# y[0]= true_w * X[0] + true_b + torch.randn(1, 1) * 2\n",
    "# y[1]= true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
    "# ...\n",
    "# y[99]= true_w * X[99] + true_b + torch.randn(1, 1) * 2\n",
    "\n",
    "# requires_grad = True로 해야 학습이 가능\n",
    "w = torch.randn(1, 1, requires_grad=True) #  requires_grad=True, 가중치 w 무작위로 초기화, 기울기 계산할 수 있도록 설정\n",
    "b = torch.randn(1, requires_grad=True) # 바이어스 b 무작위로 초기화, 기울기 계산할 수 있도록 설정\n",
    "\n",
    "learning_rate = 0.009 # 보폭: 학습률 설정 # 하이퍼파라미터\n",
    "epochs = 5000 # 총 학습 에폭 수 설정\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: compute predicted y (예측된 y계산)\n",
    "    # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
    "    y_pred = X * w + b # y_pred: 100, 1\n",
    "\n",
    "    # Compute and print loss # 손실계산\n",
    "    loss = torch.mean((y_pred - y) ** 2) # 평균 제곱 오차(MSE) 손실을 계산 # 100, 1\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward() # 손실에 대한 기울기 계산\n",
    "\n",
    "    # Update parameters using gradient descent # 경량 경량화\n",
    "    with torch.no_grad(): # 기울기 업데이트 시 기울기를 추적하지 않도록 설정 기울기\n",
    "        w -= learning_rate * w.grad # w 업데이트\n",
    "        b -= learning_rate * b.grad # b 업데이트\n",
    "\n",
    "    # Zero gradients after updating # 업데이트 후  w,b의 그레디언트 기울기를 0으로 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # 100에폭마다 현재 파라미터와 손실을 출력.\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
    "        \n",
    "    # 로컬미니마이즈: 손실함수의 특정 지점, 주변보다 손실값이 낮지만, 전체 함수에서 최저점(글로벌미니마이즈)이 아닌 곳\n",
    "    #   그래디언트가 로컬미니마이즈에 빠지면 더이상 파라미터를 업데이트 하지못해 최적화가 안되는 구조\n",
    "    #   특히 비선형함수에서 발생 가능\n",
    "    #   해결법:\n",
    "    #       모멘텀(Momentum): 이전 기울기를 반영, 업데이트 방향을 조정함으로써 로컬 미니마를 피하도록 유도\n",
    "    #       학습률 조정: 학습률을 조금씩 감소시키거나, 동적으로 조정하는 기법으로 최적화 성능 개선가능\n",
    "    #       다양한 초기화: 파라미터를 여러 번 다른 초기값으로 초기화 하여 최적화 과정을 반복함으로 로컬미니마에 빠질 가능성을 감소\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 조금 더 해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import random\n",
    "\n",
    "# 선형방정식 기반 데이터 생성\n",
    "def generate_artificial_data(true_w, true_b, n_data, x_amplitude = 10, noise_amplitude = 0.2):\n",
    "    \"\"\"Generate data from y = true_w * x + true_b\"\"\"\n",
    "    X = torch.randn(n_data, 1) * x_amplitude\n",
    "    y = true_w * X + true_b + torch.randn(n_data, 1) * noise_amplitude # 노이즈를 주는것이 꽤 중요하고 유의미\n",
    "    return X, y\n",
    "\n",
    "def linear_regression(data, learning_rate, epochs, quiet = False): # 선형회귀 모델 학습\n",
    "    X, y = data\n",
    "    w = torch.randn(1, 1, requires_grad=True) # 가중치와 bias초기화\n",
    "    b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Forward pass: compute predicted y\n",
    "        # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
    "        y_pred = X * w + b # y_pred: 100, 1\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = torch.mean((y_pred - y) ** 2) # 100, 1\n",
    "\n",
    "        if epoch % 100 == 0 and not quiet:\n",
    "            print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward() # 오토 그래디언트 오토그라드?\n",
    "\n",
    "        # Update parameters using gradient descent 경사하강법 업데이트\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "            b -= learning_rate * b.grad\n",
    "\n",
    "        # Zero gradients after updating\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_() # gpu가 구려서 한번에 계산할 사이즈가 작으면 두번계산한걸 합치기도 한다?\n",
    "    return w, b\n",
    "\n",
    "data = generate_artificial_data(2.0, 1.0, 1000)\n",
    "linear_regression(data, 0.001, 1000)\n",
    "# 모델 평가\n",
    "def evaluate_model(true_w, true_b, w, b):\n",
    "    return (true_w - w) ** 2 / true_w ** 2 + (true_b - b) ** 2 / true_b ** 2\n",
    "# 학습률에 따른 모델 성능 평가\n",
    "def find_learning_rate():\n",
    "    true_w = random.gauss(0, 10) # 랜덤가중치\n",
    "    true_b = random.gauss(0, 10) # 랜덤bias\n",
    "\n",
    "    data = generate_artificial_data(true_w, true_b, 1000) # data생성\n",
    "    # 학습률에 대해 모델 학습 및 평가\n",
    "    for learning_rate in [0.0001 * (i+1) for i in range(10)]:\n",
    "        w, b = linear_regression(data, learning_rate, 1000, quiet = True) # 출력없이 조용히 학습\n",
    "        score = evaluate_model(true_w, true_b, w, b) # 모델 성능 평가\n",
    "        print(score, learning_rate)\n",
    "\n",
    "find_learning_rate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
    "\n",
    "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n",
    "import random\n",
    "\n",
    "# 3개의 입력을 사용하는 데이터 생성 함수\n",
    "def generate_artificial_3data(true_w, true_b, n_data, x_amplitude=10, noise_amplitude=0.2):\n",
    "    \"\"\"Generate data from y = w1*x1 + w2*x2 + w3*x3 + b 형태의 데이터를 생성\"\"\"\n",
    "    X = torch.randn(n_data, 3) * x_amplitude  # x1, x2, x3 3개의 입력 데이터 생성\n",
    "    y = (true_w[0]*X[0] + true_w[1]*X[1] + true_w[2]*X[2] + true_b + torch.randn(n_data,1) * noise_amplitude)\n",
    "    return X, y\n",
    "\n",
    "# 입력이 3개\n",
    "def linear_regression_3(data, learning_rate, epochs, quiet=False):\n",
    "    X, y = data\n",
    "    w = torch.randn(3, 1, requires_grad=True)  # 3개의 가중치 w1, w2, w3\n",
    "    b = torch.randn(1, requires_grad=True) # bias는 1개\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 예측 값 계산\n",
    "        y_pred = X @ w + b\n",
    "        # 손실(Mean Squared Error) 계산\n",
    "        loss = torch.mean((y_pred - y) ** 2)\n",
    "        if epoch % 100 == 0 and not quiet:\n",
    "            print(f'Epoch {epoch}: w = {[w[i].item() for i in range(3)]}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
    "        # 역전파로 그래디언트 계산\n",
    "        loss.backward()\n",
    "        # 경사 하강법으로 w,b 업데이트\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "            b -= learning_rate * b.grad\n",
    "        # 그래디언트 초기화\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(true_w, true_b, w, b):\n",
    "    w_difference = sum([(true_w[i] - w[i])**2 / true_w[i]**2 for i in range(3)])\n",
    "    b_difference = (true_b - b)**2 / true_b**2\n",
    "    return w_difference + b_difference\n",
    "\n",
    "# 실제 weight와 bias를 랜덤생성 : 다양한 학습률에 따른 모델 성능 평가\n",
    "def find_learning_rate(): # weight bias 랜덤설정\n",
    "    true_w = [random.gauss(0, 10) for _ in range(3)]\n",
    "    true_b = random.gauss(0, 10)\n",
    "    data = generate_artificial_3data(true_w, true_b, 1000)  # data 생성\n",
    "\n",
    "    # 여러 학습률에 대해 모델 학습 및 평가\n",
    "    for learning_rate in [0.0001 * (i+1) for i in range(10)]:\n",
    "        w, b = linear_regression_3(data, learning_rate, 1000, quiet=True)  # 출력없이 조용히 학습\n",
    "        score = evaluate_model(true_w, true_b, w, b)  # 모델 성능 평가\n",
    "        print(f'학습률: {learning_rate:.5f}, 평가 점수: {score.item()}')\n",
    "\n",
    "# 하이퍼파라미터 탐색 실행\n",
    "find_learning_rate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뒤에서 할 내용 미리 살짝 엿보기 - Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 adam optimizer를 한번 사용해보자.\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 임의로 데이터를 한번 만들어 보자.\n",
    "# True parameters\n",
    "true_w = 2.0\n",
    "true_b = 1.0\n",
    "\n",
    "# Generate data\n",
    "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
    "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
    "\n",
    "# requires_grad = True로 해야 학습이 가능\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 아담 옵티마이저 하이퍼파라미터 설정\n",
    "learning_rate = 0.005\n",
    "epochs = 10000\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
    "m_w = torch.zeros_like(w)\n",
    "v_w = torch.zeros_like(w)\n",
    "m_b = torch.zeros_like(b)\n",
    "v_b = torch.zeros_like(b)\n",
    "\n",
    "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
    "t = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = X * w + b\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 아담 옵티마이저 업데이트\n",
    "    with torch.no_grad():\n",
    "        t += 1  # 시간 스텝 증가\n",
    "\n",
    "        # w 파라미터 업데이트\n",
    "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
    "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
    "        # 편향 보정\n",
    "        m_w_hat = m_w / (1 - beta1 ** t)\n",
    "        v_w_hat = v_w / (1 - beta2 ** t)\n",
    "        # 파라미터 업데이트\n",
    "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
    "\n",
    "        # b 파라미터 업데이트\n",
    "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
    "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
    "        # 편향 보정\n",
    "        m_b_hat = m_b / (1 - beta1 ** t)\n",
    "        v_b_hat = v_b / (1 - beta2 ** t)\n",
    "        # 파라미터 업데이트\n",
    "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "    # Gradients 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자꾸 local minima 어딘가에 빠지는 것 같다. 이걸 수정하기 위해서, 일정 횟수 이상 바뀌지 않으면 noise를 주는 방식을 생각해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500: w = 1.8663, b = 1.0852, loss = 4.9355\n",
      "Epoch 1000: w = 2.0011, b = 1.0771, loss = 3.1216\n",
      "  Epoch 1143: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 1500: w = 2.0029, b = 1.0675, loss = 3.1215\n",
      "  Epoch 1719: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 2000: w = 2.0023, b = 1.0716, loss = 3.1214\n",
      "  Epoch 2132: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 2500: w = 2.0024, b = 1.0714, loss = 3.1214\n",
      "  Epoch 2589: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 3000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 3040: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "  Epoch 3459: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 3500: w = 2.0024, b = 1.0742, loss = 3.1215\n",
      "  Epoch 3811: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 4000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 4227: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 4500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 4635: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 5000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 5044: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "  Epoch 5467: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 5500: w = 2.0052, b = 1.0815, loss = 3.1223\n",
      "  Epoch 5830: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 6000: w = 2.0024, b = 1.0714, loss = 3.1214\n",
      "  Epoch 6236: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 6500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 6627: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 7000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 7034: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "  Epoch 7453: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 7500: w = 1.9919, b = 1.0830, loss = 3.1343\n",
      "  Epoch 7885: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 8000: w = 2.0025, b = 1.0722, loss = 3.1214\n",
      "  Epoch 8295: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 8500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 8687: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 9000: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 9103: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 9500: w = 2.0024, b = 1.0713, loss = 3.1214\n",
      "  Epoch 9530: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "  Epoch 9914: No significant updates in the last 300 epochs. Adding noise to parameters.\n",
      "Epoch 10000: w = 2.0019, b = 1.0707, loss = 3.1215\n",
      "\n",
      "Final Parameters: w = 2.0019, b = 1.0707, loss = 3.1215\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 임의로 데이터를 한번 만들어 보자.\n",
    "# True parameters\n",
    "true_w = 2.0\n",
    "true_b = 1.0\n",
    "\n",
    "# Generate data\n",
    "torch.manual_seed(42)  # 재현성을 위해 시드 설정\n",
    "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
    "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
    "\n",
    "# requires_grad = True로 해야 학습이 가능\n",
    "w = torch.randn(1, 1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 아담 옵티마이저 하이퍼파라미터 설정\n",
    "learning_rate = 0.005\n",
    "epochs = 10000\n",
    "\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "\n",
    "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
    "m_w = torch.zeros_like(w)\n",
    "v_w = torch.zeros_like(w)\n",
    "m_b = torch.zeros_like(b)\n",
    "v_b = torch.zeros_like(b)\n",
    "\n",
    "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
    "t = 0\n",
    "\n",
    "\n",
    "patience = 300  # 손실과 파라미터 변화가 임계값 이하로 유지되는 에포크 수\n",
    "threshold_loss = 1e-4  # 손실 변화 임계값\n",
    "threshold_w = 1e-4     # w 변화 임계값\n",
    "threshold_b = 1e-4     # b 변화 임계값\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = X * w + b\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # 아담 옵티마이저 업데이트\n",
    "    with torch.no_grad():\n",
    "        t += 1  # 시간 스텝 증가\n",
    "\n",
    "        # w 파라미터 업데이트\n",
    "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
    "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
    "        # 편향 보정\n",
    "        m_w_hat = m_w / (1 - beta1 ** t)\n",
    "        v_w_hat = v_w / (1 - beta2 ** t)\n",
    "        # 파라미터 업데이트\n",
    "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
    "\n",
    "        # b 파라미터 업데이트\n",
    "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
    "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
    "        # 편향 보정\n",
    "        m_b_hat = m_b / (1 - beta1 ** t)\n",
    "        v_b_hat = v_b / (1 - beta2 ** t)\n",
    "        # 파라미터 업데이트\n",
    "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "    # Gradients 초기화\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # 손실과 파라미터 값을 기록\n",
    "    loss_history.append(loss.item())\n",
    "    w_history.append(w.item())\n",
    "    b_history.append(b.item())\n",
    "\n",
    "    # Patience에 도달했는지 확인\n",
    "    if epoch >= patience :\n",
    "        # 최근 'patience' 에포크의 손실 변화 계산\n",
    "        recent_losses = loss_history[-patience:]\n",
    "        loss_deltas = [abs(recent_losses[i] - recent_losses[i-1]) for i in range(1, patience)]\n",
    "        max_loss_delta = max(loss_deltas)\n",
    "\n",
    "        # 최근 'patience' 에포크의 w 변화 계산\n",
    "        recent_ws = w_history[-patience:]\n",
    "        w_deltas = [abs(recent_ws[i] - recent_ws[i-1]) for i in range(1, patience)]\n",
    "        max_w_delta = max(w_deltas)\n",
    "\n",
    "        # 최근 'patience' 에포크의 b 변화 계산\n",
    "        recent_bs = b_history[-patience:]\n",
    "        b_deltas = [abs(recent_bs[i] - recent_bs[i-1]) for i in range(1, patience)]\n",
    "        max_b_delta = max(b_deltas)\n",
    "\n",
    "        # 변화가 모두 임계값 이하인 경우 노이즈 추가\n",
    "        if (max_loss_delta < threshold_loss) and (max_w_delta < threshold_w) and (max_b_delta < threshold_b):\n",
    "            print(f'  Epoch {epoch}: No significant updates in the last {patience} epochs. Adding noise to parameters.')\n",
    "            # 파라미터에 노이즈 추가\n",
    "            noise_w = torch.randn_like(w) * 0.1\n",
    "            noise_b = torch.randn_like(b) * 0.1\n",
    "            w.data += noise_w\n",
    "            b.data += noise_b\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
    "\n",
    "# 최종 파라미터 출력\n",
    "print(f'\\nFinal Parameters: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 들어가기\n",
    "\n",
    "\n",
    "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 파이토치(Torch) 라이브러리: 딥러닝 및 텐서 연산을 위한 프레임워크\n",
    "import torch.nn as nn  # 신경망 모듈(nn.Module)을 정의하고 사용하는 모듈\n",
    "import torch.optim as optim  # 옵티마이저(Optimizer) 모듈: 모델의 가중치를 업데이트하는 방법을 제공\n",
    "from torch.utils.data import DataLoader, TensorDataset  # DataLoader: 데이터를 배치로 묶어주는 유틸리티, TensorDataset: 텐서 데이터를 다루기 쉽게 하는 클래스\n",
    "import torch.nn.functional as F  # 활성화 함수(ReLU, Softmax 등) 및 손실 함수 등 다양한 함수들 제공\n",
    "import numpy as np  # 넘파이(Numpy): 수치 연산을 위한 라이브러리, 텐서 데이터 관리 및 수학적 연산에 사용\n",
    "\n",
    "# Seed for reproducibility # 결과 재현성을 위해 시드 설정\n",
    "torch.manual_seed(0)  # 파이토치에서 시드(seed)를 설정하여 같은 결과가 나오도록 함 (랜덤 연산 고정)\n",
    "# 파이토치에서 무작위 연산(랜덤 값 생성, 가중치 초기화 등)의 결과를 고정하여 같은 결과가 나오도록 설정하는 함수. 이를 통해 실험을 재현\n",
    "np.random.seed(0)  # 넘파이에서 시드(seed)를 설정하여 같은 결과가 나오도록 함 (랜덤 연산 고정)\n",
    "# 넘파이에서 무작위 연산의 결과를 고정하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래대로라면 실제 데이터를 쓰는 것이 좋지만, 실험의 편리함을 위해 인공적으로 만든 합성 데이터를 사용하자.\n",
    "\n",
    "입력 feature는 20개다. 1시간 후에 비가 올지 안 올지 알고 싶은데, 현재 가지고 있는 데이터가 서울 각지의 습도 데이터라고 하자. 20곳에서 동시에 각각 습도를 잰 것이다. 이 데이터를 가지고 비가 오면 1, 비가 오지 않으면 0이라고 하자. 실제 데이터를 안 가지고 있기 때문에, 적당히 수식을 써서 비가 오는 경우와 안 오는 경우를 임의로 구분하여 합성 데이터를 만들자. 이러한 데이터 포인트 1000개를 만들자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "input_features = 20\n",
    "\n",
    "# Features: random numbers\n",
    "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
    "\n",
    "# Labels: sum of features > 0 => class 1, else class 0\n",
    "# Please convince yourself this is the real data, while actuall not the case\n",
    "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X)\n",
    "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0 + x_1 + ... + x_{19} >0 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
    "\n",
    "비가 오는지 오지 않는지 어떻게 추정할 수 있을까? 위 수식을 계산하여 결정한다. 비가 오면 값이 커지고, 비가 오지 않으면 값이 작아지도록 한다. 0을 기준으로 양수이면 비가 오는 것으로 판단, 음수면 비가 오지 않는 것으로 판단한다.\n",
    "\n",
    "`X`는 주어진 데이터이니, 우리는 원하는 값을 계산할 수 있도록 해 주는 `w`와 `b`의 값을 구하면 된다.\n",
    "\n",
    "일단 랜덤으로 값을 초기화해 놓고, 이후 경사하강법으로 좋은 값을 찾아간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(input_features, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본격적으로 답을 찾기 전에, 랜덤으로 만든 값들이 얼마나 유용한지 테스트해보자.\n",
    "\n",
    "혹시라도 랜덤으로 만들었는데 이미 잘 맞춘다면 우리가 이 고생을 할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_total = 0\n",
    "cnt_correct = 0\n",
    "for x_i, y_i in zip(X_tensor, y_tensor):\n",
    "    with torch.no_grad():\n",
    "        score = torch.sum(x_i*w) + b\n",
    "    predict_flag = score > 0\n",
    "    answer_flag = y_i == 1\n",
    "\n",
    "    if predict_flag == answer_flag:\n",
    "        cnt_correct += 1\n",
    "    cnt_total += 1\n",
    "\n",
    "print(f'Accuracy: {cnt_correct/cnt_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 놀랍게도 실제 뉴럴네트워크의 학습 과정이다.\n",
    "```\n",
    "면접관: 당신의 장점은?\n",
    "나: 저는 머신러닝 전문가입니다.\n",
    "면접관: 9+10은?\n",
    "나: 3 입니다.\n",
    "면접관: 틀렸네. 전혀 달라. 답은 19일세.\n",
    "나: 16 입니다.\n",
    "면접관: 틀렸네. 답은 19일세.\n",
    "나: 18 입니다.\n",
    "면접관: 틀렸네. 답은 19일세.\n",
    "나: 19 입니다.\n",
    "면접관: 자넨 합격일세.\n",
    "```\n",
    "\n",
    "다만 아래와 같이 좀 더 정확하게 고칠 수 있다.\n",
    "\n",
    "```\n",
    "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
    "신경망: 수식을 계산한 결과 8이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
    "훈련교관: 답은 비가 오지 않는다네.\n",
    "신경망: 명심하겠습니다.\n",
    "\n",
    "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
    "신경망: 수식을 계산한 결과 3이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
    "훈련교관: 답은 비가 온다네.\n",
    "신경망: 명심하겠습니다.\n",
    "\n",
    "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가? (1. 학습 데이터 샘플링)\n",
    "신경망: 수식을 계산한 결과 -10이 나왔습니다. 0보다 작으니 비가 안 올 것입니다. (2. 추론)\n",
    "훈련교관: 답은 비가 오지 않는다네.\n",
    "신경망: 명심하겠습니다. (3. 손실함수에서 역전파된 그래디언트를 바탕으로 파라미터 조정)\n",
    "```\n",
    "위에서 이미 1단계와 2단계를 끝냈다.\n",
    "이제 위 코드에 약간 추가해 3단계를 수행하자.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수를 계산하기 이전에 손실함수에 대해 짚고 넘어가는 것이 좋겠다. `torch.nn.BCEWithLogitsLoss()`가 우리가 사용할 손실함수다. 이 함수의 수학적 구조에 대해 논하는 것은 이론 시간의 역할이고, 실습 시간에는 이 손실함수가 우리가 원하는 성질을 만족시키는지만 간단히 체크하고 넘어가자.\n",
    "\n",
    "손실함수는 말 그대로의 의미를 지닌다. 손실이다. 크면 안 된다. 우리는 손실을 줄여야 한다. 좀 더 정확히는, 절대적인 크기보다는 상대적인 크기가 중요하다.\n",
    "\n",
    "신경망은 학습을 통해 나쁜 상태에서 좋은 상태로 나아간다. 나쁜 상태는 비가 오는지 오지 않는지 잘 예측하지 못하는 상태이고, 좋은 상태는 그 반대다. 현재 신경망이 처한 상태가 얼마나 좋은지, 또는 얼마나 나쁜지 알려주는 가이드가 손실함수다.\n",
    "\n",
    "우리에게 $s_1$과 $s_2$라는 두 가지의 상태가 있고, $s_1$이 더 바람직한 상태라고 하자. 손실함수를 $f$라고 하면, 다음을 만족해야 한다.\n",
    "\n",
    "$$f(s_1) < f(s_2)$$\n",
    "\n",
    "(상태란 단적으로 말하면 `w`와 `b`의 값을 말한다.)\n",
    "\n",
    "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
    "\n",
    "A. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 온다면 현재 신경망은 좋은 상태에 있는 것이다.\n",
    "\n",
    "B. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 오지 않는다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
    "\n",
    "C. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 온다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
    "\n",
    "D. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 오지 않는다면 현재 신경망은 좋은 상태에 있는 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 비가 오면 1, 안 오면 0\n",
    "\n",
    "# A\n",
    "a = criterion(torch.tensor([10.]), torch.tensor([1.]))\n",
    "print(a)\n",
    "\n",
    "# B\n",
    "b = criterion(torch.tensor([10.]), torch.tensor([0.]))\n",
    "print(b)\n",
    "\n",
    "# C\n",
    "c = criterion(torch.tensor([-6.]), torch.tensor([1.]))\n",
    "print(c)\n",
    "\n",
    "# D\n",
    "d = criterion(torch.tensor([-6.]), torch.tensor([0.]))\n",
    "print(d)\n",
    "\n",
    "assert a < b\n",
    "assert a < c\n",
    "assert d < b\n",
    "assert d < c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수가 원하는 성질을 가진다는 것을 확인했으니 이제 신경망의 학습과정을 구현하자. 학습이 진행되며 손실함수가 점점 줄어들어야 정상이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epoch = 300\n",
    "learning_rate = 0.003\n",
    "\n",
    "w.grad = torch.zeros_like(w) # Initialize gradient\n",
    "b.grad = torch.zeros_like(b) # Initialize gradient\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    loss_sum = 0\n",
    "    for x_i, y_i in zip(X_tensor, y_tensor):\n",
    "        # Evaluation code is commented out because not needed\n",
    "        # predict_flag = (x_i*w).sum() + b > 0\n",
    "        # answer_flag = y_i == 1\n",
    "\n",
    "        w.grad.zero_() # Clear gradient\n",
    "        b.grad.zero_() # Clear gradient\n",
    "\n",
    "        # score = x_1 * w_1 + x_2 * w_2 + ... + x_19 * w_19 + b\n",
    "        score = torch.sum(x_i*w) + b\n",
    "        loss = criterion(score.squeeze(), y_i.squeeze())\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "            b -= learning_rate * b.grad\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epoch}, Loss: {loss_sum/num_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 어느정도 진행되었으면 정확도를 다시 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_total = 0\n",
    "cnt_correct = 0\n",
    "for x_i, y_i in zip(X_tensor, y_tensor):\n",
    "    with torch.no_grad():\n",
    "        score = torch.sum(x_i*w) + b\n",
    "    predict_flag = score > 0\n",
    "    answer_flag = y_i == 1\n",
    "\n",
    "    if predict_flag == answer_flag:\n",
    "        cnt_correct += 1\n",
    "    cnt_total += 1\n",
    "\n",
    "print(f'Accuracy: {cnt_correct/cnt_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 같은 문제를 다층 퍼셉트론, `torch.nn.Module`, `torch.optim` 등을 통해 해결한 것이다. 위는 원리를 설명하기 위한 실습이고, 현업에서는 아래와 같은 코드를 작성하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module): # Multi-Layer Perceptron(Multi-Layer Neural Network) 클래스 정의\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Define layers # 모델의 레이어(층) 정의\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer # 첫 번째 완전연결층 레이어 정의 (입력 크기 -> 은닉층 크기)\n",
    "        self.relu = nn.ReLU()             # ReLU activation 활성화 함수 정의 (비선형 변환)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer # 두 번째 완전연결층 레이어 정의 (은닉층 크기 -> 출력 크기)\n",
    "\n",
    "    def forward(self, x): # 데이터를 받아서 순전파(forward pass)를 수행하는 함수\n",
    "        out = self.fc1(x)      # Input to first layer # 첫 번째 레이어에 입력값을 전달\n",
    "        out = self.relu(out)   # Apply ReLU 활성화 함수 적용\n",
    "        out = self.fc2(out)    # Output layer # 두 번째 레이어에 출력값 전달\n",
    "        return out             # 최종 출력값 반환\n",
    "\n",
    "\n",
    "# Create a dataset and data loader # 데이터셋과 데이터로더 생성\n",
    "dataset = TensorDataset(X_tensor, y_tensor) # 입력 데이터(X_tensor)와 정답 데이터(y_tensor)로 텐서 데이터셋 생성\n",
    "batch_size = 32 # 한 번에 모델에 넣는 데이터 묶음 크기 (미니배치)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # 배치로 데이터를 묶어서 모델에 전달할 수 있도록 데이터로더 생성, 셔플을 통해 데이터 순서를 무작위로 섞음\n",
    "\n",
    "# dataloader = create_dataset()\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = input_features # 입력 크기 (특징 개수)\n",
    "hidden_size = 64 # 은닉층 크기\n",
    "output_size = 1  # Binary classification # 출력 크기 (이진 분류이므로 1개의 출력)\n",
    "\n",
    "# Instantiate the model # 모델 생성\n",
    "model = SimpleMLP(input_size, hidden_size, output_size) # 정의한 모델 인스턴스화\n",
    "\n",
    "# Loss function and optimizer # 손실함수, 옵티마이저 설정\n",
    "criterion = nn.BCEWithLogitsLoss()  # Combines a sigmoid layer and the BCELoss # Binary Cross Entropy와 로짓 결합한 손실 함수 (출력값에 시그모이드를 결합하여 이진 분류)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam 옵티마이저 사용, 학습률은 0.001 (모델의 가중치를 업데이트하는 역할)\n",
    "# 그래티언트 디센트 자체를 해주는걸 옵티마이저라 한다.\n",
    "# Training parameters\n",
    "num_epochs = 20 # 학습할 에폭(epoch) 수 (데이터셋을 몇 번 반복 학습할지)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs): # epoch학습횟수 실행\n",
    "    for batch_X, batch_y in dataloader: # 데이터로더에서 미니배치를 하나씩 꺼내서 학습\n",
    "        # 순전파(Forward pass)\n",
    "        outputs = model(batch_X) # 모델에 입력값을 넣어서 예측값(outputs)을 얻음\n",
    "        loss = criterion(outputs, batch_y) # 예측값과 실제 정답(batch_y)을 비교하여 손실 계산\n",
    "\n",
    "        # Backward pass and optimization # 역전파 및 옵티마이저\n",
    "        optimizer.zero_grad()  # Clear gradients # 역전파를 하기 전에 기존의 기울기(gradient) 값을 초기화\n",
    "        loss.backward()        # Compute gradients # 역전파를 통해 기울기 계산\n",
    "        optimizer.step()       # Update weights # 옵티마이저가 기울기를 사용해 가중치 업데이트\n",
    "\n",
    "    # Print loss for every epoch # 각 에폭마다 손실 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') # 손실값을 출력하여 학습 진행 상황 확인\n",
    "\n",
    "# train()\n",
    "\n",
    "# Evaluation\n",
    "model.eval() # 모델을 평가 모드로 전환 (dropout과 같은 학습 중에만 사용하는 기능을 비활성화)\n",
    "\n",
    "with torch.no_grad(): # 평가할 때는 기울기를 계산하지 않음\n",
    "    outputs = model(X_tensor) # 전체 데이터를 모델에 넣어서 예측값 얻음\n",
    "    predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities # 출력값에 시그모이드 함수 적용해서 확률로 변환\n",
    "    predicted_classes = (predictions >= 0.5).float() # 확률이 0.5 이상이면 클래스 1, 그렇지 않으면 0으로 분류\n",
    "\n",
    "    # Calculate accuracy # 정확도 계산\n",
    "    accuracy = (predicted_classes == y_tensor).float().mean() # 예측값과 실제값을 비교하여 정확도 계산\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%') # 정확도를 퍼센트로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # Define layers\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()             # ReLU activation\n",
    "        self.fc2 = nn.Linear(hidden_size, 1) # Second fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)      # Input to first layer\n",
    "        out = self.relu(out)   # Apply ReLU\n",
    "        out = self.fc2(out)    # Output layer\n",
    "        return out\n",
    "\n",
    "    def create_dataset(self, num_samples = 1000, batch_size = 32):\n",
    "        # Features: random numbers\n",
    "        input_features = self.input_size\n",
    "        X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
    "\n",
    "        # Labels: sum of features > 0 => class 1, else class 0\n",
    "        # Please convince yourself this is the real data, while actuall not the case\n",
    "        y = (X.sum(axis=1) > 0).astype(np.float32)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.from_numpy(X)\n",
    "        y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def train(self, train_data, num_epochs = 20, learning_rate = 0.001, criterion = nn.BCEWithLogitsLoss):\n",
    "        criterion = criterion()  # Combines a sigmoid layer and the BCELoss\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_X, batch_y in train_data:\n",
    "                # Forward pass\n",
    "                outputs = self(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()  # Clear gradients\n",
    "                loss.backward()        # Compute gradients\n",
    "                optimizer.step()       # Update weights\n",
    "\n",
    "            # Print loss for every epoch\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # self.eval()\n",
    "        with torch.no_grad():\n",
    "            accuracy_history = []\n",
    "            for batch_X, batch_y in test_data:\n",
    "                outputs = self(batch_X)\n",
    "                predictions = torch.sigmoid(outputs)\n",
    "                predicted_classes = (predictions >= 0.5).float()\n",
    "                accuracy = (predicted_classes == batch_y).float().mean()\n",
    "                accuracy_history.append(accuracy)\n",
    "            accuracy = sum(accuracy_history) / len(accuracy_history)\n",
    "            print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "            return accuracy\n",
    "\n",
    "\n",
    "model = SimpleMLP(20, 64)\n",
    "train_dataset = model.create_dataset()\n",
    "test_dataset = model.create_dataset()\n",
    "print(train_dataset)\n",
    "model.train(train_dataset)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_size_candidates():\n",
    "    return [2**n for n in range(7)]\n",
    "\n",
    "def learning_rate_candidates():\n",
    "    return [0.001]\n",
    "\n",
    "def train_and_eval(hidden_size, learning_rate):\n",
    "    model = SimpleMLP(20, hidden_size)\n",
    "    train_dataset = model.create_dataset(1000)\n",
    "    test_dataset = model.create_dataset(100)\n",
    "\n",
    "    model.train(train_dataset, learning_rate = learning_rate)\n",
    "    evaluation_result = model.evaluate(test_dataset)\n",
    "\n",
    "    return evaluation_result\n",
    "\n",
    "def find_hyperparameter():\n",
    "    hidden_sizes = hidden_size_candidates()\n",
    "    learning_rates = learning_rate_candidates()\n",
    "    result_history = []\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for learning_rate in learning_rates:\n",
    "            eval_result = train_and_eval(hidden_size, learning_rate)\n",
    "            result_history.append((hidden_size, learning_rate, eval_result))\n",
    "            print(hidden_size, learning_rate, eval_result)\n",
    "    return max(result_history, key = lambda x:x[2])\n",
    "\n",
    "find_hyperparameter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "# 함수 생성\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1,2,3,4]\n",
    "y = [2,4,6,8]\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝으로\n",
    "\n",
    "1) 선형회귀 y = 4 + 3x 를 해볼 것\n",
    "    1.   선형 회귀 y = 4 + 3x를 해볼 것\n",
    "           - 맨 처음에는 torch.tensor이용해서 아예 바닥부터 구현\n",
    "           - 그 이후에 nn.Linear 써서 구현\n",
    "           - 그 이후에 dataloader 써서 구현\n",
    "           - batching 구현해보기\n",
    "           - optimizer 써보기\n",
    "\n",
    "2. 다 하고 나서 $ y = w_1 x_1  + w_2 x_2  + w_3 x_3 + b$  해볼 것\n",
    "\n",
    "$ y = (w_1, w_2, w_3) * (x_1, x_2, x_3)^T +b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
    "\n",
    "    y = 4 + 3 * X + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_synthetic_data()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
    "\n",
    "    y = 4 + 3 * X + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_synthetic_data()\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "def plot_data(x, y):\n",
    "    plt.scatter(x, y, alpha = 0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "def plot_fitted_line(x, y, y_pred):\n",
    "    plt.scatter(x, y, alpha = 0.3)\n",
    "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
    "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
    "    sorted_x = [e[0] for e in sorted_line]\n",
    "    sorted_y_pred = [e[1] for e in sorted_line]\n",
    "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.w = torch.randn(1, 1, requires_grad = True)\n",
    "        self.b = torch.randn(1, requires_grad = True)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def train(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
    "        # loss_function = nn.MSELoss()\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self(x)\n",
    "            # print(y_pred, y)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # compute the gradient of the loss\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.w -= learning_rate * self.w.grad\n",
    "                self.b -= learning_rate * self.b.grad\n",
    "\n",
    "            self.w.grad.zero_()\n",
    "            self.b.grad.zero_()\n",
    "\n",
    "            if print_log:\n",
    "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
    "        if plot:\n",
    "            plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "\n",
    "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
    "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
    "\n",
    "plot_loss_history(train_loss_history)\n",
    "plot_loss_history(test_loss_history)\n",
    "\n",
    "print(model.w.item(), model.b.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor 대신 nn.Linear 써서 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
    "\n",
    "    y = 4 + 3 * X + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_synthetic_data()\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "def plot_data(x, y):\n",
    "    plt.scatter(x, y, alpha = 0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "def plot_fitted_line(x, y, y_pred):\n",
    "    plt.scatter(x, y, alpha = 0.3)\n",
    "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
    "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
    "    sorted_x = [e[0] for e in sorted_line]\n",
    "    sorted_y_pred = [e[1] for e in sorted_line]\n",
    "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
    "        # self.b = torch.randn(1, requires_grad = True)\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # return X @ self.w + self.b\n",
    "        return self.layer(X)\n",
    "\n",
    "    def train(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
    "        # loss_function = nn.MSELoss()\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self(x)\n",
    "            # print(y_pred, y)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # compute the gradient of the loss\n",
    "            loss.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # self.w -= learning_rate * self.w.grad\n",
    "                # self.b -= learning_rate * self.b.grad\n",
    "                self.layer.weight -= learning_rate * self.layer.weight.grad\n",
    "                self.layer.bias -= learning_rate * self.layer.bias.grad\n",
    "\n",
    "            # self.w.grad.zero_()\n",
    "            # self.b.grad.zero_()\n",
    "            self.layer.weight.grad.zero_()\n",
    "            self.layer.bias.grad.zero_()\n",
    "\n",
    "            if print_log:\n",
    "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
    "        if plot:\n",
    "            plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "\n",
    "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
    "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
    "\n",
    "plot_loss_history(train_loss_history)\n",
    "plot_loss_history(test_loss_history)\n",
    "\n",
    "print(model.layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer를 써서 gradient descent를 해보기 (SGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
    "\n",
    "    y = 4 + 3 * X + noise\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_synthetic_data()\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "def plot_data(x, y):\n",
    "    plt.scatter(x, y, alpha = 0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "def plot_fitted_line(x, y, y_pred):\n",
    "    plt.scatter(x, y, alpha = 0.3)\n",
    "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
    "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
    "    sorted_x = [e[0] for e in sorted_line]\n",
    "    sorted_y_pred = [e[1] for e in sorted_line]\n",
    "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
    "        # self.b = torch.randn(1, requires_grad = True)\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.SGD\n",
    "\n",
    "    def forward(self, X):\n",
    "        # return X @ self.w + self.b\n",
    "        return self.layer(X)\n",
    "\n",
    "    def train(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
    "        # loss_function = nn.MSELoss()\n",
    "        loss_history = []\n",
    "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self(x)\n",
    "            # print(y_pred, y)\n",
    "            loss = self.loss(y_pred, y)\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            # compute the gradient of the loss\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     # self.w -= learning_rate * self.w.grad\n",
    "            #     # self.b -= learning_rate * self.b.grad\n",
    "            #     self.layer.weight -= learning_rate * self.layer.weight.grad\n",
    "            #     self.layer.bias -= learning_rate * self.layer.bias.grad\n",
    "            optimizer.step()\n",
    "\n",
    "            # self.w.grad.zero_()\n",
    "            # self.b.grad.zero_()\n",
    "            # self.layer.weight.grad.zero_()\n",
    "            # self.layer.bias.grad.zero_()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if print_log:\n",
    "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
    "        if plot:\n",
    "            plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            return self.loss(y_pred, y).item()\n",
    "\n",
    "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
    "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
    "\n",
    "plot_loss_history(train_loss_history)\n",
    "plot_loss_history(test_loss_history)\n",
    "\n",
    "print(model.layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch DataLoader 써서 데이터 다루기 (자동으로 알아서 batching 해줌!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def plot_data(x, y):\n",
    "    plt.scatter(x, y, alpha = 0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "def plot_fitted_line(x, y, y_pred):\n",
    "    plt.scatter(x, y, alpha = 0.3)\n",
    "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
    "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
    "    sorted_x = [e[0] for e in sorted_line]\n",
    "    sorted_y_pred = [e[1] for e in sorted_line]\n",
    "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1, batch_size = 32):\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
    "\n",
    "    y = 4 + 3 * X + noise\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
    "        # self.b = torch.randn(1, requires_grad = True)\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.SGD\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X.shape: (batch_size, n_feature)\n",
    "        # self.layer.shape: (n_feature, hidden_layer)\n",
    "        # X.shape @ self.layer: (batch_size, hidden_layer)\n",
    "        # m, n @ n, p -> m, p\n",
    "        # return X @ self.w + self.b\n",
    "        return self.layer(X) # 입력 X를 통해 예측값 계산\n",
    "\n",
    "    def train(self, data, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
    "        # loss_function = nn.MSELoss()\n",
    "        loss_history = []\n",
    "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in data:\n",
    "                y_pred = self(x)\n",
    "\n",
    "                loss = self.loss(y_pred, y)\n",
    "                loss_history.append(loss.item())\n",
    "\n",
    "                # compute the gradient of the loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if print_log:\n",
    "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
    "        if plot:\n",
    "            plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        with torch.no_grad():\n",
    "            loss_list = []\n",
    "\n",
    "            for x, y in data:\n",
    "                y_pred = model(x)\n",
    "                loss = self.loss(y_pred, y).item()\n",
    "                loss_list.append(loss)\n",
    "\n",
    "            return sum(loss_list) / len(loss_list)\n",
    "\n",
    "train_data = generate_synthetic_data(noise_amplitude = 1)\n",
    "test_data = generate_synthetic_data(noise_amplitude = 1)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = model.train(train_data, epochs = 1, print_log = False, plot = False)\n",
    "    train_loss_history.extend(train_loss)\n",
    "    test_loss_history.append(model.evaluate(test_data))\n",
    "\n",
    "plot_loss_history(train_loss_history)\n",
    "plot_loss_history(test_loss_history)\n",
    "\n",
    "print(model.layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $x+y<1$인지 판별하는\n",
    "  - 데이터셋을 만들고\n",
    "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
    "  - 학습/평가하고\n",
    "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
    "2. $x^2+y^2<1$인지 판별하는\n",
    "  - 데이터셋을 만들고\n",
    "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
    "  - 학습/평가하고\n",
    "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def plot_data(x, y):\n",
    "    plt.scatter(x, y, alpha = 0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "def generate_artificial_data(n_samples, batch_size = 32):\n",
    "    x = torch.randn(n_samples, 2)\n",
    "    y = torch.sum(x**2, dim = 1).unsqueeze(dim = 1)\n",
    "\n",
    "    y = (y > 1).float()\n",
    "\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(2, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.SGD\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer1(x) # output.shape: (batch_size, hidden_size)\n",
    "        output = self.activation(output)\n",
    "        output = self.layer2(output) # output.shape: (batch_size, 1)\n",
    "        output = self.sigmoid(output) # output.shape: (batch_size, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train_model(self, train_data, valid_data, epochs = 20, learning_rate = 0.002):\n",
    "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "        train_loss_history = []\n",
    "        valid_loss_history = []\n",
    "        valid_acc_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_data:\n",
    "                y_pred = model(x)\n",
    "                loss = self.loss(y, y_pred)\n",
    "                train_loss_history.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                acc, valid_loss = self.evaluate_model(valid_data)\n",
    "                self.train()\n",
    "                valid_acc_history.append(acc)\n",
    "                valid_loss_history.append(valid_loss)\n",
    "\n",
    "            # print(f'{epoch+1}/{epochs} Epoch, train loss = {loss}, valid loss = {valid_loss}, acc = {acc}')\n",
    "\n",
    "        return train_loss_history, valid_loss_history, valid_acc_history\n",
    "\n",
    "    def evaluate_model(self, data):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            loss_history = []\n",
    "            for x, y in data:\n",
    "                y_pred = self(x)\n",
    "                loss_history.append(self.loss(y, y_pred).item())\n",
    "                correct += torch.sum(((y_pred > 0.5).float() == y).float())\n",
    "                total += 32\n",
    "\n",
    "            return correct / total, sum(loss_history) / len(loss_history)\n",
    "\n",
    "model = BinaryClassifier(32) #\n",
    "train_data = generate_artificial_data(1000)\n",
    "valid_data = generate_artificial_data(100)\n",
    "test_data = generate_artificial_data(100)\n",
    "\n",
    "train_loss_history, valid_loss_history, valid_acc_history = model.train_model(train_data, valid_data, epochs = 100)\n",
    "plot_loss_history(train_loss_history)\n",
    "plot_loss_history(valid_loss_history)\n",
    "plot_loss_history(valid_acc_history)\n",
    "\n",
    "model.evaluate_model(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
