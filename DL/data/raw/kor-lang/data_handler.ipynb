{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement debug_shell (from versions: none)\n",
      "ERROR: No matching distribution found for debug_shell\n"
     ]
    }
   ],
   "source": [
    "!pip install debug_shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "# y값이 어쩌고 일때 쓰면 안된다 성능에 영향 꽤 준다\n",
    "from collections import defaultdict\n",
    "from debug_shell import debug_shell\n",
    "\n",
    "class Vocabulary:\n",
    "    PAD = '[PAD]' # 무조건넣어주고\n",
    "    SOS = '[SOS]' # \n",
    "    EOS = '[EOS]' # S2S에서는 무조건필요\n",
    "    OOV = '[OOV]'\n",
    "    SPECIAL_TOKENS = [PAD, SOS, EOS, OOV]\n",
    "    PAD_IDX = 0\n",
    "    SOS_IDX = 1\n",
    "    EOS_IDX = 2\n",
    "    OOV_IDX = 3\n",
    "    \n",
    "    def __init__(self, word_count, coverage = 0.999):\n",
    "        '''Accept word_count dictionary having word as key, and frequency as value.\n",
    "        '''\n",
    "        word_freq_list = []\n",
    "        total = 0\n",
    "        \n",
    "        for word, freq in word_count.items():\n",
    "            word_freq_list.append((word, freq))\n",
    "            total += freq\n",
    "            \n",
    "        word_freq_list = sorted(word_freq_list, key = lambda x: x[1], reverse = True)\n",
    "        word2idx = {}\n",
    "        idx2word = {}\n",
    "        s = 0\n",
    "        \n",
    "        for idx, (word, freq) in enumerate([(e, 0) for e in Vocabulary.SPECIAL_TOKENS):\n",
    "            s += freq\n",
    "            if s > coverage\n",
    "    \n",
    "    def word_to_index(self, word):\n",
    "        if word in self.word2idx:\n",
    "            return self.word2idx[word]\n",
    "        return Vocabulary.OOV_IDX\n",
    "    \n",
    "def parse_file(file_path, train_valid_test_ratio = (0.8, 0.1, 0.1), batch_size = 32, ):\n",
    "    f = open(file_path, 'r', encoding = 'utf-8')\n",
    "    data = []\n",
    "    \n",
    "    source_word_count = defaultdict(int)\n",
    "    target_word_count = defaultdict(int)\n",
    "\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        source, target, etc = line.split('\\t')\n",
    "        \n",
    "        source = source.split()\n",
    "        for source_token in source:\n",
    "            source_word_count[source_token] += 1\n",
    "        \n",
    "        target = target.split()\n",
    "        for target_token in target:\n",
    "            target_word_count[target_token] += 1\n",
    "        \n",
    "        data.append((source, target))\n",
    "        \n",
    "    source_vocab = Vocabulary(source_word_count)\n",
    "    target_vocab = Vocabulary(target_word_count)\n",
    "    for idx, (source, target) in enumerate(data):\n",
    "        data[idx] = (\n",
    "            list(map(source_vocab.word_to_index, source)),\n",
    "            list(map(target_vocab.word_to_index, target)),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    lengths = [int(len(data) * ratio) for ratio in train_valid_test_ratio]\n",
    "    lengths[-1] = len(data) - sum(lengths[:-1])\n",
    "    datasets = random_split(data, lengths)\n",
    "    dataloaders = [DataLoader(dataset,\n",
    "                              batch_size = batch_size,\n",
    "                              shuffle = True,\n",
    "                              collate_fn = lambda x: preprocessing(x, source_vocab, target_vocab))\n",
    "                   for dataset in datasets]\n",
    "    \n",
    "    return dataloaders, source_vocab, target_vocab\n",
    "\n",
    "def preprocessing(batch, source_vocab, target_vocab):\n",
    "    sources = [e[0] for e in batch]\n",
    "    targets = [e[1] for e in batch]\n",
    "    \n",
    "    source_seqs = []\n",
    "    target_seqs = []\n",
    "    \n",
    "    for source_seq in source:\n",
    "        source_seqs.append([target_vocab.SOS_IDX] + target_seq + [target_vocab.EOS_IDX])\n",
    "    \n",
    "    for target_seq in targets:\n",
    "        target_seqs.append([target_vocab.SOS_IDX] + target_seq + [target_vocab.EOS_IDX])\n",
    "    \n",
    "    source_max_length = max([len(s) for s in source_seqs])\n",
    "    target_max_length = max([len(s) for s in target_seqs])\n",
    "    \n",
    "    for idx, seq in enumerate(source_seqs):\n",
    "        seq = seq + [source_vocab.PAD_DIX] * (source_max_length - len(seq))\n",
    "        assert len(seq) == source_max_length, f'Expected to have {source_max_length}, now {len(seq)}'\n",
    "        source_seqs[idx] = seq\n",
    "    \n",
    "    for idx, seq in enumerate(target_seqs):\n",
    "        seq = seq + [target_vocab.PAD_DIX] * (target_max_length - len(seq))\n",
    "        assert len(seq) == target_max_length, f'Expected to have {target_max_length}, now {len(seq)}'\n",
    "        target_seq[idx] = seq\n",
    "    \n",
    "    return torch.tensor(source_seqs), torch.tensor\n",
    "    (target_seqs)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    from code import interact\n",
    "    batch_size = 32\n",
    "    (train, valid, test), source_vocab, target_vocab = parse_file('for.txt', batch_size = batch_size)\n",
    "    \n",
    "    for source_batch, target_batch in train:\n",
    "        assert source_batch.shape[0] == batch_size\n",
    "        print(source_batch)\n",
    "        \n",
    "        assert target_batch.shape[0] == batch_size\n",
    "        print(target_batch)\n",
    "    # interact(local = locals()) # 이 시점에서 파이썬 쉘이 써진다.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class Seq2Seq(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (227116599.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    pass\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    pass\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        encoder_hidden = self.encoder(source)\n",
    "        outputs = self.decoder(target, encoder_hidden)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = Seq2Seq(encoder = ...\n",
    "                    decoder = ...)\n",
    "    train, valid, test = parse_file(...)\n",
    "    \n",
    "    for source_batch, target_batch in train:\n",
    "        model(source_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class RNNCellManual(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCellManual(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(RNNCellManual, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.i2h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x_t, h_t):\n",
    "        \"\"\"\n",
    "        args:\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        assert x_t.size(1) == self.input_dim, f'Input dimension was expected to be {self.input_dim}, got{x_t.size(1)}'\n",
    "        assert h_t.size(0) == batch_size, f'0th dimension of h_t is expected to be {batch_size}, got {h_t.size(0)}'\n",
    "        assert h_t.size(1) == self.hidden_dim, f'Hidden dimension was expected to be {self.hidden_dim}, got {h_t.size(1)}'\n",
    "        \n",
    "        h_t = torch.tanh(self.i2h(x_t) + self.h2h(h_t))\n",
    "        \n",
    "        # 여기부터 짜다 말은것\n",
    "        assert h_t.size(0) ==\n",
    "        return h_t\n",
    "\n",
    "class LSTMCellManual(nn.Module): # An LSTM cell implemented using nn.Linear layers.\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LSTMCellManual, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.i2i = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2i = nn.Linear(input_dim, hidden_dim)\n",
    "        self.i2f = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2f = nn.Linear(input_dim, hidden_dim)\n",
    "        self.i2g = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2g = nn.Linear(input_dim, hidden_dim)\n",
    "        self.i2o = nn.Linear(input_dim, hidden_dim)\n",
    "        self.h2o = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "    \n",
    "    def forward(self, x_t, h_t, c_t):\n",
    "        batch_size = x_t \n",
    "        i_t = torch.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
    "        f_t = torch.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
    "        o_t = torch.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
    "        g_t = torch.tanh(self.W_g(x) + self.U_g(h_prev))\n",
    "        \n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class MyLSTM(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import linecache\n",
    "\n",
    "from code import interact, InteractiveInterpreter\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "def debug_shell(line_window = 5):\n",
    "    \"\"\"Print the usual traceback information, followed by a listing of all the local variables in each frame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert False # intentional exception\n",
    "    except AssertionError:\n",
    "        tb = sys.exc_info()[2]\n",
    "\n",
    "    while True:\n",
    "        if not tb.tb_next:\n",
    "            break\n",
    "        tb = tb.tb_next\n",
    "\n",
    "    stack = []\n",
    "    f = tb.tb_frame.f_back\n",
    "    log = []\n",
    "    local_history = []\n",
    "\n",
    "    while f:\n",
    "        line_range = range(-line_window, line_window + 1)\n",
    "\n",
    "        code_lines = []\n",
    "        for line_no in line_range:\n",
    "            # if line_no + f.f_lineno >= 1:\n",
    "            l = linecache.getline(f.f_code.co_filename, f.f_lineno + line_no)\n",
    "            if l != '':\n",
    "                code_lines.append('%d | %s'%(f.f_lineno + line_no, l))\n",
    "\n",
    "        code_lines = ''.join(code_lines)\n",
    "\n",
    "        log.append('{}, {}:{}{}{}{}{}'.format(f.f_code.co_name, f.f_code.co_filename, f.f_lineno, os.linesep, os.linesep, code_lines, os.linesep))\n",
    "        stack.append(f)\n",
    "        local_history.append(f.f_locals)\n",
    "\n",
    "        f = f.f_back\n",
    "\n",
    "    log.reverse()\n",
    "    local_history.reverse()\n",
    "    frame = stack[0]\n",
    "\n",
    "    # helper functions for debugging\n",
    "\n",
    "    def extract_history(var_name):\n",
    "        \"\"\"Show the history of specific variable name.\n",
    "        \"\"\"\n",
    "\n",
    "        res = {}\n",
    "\n",
    "        for idx, hist in enumerate(local_history):\n",
    "            if var_name in hist:\n",
    "                print('=============')\n",
    "                print(log[idx].split(os.linesep)[0])\n",
    "                print('%s : %s (type %s)'%(var_name, hist[var_name], type(hist[var_name])))\n",
    "                res[idx] = hist[var_name]\n",
    "\n",
    "        tmp = []\n",
    "\n",
    "        for k, v in res.items():\n",
    "            for e in tmp:\n",
    "                if type(e) == type(v) and e == v:\n",
    "                    break\n",
    "            else:\n",
    "                tmp.append(v)\n",
    "\n",
    "        if tmp == 1:\n",
    "            return tmp[0]\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "    tools = {'extract_history' : extract_history,\n",
    "             'history' : extract_history, }\n",
    "\n",
    "    print(('================' + os.linesep).join(log))\n",
    "\n",
    "    debugger_locals = {\\\n",
    "        'log' : log,\n",
    "        'traceback_log' : ('================' + os.linesep).join(log),\n",
    "        'debug_pos' : '{}, {}:{}'.format(frame.f_code.co_name, frame.f_code.co_filename, frame.f_lineno),\n",
    "        'local_history' : local_history,\n",
    "        'pprint' : pprint,\n",
    "        'pformat' : pformat,\n",
    "        **frame.f_globals,\n",
    "        **frame.f_locals,\n",
    "        **tools,\n",
    "        **globals()}\n",
    "\n",
    "    def run(file_name = 'tmp_test.py', tmp_locals = debugger_locals):\n",
    "        cur_dir = os.getcwd()\n",
    "        new_dir = os.sep.join(frame.f_code.co_filename.split(os.sep)[:-1])\n",
    "        os.chdir(new_dir)\n",
    "        code_file = open(file_name, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "        i = InteractiveInterpreter(locals = tmp_locals)\n",
    "        i.runcode(code_file)\n",
    "        os.chdir(cur_dir)\n",
    "\n",
    "    interact(local = {'run' : run, **debugger_locals, })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
