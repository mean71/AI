{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mean71\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'reshape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 176\u001b[0m\n\u001b[0;32m    173\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_letters)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(all_categories))\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# predict_nationality(model, 'ang')\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 123\u001b[0m, in \u001b[0;36mMLP.train_model\u001b[1;34m(self, train_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    120\u001b[0m total_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# hidden = self.initHidden() # 은닉상태 초기화 # MLP는 시간적인 정보를 처리하지 않아 히든상태의 개념이 없다 무의미하다#RNN은 반복되어 필수\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
          ]
        }
      ],
      "source": [
        "# 순방향 신경망(Feedforward Neural Network) -> 다층 퍼셉트론(MLP: Multi-Layer Perceptron)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import glob\n",
        "import string\n",
        "\n",
        "unknown_char,padding = '?','_'\n",
        "all_letters = string.ascii_lowercase + unknown_char + padding\n",
        "category_names,all_categories = {},[]\n",
        "max_word_length,batch_size = 10,32\n",
        "len_all_categories=0\n",
        "test_size,val_size = 0.2, 0.1\n",
        "\n",
        "def activate_config(unknown_char='?', padding='_', max_word_length=10, batch_size=32, test_size=0.2, val_size=0.1):\n",
        "    unknown_char,padding = unknown_char,padding\n",
        "    all_letters = string.ascii_lowercase + unknown_char + padding\n",
        "    max_word_length,batch_size = max_word_length,batch_size\n",
        "    test_size=test_size\n",
        "    val_size=val_size\n",
        "\n",
        "def category_to_tensor(idx, N):\n",
        "    return torch.tensor([(i==idx)/1 for i in range(N)])\n",
        "\n",
        "def input_padding_and_unknown_char_handling(word):\n",
        "    word = ''.join(char if char in string.ascii_lowercase else unknown_char for char in word.lower())\n",
        "    return word + '_'*(10 - len(word)) if len(word)!=10 else word\n",
        "\n",
        "def word_to_tensor(word):\n",
        "    word = input_padding_and_unknown_char_handling(word)\n",
        "    res = torch.zeros(len(word), len(all_letters))\n",
        "    for idx, letter in enumerate(word):\n",
        "        res[idx][all_letters.find(letter)] = 1\n",
        "    # print(word, res, res.shape)\n",
        "    return res #.squeeze(dim = 1) # shape(10,28)    \n",
        "\n",
        "def preprocessing_data(batch_size = 32, test_size=0.2, val_size=0.1):\n",
        "    files = glob.glob('./data/names/*.txt')\n",
        "    # print(f'{len(files)}fiels',*files,sep='\\n')\n",
        "    \n",
        "    for file in files:\n",
        "        with open(file) as f:\n",
        "            words = f.read().strip().split('\\n')\n",
        "        category = file.split('\\\\')[-1].split('.')[0]\n",
        "        all_categories.append(category)\n",
        "\n",
        "        words = [word for word in words if len(word) <= max_word_length]\n",
        "        category_names[category] = words\n",
        "        # print(f'{category}: {len(words)} |', *words[0:10])\n",
        "    # print(all_categories)\n",
        "    # print(category_names)\n",
        "\n",
        "    \n",
        "    for category, words in category_names.items():\n",
        "        x = [word_to_tensor(word) for word in words]\n",
        "        y = [torch.tensor(all_categories.index(category)) for word in words]\n",
        "    \n",
        "    x = torch.stack(x)\n",
        "    y = torch.stack(y)\n",
        "\n",
        "    x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=(test_size + val_size))\n",
        "    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=(test_size / (test_size + val_size)))\n",
        "    \n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    val_dataset = TensorDataset(x_val, y_val)\n",
        "    test_dataset = TensorDataset(x_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def plot_loss_history(loss_history, val_loss_history=None): # 훈련손실, 검증손실\n",
        "    plt.figure(figsize=(10, 6)) \n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history, label='Training Loss', color='blue', marker='o')\n",
        "    if val_loss_history is not None: plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, label='Validation Loss', color='orange', marker='x')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(range(1, len(loss_history) + 1)) # x축 눈금 설정\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.input_hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_hidden = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_output = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.LogSoftmax(dim = 1) # 출력값을 확률분포로 변환\n",
        "        self.optimizer = optim.Adam # 다양한 이동 평균(학습률)을 적용하여 손실함수 최적화 # 모델의 파라미터(가중치)를 업데이트하여 손실을 최소화하는 역할\n",
        "        self.loss = torch.nn.NLLLoss() # 모델의 예측과 실제 값 간의 차이를 측정 : 다중 클래스 분류문제에서 모델성능평가에 사용되는 중요 손실함수\n",
        "\n",
        "    def forward(self, input):\n",
        "        hidden = self.relu(self.input_hidden(input))\n",
        "        hidden = self.relu(self.hidden_hidden(hidden))\n",
        "        output = self.hidden_output(hidden)\n",
        "        output = self.softmax(output)\n",
        "        return output\n",
        "    \n",
        "    def train_model(self, train_data, learning_rate = 0.001, epochs = 20):\n",
        "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "            \n",
        "            for input, label in train_data:\n",
        "                input = input.reshape(input.size(0), input.size(1)*input.size(2))\n",
        "                # hidden = self.initHidden() # 은닉상태 초기화 # MLP는 시간적인 정보를 처리하지 않아 히든상태의 개념이 없다 무의미하다#RNN은 반복되어 필수\n",
        "                optimizer.zero_grad()\n",
        "                output = self(input)# 전체 입력 처리\n",
        "                # print(output)\n",
        "                label = label.long()  # label을 LongTensor로 변환\n",
        "                \n",
        "                loss = self.loss(output, label)\n",
        "                loss.backward() # 기울기 누적\n",
        "                optimizer.step() # 파라미터 업데이트\n",
        "                \n",
        "                running_loss += loss.item() # 손실기록\n",
        "                \n",
        "                print(torch.mean(loss).item())\n",
        "                print(output.shape)\n",
        "                # predicted = output.argmax(1)  # 가장 높은 확률의 인덱스\n",
        "                predicted = torch.argmax(output, 1)  # 가장 높은 확률의 인덱스\n",
        "                \n",
        "                print(predicted.shape)\n",
        "                print(label.shape)\n",
        "                correct_predictions += (predicted == label).float().sum().item()  # 맞춘 예측 개수\n",
        "                total_predictions += label.size(0)  # 총 예측 개수\n",
        "                print(correct_predictions)\n",
        "                print(total_predictions)\n",
        "                \n",
        "                if len(loss_history)%1000 == 0:\n",
        "                    print(torch.mean(loss).item())\n",
        "            \n",
        "            \n",
        "            \n",
        "            epoch_loss = running_loss / len(train_data)\n",
        "            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0 # 정확도 계산\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.5f}, Accuracy: {accuracy:.5f}') # 정확도 출력\n",
        "            loss_history.append(epoch_loss)  # 손실 기록\n",
        "        plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "def predict_nationality(model, word):\n",
        "    word_tensor = word_to_tensor(word).unsqueeze(0)\n",
        "    print(word_tensor.shape)\n",
        "    \n",
        "    output = model(word_tensor)\n",
        "    _, predicted_index = torch.max(output, 1)\n",
        "    return predicted_index\n",
        "\n",
        "activate_config(unknown_char='?', padding='_',\n",
        "                max_word_length=10, batch_size=32,\n",
        "                test_size=0.2, val_size=0.1)\n",
        "dataset = preprocessing_data(batch_size,test_size,val_size)\n",
        "\n",
        "model = MLP(input_size=len(all_letters)*10, hidden_size=32, output_size=len(all_categories))\n",
        "# predict_nationality(model, 'ang')\n",
        "\n",
        "model.train_model(dataset, learning_rate = 0.001, epochs = 200)\n",
        "print(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "오늘은 지금까지 배운 전반부 딥러닝에서의 내용을 총체적으로 활용하여 문제를 직접 a부터 z까지 해결해보는 시간입니다.\n",
        "\n",
        "오늘 다룰 데이터는 names 안에 있는 각국의 이름을 받아, 이름의 국적을 추론하는 뉴럴 넷을 만드는 것입니다. 방법은 자유이나, 다음의 두 가지 방법을 꼭 포함시켜 구현하고, 성능 측정/튜닝의 과정을 거치세요.\n",
        "\n",
        "1. Feedforward Network(MLP)를 사용하여 해볼 것.\n",
        "  -  Feedforward Network를 이용하여 할 경우, input size가 이름에 따라 다른데 이를 어떤 식으로 다룰지 생각해 볼 것\n",
        "2. RNN을 이용하여 해볼 것.\n",
        "  - torch.DataLoader을 사용할 경우 batching을 하게 되는데, 이 경우 위 1에서 써 있는 문제와 비슷한 문제가 있으며 해결 또한 비슷하게 할 수 있음. torch.DataLoader을 쓸 때와 쓰지 않을 때를 비교해볼 것.\n",
        "  - 어제는 영어 알파벳만 사용하였는데, 다른 알파벳까지 사용할 경우 성능이 올라가는지 내려가는지 관찰해볼 것.\n",
        "  - 데이터에서 관찰하지 못한 알파벳들을 OOV로 처리하는 방식을 시도할 경우 성능이 올라가는지 내려가는지 관찰해볼 것\n",
        "\n",
        "성능 측정을 위해서 train-valid-test를 적절히 나누고, valid에서의 loss와 정확도를 측정하면서 학습을 적절할 때 멈추거나 할 것.\n",
        "\n",
        "튜닝을 위해서는 다음의 방법들을 사용할 수 있습니다.\n",
        "\n",
        "- hyperparameter을 다양하게 시도해볼 것\n",
        "- 뉴럴 넷 구조를 조금씩 바꿔볼 것\n",
        "- 데이터를 더 만들어 볼 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "d = defaultdict(int)\n",
        "\n",
        "for i in range(10):\n",
        "    d[i] += 1\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset(batch_size = 32):\n",
        "    files = glob.glob('./data/names/*.txt')\n",
        "\n",
        "    assert len(files) == 18\n",
        "\n",
        "    character_dict = defaultdict(int)\n",
        "    name_length_dict = defaultdict(int)\n",
        "\n",
        "    data = []\n",
        "    languages = []\n",
        "\n",
        "    for file in files:\n",
        "        with open(file) as f:\n",
        "            names = f.read().strip().split('\\n')\n",
        "        lang = file.split('/')[1].split('.')[0]\n",
        "\n",
        "        if lang not in languages:\n",
        "            languages.append(lang)\n",
        "\n",
        "        for name in names:\n",
        "            for char in name:\n",
        "                character_dict[char.lower()] += 1\n",
        "            name_length_dict[len(name)] += 1\n",
        "            data.append([name, lang])\n",
        "\n",
        "    lst = []\n",
        "    for k, v in character_dict.items():\n",
        "        lst.append((k, v))\n",
        "\n",
        "    name_length_lst = []\n",
        "    for k, v in name_length_dict.items():\n",
        "        name_length_lst.append((k, v))\n",
        "\n",
        "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
        "    name_length_lst = sorted(name_length_lst, key = lambda x:x[1], reverse = True)\n",
        "\n",
        "    total_count = sum([e[1] for e in lst])\n",
        "    total_count_name = sum([e[1] for e in name_length_lst])\n",
        "\n",
        "    s = 0\n",
        "    alphabets = []\n",
        "\n",
        "    oov = '[OOV]'\n",
        "    pad = '[PAD]'\n",
        "\n",
        "    for k, v in lst:\n",
        "        s += v\n",
        "        if s < 0.999*total_count:\n",
        "            alphabets.append(k)\n",
        "    s = 0\n",
        "    max_length_candidate = []\n",
        "    for k, v in name_length_lst:\n",
        "        s += v\n",
        "        if s < 0.999*total_count_name:\n",
        "            max_length_candidate.append(k)\n",
        "\n",
        "    alphabets.append(oov)\n",
        "\n",
        "    for elem in data:\n",
        "        tmp = []\n",
        "        for char in data[0]:\n",
        "            if char in alphabets:\n",
        "                tmp.append(char)\n",
        "            else:\n",
        "                tmp.append(oov)\n",
        "        data[0] = word2tensor(tmp)\n",
        "\n",
        "    x = torch.stack([e[0] for e in data])\n",
        "    y = torch.stack([torch.tensor(e[1]) for e in data])\n",
        "\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# now the data is list of[list of character, lang_name]\n",
        "\n",
        "alphabets = 'abcdefghijklmnopqrstuvwxzy'\n",
        "max_length = 17\n",
        "\n",
        "\n",
        "\n",
        "dataset = generate_dataset()\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "OOV = '[OOV]'\n",
        "PAD = '[PAD]'\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 32\n",
        "\n",
        "def letter2tensor(letter, alphabets, oov = OOV):\n",
        "    res = [0 for _ in range(len(alphabets))]\n",
        "\n",
        "    if letter in alphabets:\n",
        "        idx = alphabets.index(letter)\n",
        "    else:\n",
        "        idx = alphabets.index(oov)\n",
        "\n",
        "    res[idx] = 1\n",
        "\n",
        "    return torch.tensor(res)\n",
        "\n",
        "def word2tensor(word, max_length, alphabets, pad = PAD, oov = OOV):\n",
        "    # return torch.tensor with size (max_length, len(alphabets))\n",
        "    res = torch.zeros(max_length, len(alphabets))\n",
        "\n",
        "    for idx, char in enumerate(word):\n",
        "        if idx < max_length:\n",
        "            res[idx] = letter2tensor(char, alphabets, oov = oov)\n",
        "\n",
        "    for i in range(max_length - len(word)):\n",
        "        res[len(word) + i] = letter2tensor(pad, alphabets, oov = oov)\n",
        "\n",
        "    return res\n",
        "\n",
        "def determine_alphabets(data, pad = PAD, oov = OOV, threshold = 0.999):\n",
        "    # data = list of [name, language_name]\n",
        "    lst = []\n",
        "    character_dict = defaultdict(int)\n",
        "\n",
        "    for name, lang in data:\n",
        "        for char in name:\n",
        "            character_dict[char.lower()] += 1\n",
        "\n",
        "    for k, v in character_dict.items():\n",
        "        lst.append((k, v))\n",
        "\n",
        "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
        "    total_count = sum([e[1] for e in lst])\n",
        "    s = 0\n",
        "\n",
        "    alphabets = []\n",
        "\n",
        "    for k, v in lst:\n",
        "        s += v\n",
        "        if s < threshold * total_count:\n",
        "            alphabets.append(k)\n",
        "\n",
        "    alphabets.append(pad)\n",
        "    alphabets.append(oov)\n",
        "\n",
        "    return alphabets\n",
        "\n",
        "def determine_max_length(data, threshold = 0.99):\n",
        "    lst = []\n",
        "    name_length_dict = defaultdict(int)\n",
        "\n",
        "    for name, lang in data:\n",
        "         name_length_dict[len(name)] += 1\n",
        "\n",
        "    for k, v in name_length_dict.items():\n",
        "        lst.append((k, v))\n",
        "\n",
        "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
        "    total_count = sum([e[1] for e in lst])\n",
        "    s = 0\n",
        "\n",
        "    for k, v in lst:\n",
        "        s += v\n",
        "        if s > threshold * total_count:\n",
        "            return k - 1\n",
        "    # if not, return the maximum value in lst\n",
        "    return max(lst, key = lambda x:x[0])[0]\n",
        "\n",
        "def load_file(): \n",
        "    files = glob.glob('./data/names/*.txt')\n",
        "    \n",
        "    assert len(files) == 18\n",
        "\n",
        "    data = []\n",
        "    languages = []\n",
        "\n",
        "    for file in files:\n",
        "        with open(file) as f:\n",
        "            names = f.read().strip().split('\\n')\n",
        "        lang = file.split('/')[1].split('.')[0]\n",
        "\n",
        "        if lang not in languages:\n",
        "            languages.append(lang)\n",
        "\n",
        "        for name in names:\n",
        "            data.append([name, lang])\n",
        "\n",
        "    return data, languages\n",
        "\n",
        "def generate_dataset(batch_size = 32, pad = PAD, oov = OOV):\n",
        "    data, languages = load_file()\n",
        "\n",
        "    alphabets = determine_alphabets(data, pad = pad, oov = oov)\n",
        "    max_length = determine_max_length(data)\n",
        "    print(alphabets, max_length)\n",
        "\n",
        "    for idx, elem in enumerate(data):\n",
        "        tmp = []\n",
        "        for char in elem[0]:\n",
        "            if char.lower() in alphabets:\n",
        "                tmp.append(char.lower())\n",
        "            else:\n",
        "                tmp.append(oov)\n",
        "\n",
        "        data[idx][0] = word2tensor(tmp, max_length, alphabets, pad = pad, oov = oov)\n",
        "        data[idx][1] = languages.index(data[idx][1])\n",
        "\n",
        "    x = [e[0] for e in data]\n",
        "    y = [torch.tensor(e[1]) for e in data]\n",
        "\n",
        "    train_x, train_y, valid_x, valid_y, test_x, test_y = split_train_valid_test(x, y)\n",
        "\n",
        "    train_x = torch.stack(train_x)\n",
        "    train_y = torch.stack(train_y)\n",
        "    valid_x = torch.stack(valid_x)\n",
        "    valid_y = torch.stack(valid_y)\n",
        "    test_x = torch.stack(test_x)\n",
        "    test_y = torch.stack(test_y)\n",
        "\n",
        "    train_dataset = TensorDataset(train_x, train_y)\n",
        "    valid_dataset = TensorDataset(valid_x, valid_y)\n",
        "    test_dataset = TensorDataset(test_x, test_y)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader, alphabets, max_length, languages\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset, alphabets, max_length, languages  = generate_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x, y in dataset:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = torch.randn(3,4,2)\n",
        "print(t)\n",
        "print(t.reshape(3, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tensor2word(t, alphabets):\n",
        "    # t.shpae: max_length, len(alphabets)\n",
        "    res = []\n",
        "    for char_tensor in t:\n",
        "        char = alphabets[int(torch.argmax(char_tensor).item())]\n",
        "        res.append(char)\n",
        "\n",
        "    return res\n",
        "\n",
        "def idx2lang(idx, languages):\n",
        "    return languages[idx]\n",
        "\n",
        "for batch_x, batch_y in dataset:\n",
        "    for i in range(batch_x.size(0)):\n",
        "        print(tensor2word(batch_x[i], alphabets), idx2lang(batch_y[i], languages))\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def pick_train_valid_test(train, valid, test):\n",
        "    assert [train, valid, test] != [0, 0, 0]\n",
        "    options = [train, valid, test]\n",
        "\n",
        "    pick = random.choice([0, 1, 2])\n",
        "\n",
        "    while options[pick] == 0:\n",
        "        pick = random.choice([0, 1, 2])\n",
        "    assert options[pick] != 0\n",
        "    return pick\n",
        "\n",
        "print(pick_train_valid_test(0,3,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = torch.tensor([1.0, 2.0])\n",
        "print(t.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "\n",
        "def modify_dataset_for_ffn(dataset):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    for batch_x, batch_y in dataset:\n",
        "        for i in range(batch_x.size(0)):\n",
        "            x.append(batch_x[i].reshape((batch_x.size(1) * batch_x.size(2))))\n",
        "            y.append(batch_y[i])\n",
        "\n",
        "    train_x, train_y, valid_x, valid_y, test_x, test_y = split_train_valid_test(x, y)\n",
        "\n",
        "    train_x = torch.stack(train_x)\n",
        "    train_y = torch.stack(train_y)\n",
        "    valid_x = torch.stack(valid_x)\n",
        "    valid_y = torch.stack(valid_y)\n",
        "    test_x = torch.stack(test_x)\n",
        "    test_y = torch.stack(test_y)\n",
        "\n",
        "    train_dataset = TensorDataset(train_x, train_y)\n",
        "    valid_dataset = TensorDataset(valid_x, valid_y)\n",
        "    test_dataset = TensorDataset(test_x, test_y)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return train_dataloader, valid_dataloader, test_dataloader\n",
        "\n",
        "def pick_train_valid_test(train, valid, test):\n",
        "    assert [train, valid, test] != [0, 0, 0]\n",
        "    options = [train, valid, test]\n",
        "\n",
        "    pick = random.choice([0, 1, 2])\n",
        "\n",
        "    while options[pick] == 0:\n",
        "        pick = random.choice([0, 1, 2])\n",
        "    assert options[pick] != 0\n",
        "    return pick\n",
        "\n",
        "def split_train_valid_test(x, y, train_valid_test_ratio = (0.7, 0.15, 0.15)):\n",
        "    # TensorDataset -> TensorDataset, TensorDataset, TensorDataset\n",
        "    # x, y: list of data\n",
        "    train_ratio, valid_ratio, test_ratio = train_valid_test_ratio\n",
        "    y_label_dict = defaultdict(int)\n",
        "    for y_data in y:\n",
        "        y_label_dict[y_data.item()] += 1\n",
        "\n",
        "    no_per_labels = {} # y_label별로 각각 train, valid, test\n",
        "\n",
        "    for y_label, freq in y_label_dict.items():\n",
        "        train_size, valid_size, test_size = int(freq * train_ratio), int(freq * valid_ratio), freq - int(freq * train_ratio) - int(freq * valid_ratio)\n",
        "        no_per_labels[y_label] = [train_size, valid_size, test_size]\n",
        "\n",
        "    train_x, train_y = [], []\n",
        "    valid_x, valid_y = [], []\n",
        "    test_x, test_y = [], []\n",
        "\n",
        "    for x_data, y_data in zip(x, y):\n",
        "        idx = pick_train_valid_test(*no_per_labels[y_data.item()])\n",
        "        assert no_per_labels[y_data.item()][idx] > 0\n",
        "        no_per_labels[y_data.item()][idx] -= 1\n",
        "\n",
        "        if idx == 0:\n",
        "            train_x.append(x_data)\n",
        "            train_y.append(y_data)\n",
        "        elif idx == 1:\n",
        "            valid_x.append(x_data)\n",
        "            valid_y.append(y_data)\n",
        "        elif idx == 2:\n",
        "            test_x.append(x_data)\n",
        "            test_y.append(y_data)\n",
        "\n",
        "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
        "\n",
        "\n",
        "def plot_loss_history(loss_history, other_loss_history = []):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    if other_loss_history != []:\n",
        "        plt.plot(range(1, len(other_loss_history) + 1), other_loss_history)\n",
        "    plt.show()\n",
        "\n",
        "# len(alphabets) * max_length * hidden_size + hidden_size * len(languages)\n",
        "# 32 * 12 * 64 + 64 * 18 = 25000\n",
        "#\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(len(alphabets) * max_length, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, len(languages))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, max_length, len(alphabets) : 32, 12, 57) -> (32, 12*57)\n",
        "        output = self.layer1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.layer2(output)\n",
        "        output = F.log_softmax(output, dim = -1)\n",
        "\n",
        "        return output # (batch_size, len(languages) : 32, 18)\n",
        "\n",
        "    def train_model(self, train_data, valid_data, epochs = 100, learning_rate = 0.001, print_every = 1000):\n",
        "        criterion = F.nll_loss\n",
        "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "        step = 0\n",
        "        train_loss_history = []\n",
        "        valid_loss_history = []\n",
        "\n",
        "        train_log = {}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in train_data:\n",
        "                step += 1\n",
        "                y_pred = self(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                mean_loss = torch.mean(loss).item()\n",
        "\n",
        "                if step % print_every == 0 or step == 1:\n",
        "                    train_loss_history.append(mean_loss)\n",
        "                    valid_loss, valid_acc = self.evaluate(valid_data)\n",
        "                    valid_loss_history.append(valid_loss)\n",
        "                    print(f'[Epoch {epoch}, Step {step}] train loss: {mean_loss}, valid loss: {valid_loss}, valid_acc: {valid_acc}')\n",
        "                    torch.save(self, f'checkpoints/feedforward_{step}.chkpts')\n",
        "                    print(f'saved model to checkpoints/feedforward_{step}.chkpts')\n",
        "                    train_log[f'checkpoints/feedforward_{step}.chkpts'] = [valid_loss, valid_acc]\n",
        "\n",
        "        pickle.dump(train_log, open('checkpoints/train_log.dict', 'wb+'))\n",
        "\n",
        "        return train_loss_history, valid_loss_history\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        self.eval()\n",
        "        criterion = F.nll_loss\n",
        "\n",
        "        correct, total = 0, 0\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in data:\n",
        "                y_pred = self(x)\n",
        "                loss = criterion(y_pred, y)\n",
        "                loss_list.append(torch.mean(loss).item())\n",
        "                correct += torch.sum((torch.argmax(y_pred, dim = 1) == y).float())\n",
        "                total += y.size(0)\n",
        "            return sum(loss_list) / len(loss_list), correct / total\n",
        "\n",
        "train_data, valid_data, test_data = modify_dataset_for_ffn(dataset)\n",
        "\n",
        "model = FeedForwardNetwork(32)\n",
        "loss, acc = model.evaluate(train_data)\n",
        "\n",
        "train_loss_history, valid_loss_history = model.train_model(train_data, valid_data)\n",
        "\n",
        "plot_loss_history(train_loss_history, valid_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def find_best_model():\n",
        "    train_log = pickle.load(open('checkpoints/train_log.dict', 'rb'))\n",
        "    lst = []\n",
        "\n",
        "    for k, v in train_log.items():\n",
        "        lst.append((k, v))\n",
        "\n",
        "    path_to_model = max(lst, key = lambda x:x[1][1])[0]\n",
        "\n",
        "    return torch.load(path_to_model)\n",
        "\n",
        "model = find_best_model()\n",
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RecurrentNeuralNetwork(nn.Module):\n",
        "    def __init__(self, hidden_size, batch_first = True):\n",
        "        super(RecurrentNeuralNetwork, self).__init__()\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "        self.i2h = nn.Linear(len(alphabets), hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, len(languages))\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # x: (batch_size, max_length, len(alphabets))\n",
        "        hidden = F.tanh(self.i2h(x) + self.h2h(hidden)) # hidden: (batch_size, hidden_size)\n",
        "        if self.batch_first:\n",
        "            output = self.h2o(hidden)\n",
        "            output = F.log_softmax(output, dim = -1)\n",
        "        else:\n",
        "            output = F.log_softmax(self.h2o(hidden), dim = 0)\n",
        "        # output.shape: batch_size, output_size\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.hidden_size)\n",
        "\n",
        "    def train_model(self, train_data, valid_data, epochs = 100, learning_rate = 0.001, print_every = 1000):\n",
        "        criterion = F.nll_loss\n",
        "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "        step = 0\n",
        "        train_loss_history = []\n",
        "        valid_loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in train_data:\n",
        "                step += 1\n",
        "                # x: (batch_size, max_length, len(alphabets))\n",
        "                if self.batch_first:\n",
        "                    x = x.transpose(0, 1)\n",
        "                # x: (max_length, batch_size, len(alphabets))\n",
        "                hidden = self.init_hidden()\n",
        "                for char in x:\n",
        "                    output, hidden = self(char, hidden)\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                mean_loss = torch.mean(loss).item()\n",
        "\n",
        "                if step % print_every == 0 or step == 1:\n",
        "                    train_loss_history.append(mean_loss)\n",
        "                    valid_loss, valid_acc = self.evaluate(valid_data)\n",
        "                    valid_loss_history.append(valid_loss)\n",
        "                    print(f'[Epoch {epoch}, Step {step}] train loss: {mean_loss}, valid loss: {valid_loss}, valid_acc: {valid_acc}')\n",
        "\n",
        "        return train_loss_history, valid_loss_history\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        self.eval()\n",
        "        criterion = F.nll_loss\n",
        "\n",
        "        correct, total = 0, 0\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in data:\n",
        "                # x: (batch_size, max_length, len(alphabets))\n",
        "                if self.batch_first:\n",
        "                    x = x.transpose(0, 1)\n",
        "                # x: (max_length, batch_size, len(alphabets))\n",
        "                hidden = self.init_hidden()\n",
        "                for char in x:\n",
        "                    output, hidden = self(char, hidden)\n",
        "                loss = criterion(output, y)\n",
        "\n",
        "                loss_list.append(torch.mean(loss).item())\n",
        "                correct += torch.sum((torch.argmax(output, dim = 1) == y).float())\n",
        "                total += y.size(0)\n",
        "            return sum(loss_list) / len(loss_list), correct / total\n",
        "\n",
        "rnn = RecurrentNeuralNetwork(128)\n",
        "train_loss_history, valid_loss_history = rnn.train_model(train_dataset, valid_dataset)\n",
        "\n",
        "plot_loss_history(train_loss_history, valid_loss_history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
