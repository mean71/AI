{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMdOPzmklaJj"
      },
      "source": [
        "오늘은 지금까지 배운 전반부 딥러닝에서의 내용을 총체적으로 활용하여 문제를 직접 a부터 z까지 해결해보는 시간입니다.\n",
        "\n",
        "오늘 다룰 데이터는 names 안에 있는 각국의 이름을 받아, 이름의 국적을 추론하는 뉴럴 넷을 만드는 것입니다. 방법은 자유이나, 다음의 두 가지 방법을 꼭 포함시켜 구현하고, 성능 측정/튜닝의 과정을 거치세요.\n",
        "\n",
        "1. Feedforward Network(MLP)를 사용하여 해볼 것.\n",
        "  -  Feedforward Network를 이용하여 할 경우, input size가 이름에 따라 다른데 이를 어떤 식으로 다룰지 생각해 볼 것\n",
        "2. RNN을 이용하여 해볼 것.\n",
        "  - torch.DataLoader을 사용할 경우 batching을 하게 되는데, 이 경우 위 1에서 써 있는 문제와 비슷한 문제가 있으며 해결 또한 비슷하게 할 수 있음. torch.DataLoader을 쓸 때와 쓰지 않을 때를 비교해볼 것.\n",
        "  - 어제는 영어 알파벳만 사용하였는데, 다른 알파벳까지 사용할 경우 성능이 올라가는지 내려가는지 관찰해볼 것.\n",
        "  - 데이터에서 관찰하지 못한 알파벳들을 OOV로 처리하는 방식을 시도할 경우 성능이 올라가는지 내려가는지 관찰해볼 것\n",
        "\n",
        "성능 측정을 위해서 train-valid-test를 적절히 나누고, valid에서의 loss와 정확도를 측정하면서 학습을 적절할 때 멈추거나 할 것.\n",
        "\n",
        "튜닝을 위해서는 다음의 방법들을 사용할 수 있습니다.\n",
        "\n",
        "- hyperparameter을 다양하게 시도해볼 것\n",
        "- 뉴럴 넷 구조를 조금씩 바꿔볼 것\n",
        "- 데이터를 더 만들어 볼 것"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Nk4sis3qk2Y6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]]])\n",
            "tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])\n",
            "torch.Size([10]) torch.Size([32])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 164\u001b[0m\n\u001b[0;32m    161\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(n_letters, n_hidden, n_categories)\n\u001b[0;32m    162\u001b[0m predict_nationality(rnn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mang\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 164\u001b[0m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Create a dataset and data loader # 데이터셋과 데이터로더 생성\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# dataset = TensorDataset(X_tensor, y_tensor) # 입력 데이터(X_tensor)와 정답 데이터(y_tensor)로 텐서 데이터셋 생성\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# batch_size = 32 # 한 번에 모델에 넣는 데이터 묶음 크기 (미니배치)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # 배치로 데이터를 묶어서 모델에 전달할 수 있도록 데이터로더 생성, 셔플을 통해 데이터 순서를 무작위로 섞음\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[30], line 125\u001b[0m, in \u001b[0;36mRNN.train_model\u001b[1;34m(self, train_data, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# 정확도 계산\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted\u001b[38;5;241m.\u001b[39mshape, label_indices\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 125\u001b[0m correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel_indices\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    126\u001b[0m total_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    128\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(output, label)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import glob\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_lowercase + '?_'\n",
        "category_names = {} # category_names: dict[str, list[str]]\n",
        "all_categories = [] # all_categories: list[str]\n",
        "\n",
        "def idx_tensor(idx, N):\n",
        "    return torch.tensor([1 if i==idx else 0 for i in range(N)])\n",
        "\n",
        "def name_to_tensor(name):\n",
        "    res = torch.zeros(len(name), len(all_letters))\n",
        "    for idx, letter in enumerate(name):\n",
        "        res[idx][all_letters.find(letter)] = 1\n",
        "    # print(name, res, res.shape)\n",
        "    return res.squeeze(dim = 1)\n",
        "\n",
        "def input_padding(word):\n",
        "    return word + '_'*(10 - len(word)) if len(word)!=10 else word\n",
        "\n",
        "def preprocessing_data(batch_size = 32):\n",
        "    files = glob.glob('./data/names/*.txt')\n",
        "    # print(f'{len(files)}fiels',*files,sep='\\n')\n",
        "    # max_length = []\n",
        "    \n",
        "    for file in files:\n",
        "        with open(file) as f:\n",
        "            words = f.read().strip().split('\\n')\n",
        "        lang = file.split('\\\\')[-1].split('.')[0]\n",
        "        all_categories.append(lang)\n",
        "\n",
        "        # print(len([n for n in map(lambda n: ''.join(c if c in all_letters else '?' for c in n.lower()), words) if len(n)<11]), end='=')\n",
        "        # print(len([n for n in map(lambda n: ''.join(c if c in all_letters else '?' for c in n.lower()), words) if len(n)<15]))\n",
        "        words = [input_padding(n) for n in map(lambda n: ''.join(c if c in string.ascii_lowercase else '?' for c in n.lower()), words) if len(n)<11]\n",
        "\n",
        "        category_names[lang] = words\n",
        "        # print(f'{lang}: {len(names)} |', *names[0:10])\n",
        "    # print(max_length)\n",
        "    # print(all_categories)\n",
        "    # print(category_names)\n",
        "\n",
        "    x,y = [],[]\n",
        "    for lang, words in category_names.items():\n",
        "        for name in words:\n",
        "            x.append(name_to_tensor(name))\n",
        "            y.append(idx_tensor( all_categories.index(lang), len(all_categories) ))\n",
        "    \n",
        "    dataset = TensorDataset(torch.stack(x), torch.stack(y))\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "    return dataloader\n",
        "\n",
        "def plot_loss_history(loss_history, val_loss_history=None): # 훈련손실, 검증손실\n",
        "    plt.figure(figsize=(10, 6)) \n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history, label='Training Loss', color='blue', marker='o')\n",
        "    if val_loss_history is not None: plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, label='Validation Loss', color='orange', marker='x')\n",
        "    plt.title('Loss History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(range(1, len(loss_history) + 1)) # x축 눈금 설정\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class RNN(nn.Module): # MLP\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.input_hidden = nn.Linear(input_size, hidden_size)\n",
        "        self.hidden_hidden = nn.Linear(hidden_size, hidden_size)\n",
        "        self.hidden_output = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU\n",
        "        self.softmax = nn.LogSoftmax(dim = 0)\n",
        "        self.optimizer = optim.Adam\n",
        "        self.loss = torch.nn.NLLLoss()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        hidden = F.tanh(self.input_hidden(input) + self.hidden_hidden(hidden))\n",
        "        output = self.hidden_output(hidden)\n",
        "        # print('after h2o', output.shape)\n",
        "        # output = self.relu(output)\n",
        "        # print('after Relu', output.shape)\n",
        "        output = self.softmax(output)\n",
        "        # print('after softmax', output.shape)\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.hidden_size) # 히든초기화\n",
        "\n",
        "    def train_model(self, train_data, learning_rate = 0.001, epochs = 20):\n",
        "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            running_loss = 0.0\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0\n",
        "            \n",
        "            for input, label in train_data:\n",
        "                hidden = self.initHidden()\n",
        "                print(input)\n",
        "                print(label)\n",
        "                \n",
        "                for char in input:\n",
        "                    output, hidden = self(char, hidden) # nn.Module 자체에 이러고 넣으면 forward를 호출해서 output과 hiden을 반환, 그냥 그렇다.\n",
        "                    # print(output.shape)\n",
        "                \n",
        "                # print(output.shape, label.shape, input.shape)\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                \n",
        "                label_indices = torch.argmax(label, dim=1) # 레이블 인덱스 추출\n",
        "                \n",
        "                # 정확도 계산\n",
        "                print(predicted.shape, label_indices.shape)\n",
        "                correct_predictions += (predicted == label_indices).sum().item()\n",
        "                total_predictions += label.size(0)\n",
        "                \n",
        "                loss = self.loss(output, label)\n",
        "                loss_history.append(torch.log(torch.mean(loss)).item())\n",
        "                # print(torch.mean(loss).item())\n",
        "\n",
        "                if len(loss_history) % 1000 == 0:\n",
        "                    print(torch.mean(loss).item())\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item()\n",
        "            \n",
        "            epoch_loss = running_loss / len(train_data)\n",
        "            accuracy = correct_predictions / total_predictions  # 정확도 계산\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.5f}, Accuracy: {accuracy:.5f}')  # 정확도 출력\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "def predict_nationality(model, word):\n",
        "    hidden = model.initHidden()\n",
        "    word_tensor = name_to_tensor(input_padding(word))\n",
        "    for char_tensor in word_tensor:\n",
        "        output, hidden = rnn(char_tensor.unsqueeze(0), hidden)\n",
        "    # print(output.shape)\n",
        "    return output\n",
        "\n",
        "\n",
        "dataset = preprocessing_data(batch_size=32)\n",
        "n_letters = len(all_letters)\n",
        "n_hidden = 32\n",
        "n_categories = len(all_categories)\n",
        "\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\n",
        "predict_nationality(rnn, 'ang')\n",
        "\n",
        "rnn.train_model(dataset, learning_rate = 0.001, epochs = 20)\n",
        "print(1)\n",
        "\n",
        "# Create a dataset and data loader # 데이터셋과 데이터로더 생성\n",
        "# dataset = TensorDataset(X_tensor, y_tensor) # 입력 데이터(X_tensor)와 정답 데이터(y_tensor)로 텐서 데이터셋 생성\n",
        "# batch_size = 32 # 한 번에 모델에 넣는 데이터 묶음 크기 (미니배치)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # 배치로 데이터를 묶어서 모델에 전달할 수 있도록 데이터로더 생성, 셔플을 통해 데이터 순서를 무작위로 섞음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 데이터 준비\n",
        "training_data = []\n",
        "for category, words in category_names.items():\n",
        "    category_index = category_to_index(category)\n",
        "    for name in words:\n",
        "        name_tensor = name_to_tensor(name)\n",
        "        training_data.append((name_tensor, category_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def letter2tensor(letter):\n",
        "    [(res:=[1 if i==x else 0 for x in range(26)]) for i,j in enumerate(string.ascii_lowercase) if j==letter]\n",
        "    return torch.tensor(res)\n",
        "\n",
        "z_tensor = letter2tensor('z')\n",
        "print(z_tensor.shape) # (26,1)\n",
        "print(z_tensor)\n",
        "# a_tensor = torch,tensor([1,0,...,0])\n",
        "\n",
        "def word2tensor(word):\n",
        "    res = torch.zeros(len(word), 1, len(all_letters))\n",
        "    for idx, char in enumerate(word):\n",
        "        res[idx] = letter2tensor(char)\n",
        "    return res.squeeze(dim=1)\n",
        "print(word2tensor('abc').shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
