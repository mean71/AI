{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R1ybKJ7JUK7"
      },
      "source": [
        "Pytorch 설치 및 확인\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC7h8bMATYj",
        "outputId": "618b0670-eeea-427f-f982-9834d1cd501f"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_C7Xg6KJjm7",
        "outputId": "14f9c24d-4d2f-4ebb-ed00-e43133a9db39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__) # 현재 설치된 PyTorch 버전을 출력합니다.\n",
        "print(torch.cuda.is_available()) # CUDA(병렬 컴퓨팅 플랫폼)가 사용 가능한지 확인하고 결과를 출력합니다.\n",
        "# torch.cuda.is_available() 함수는 PyTorch에서 CUDA(Compute Unified Device Architecture)를 사용할 수 있는지를 확인하는 방법\n",
        "# PyTorch와 같은 딥러닝 프레임워크에서 CUDA를 활용하면 모델 학습 및 추론 속도를 크게 향상가능\n",
        "# True: CUDA가 사용 가능함. 즉, NVIDIA GPU가 설치되어 있고, 적절한 드라이버와 CUDA Toolkit이 설치되어 있습니다.\n",
        "# False: CUDA가 사용 불가능함. 즉, GPU가 없거나 드라이버가 제대로 설치되지 않았습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAIaC3rWAp3O"
      },
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
        "\n",
        "### Tensor 만드는 법\n",
        "\n",
        "\n",
        "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등임.\n",
        "\n",
        "주요 속성들\n",
        "- dtype: 데이터 타입\n",
        "- device: gpu에 있는지, cpu에 있는지\n",
        "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "matrix = torch.tensor([[1.0,2.0], [3.0,4.0]])\n",
        "matrix2 = torch.tensor([[1,2,3], [3,4,5]])\n",
        "print(matrix.shape)\n",
        "print(matrix2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "matrix2 = [[1,2,3], [3,4,5]]\n",
        "lst = [matrix2, matrix2, matrix2, matrix2]\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0]), ...\n",
        "print(tensor_3d_2.shape)\n",
        "\n",
        "# tensor([[[1.],\n",
        "#          [2.]],\n",
        "#       \n",
        "#         [[3.],\n",
        "#          [4.]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOILb3NGJrx",
        "outputId": "24ac2ce8-f298-4465-bccd-975ebe338e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5.)\n",
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "tensor([[[1.],\n",
            "         [2.]],\n",
            "\n",
            "        [[3.],\n",
            "         [4.]]])\n"
          ]
        }
      ],
      "source": [
        "# 0-D Tensor\n",
        "scalar = torch.tensor(5.0) # 0차원tensor(scalar)생성\n",
        "print(scalar)  # tensor(5.)\n",
        "\n",
        "vector = torch.tensor([1.0, 2.0, 3.0]) # 1차원tensor(vector)생성\n",
        "print(vector)  # tensor([1., 2., 3.])\n",
        "\n",
        "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "print(matrix)\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "print(tensor_3d)\n",
        "# tensor([[[1.],\n",
        "#          [2.]],\n",
        "#\n",
        "#         [[3.],\n",
        "#          [4.]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMM5BWTmTSF"
      },
      "source": [
        "### torch.tensor의 주요 속성들\n",
        "\n",
        "- tensor.shape\n",
        "- tensor.size()\n",
        "- tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJg4nrPAMt7t",
        "outputId": "7cb2dde2-f0b3-4369-d518-188cee546837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1])\n"
          ]
        }
      ],
      "source": [
        "print(vector.shape)    # torch.Size([3])\n",
        "print(matrix.size())   # torch.Size([2, 2])\n",
        "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrqNxnpxMwE3",
        "outputId": "dc16d96b-540a-4f5a-c0fa-f9f45f46f3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.int32\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(vector.dtype)    # torch.float32\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "print(int_tensor.dtype)  # torch.int32  # in32 : 32자리 이진수공간 할당?\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "print(tensor_gpu.device)  # cuda:0 or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXHR4BemnJ1"
      },
      "source": [
        "### torch.tensor 만드는 방법\n",
        "\n",
        "- torch.tensor(data)\n",
        "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
        "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
        "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
        "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴. # 유니폼디쓰리부션?\n",
        "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬. # identity 매트릭스 행렬곱시 그대로 나옴 # 자연어용?\n",
        "- 이외에도 많이 쓰이는 함수들\n",
        "  * torch.arange: range() 함수와 매우 비슷하다.\n",
        "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HuUTrq-ekUmh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([0, 0, 0])\n",
            "torch.Size([0, 0, 0])\n",
            "torch.Size([1, 2])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "list_tensor = torch.tensor([1, 2, 3])\n",
        "tuple_tensor = torch.tensor((4, 5, 6))\n",
        "\n",
        "zeros = torch.zeros((2,3))\n",
        "zeros1 = torch.zeros(2,3)\n",
        "zeros2 = torch.zeros(((0, 0, 0)))\n",
        "zeros3 = torch.zeros((([0, 0, 0])))\n",
        "zeros4 = torch.zeros((1, 2))\n",
        "ones = torch.ones((2, 3))\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)  # 3x3 Identity matrix\n",
        "\n",
        "normal = torch.randn((2, 3))  # Normal distribution\n",
        "# print(normal)\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2) # range(0,10,2)\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
        "\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "\n",
        "print(zeros.shape)\n",
        "print(zeros1.shape)\n",
        "print(zeros2.shape)\n",
        "print(zeros3.shape)\n",
        "print(zeros4.shape)\n",
        "print(ones)\n",
        "# print(rand)\n",
        "# print(eye)\n",
        "# print(arange_tensor)\n",
        "# print(linspace_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Pf1-B3vOVS"
      },
      "source": [
        "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2, 3])\n"
          ]
        }
      ],
      "source": [
        "print(torch.tensor((((2,3)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 3\n"
          ]
        }
      ],
      "source": [
        "a=((((((((((2,3))))))))))\n",
        "\n",
        "while not isinstance(a[0],int):\n",
        "    assert isinstance(a, (tuple,list)), \"not a list or tuple\"\n",
        "    a = a[0]\n",
        "print(a[0],a[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mUiK8UuXpHfE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
          ]
        }
      ],
      "source": [
        "# write your code here\n",
        "class pytorch():\n",
        "    def nested_list(shape, value=0):\n",
        "        \n",
        "        while not isinstance(a[0],int):\n",
        "            assert isinstance(a, (tuple,list)), \"not a list or tuple\"\n",
        "            a = a[0]\n",
        "        return \n",
        "    def random_nested_list():\n",
        "        return \n",
        "    def torch_zeros(self, *arg):\n",
        "        return torch.tensor()\n",
        "    def torch_ones(self,):\n",
        "        return torch.tensor()\n",
        "    def torch_rand(self,):\n",
        "        return torch.tensor()\n",
        "    def torch_arange(self, end=None, start=None, step=1):\n",
        "        return torch.tensor()\n",
        "    def torch_lnspace(self, end=None, start=None, step=1):\n",
        "        return torch.tensor()\n",
        "    @staticmethod\n",
        "    def torch_eye(x):\n",
        "        return torch.tensor([[0 if i != j else 1 for j in range(x)] for i in range(x)])\n",
        "    def __str__():\n",
        "        return torch.tensor()\n",
        "\n",
        "py = pytorch()\n",
        "# py_zeros = py.torch_zeros((2,3))\n",
        "# print(*py_zeros, sep='\\n')\n",
        "# py_ones = py.torch_ones((2,3))\n",
        "# print(*py_ones, sep='\\n')\n",
        "# py_rand = py.torch_rand((2,3))\n",
        "# print(py_rand, sep='\\n')\n",
        "py_eye = py.torch_eye(10)\n",
        "print(py_eye)\n",
        "# py_arange = py.torch_arange(start=0 ,end=10, step=2)\n",
        "# print(*py_arange, sep='\\n')\n",
        "# py_lnspace = py.torch_lnspace(0, 1, 5)\n",
        "# print(*py_lnspace, sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n",
            "tensor([[[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]]])\n",
            "tensor([[[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]]])\n",
            "tensor([[[0.7112, 0.3081, 0.0491, 0.7796],\n",
            "         [0.6231, 0.2741, 0.7289, 0.0334],\n",
            "         [0.4788, 0.1130, 0.0838, 0.9905]],\n",
            "\n",
            "        [[0.8330, 0.7324, 0.4121, 0.8628],\n",
            "         [0.5409, 0.0022, 0.8721, 0.0515],\n",
            "         [0.8311, 0.2582, 0.5533, 0.9722]]])\n",
            "tensor([[[ 0.5515,  0.7149, -0.4704, -1.2368],\n",
            "         [ 0.7926, -0.2710, -1.0941,  0.0809],\n",
            "         [-0.8256, -1.0248,  0.0860,  1.8237]],\n",
            "\n",
            "        [[-1.1565, -0.4196, -0.0352, -0.1363],\n",
            "         [-0.7713,  1.6323,  0.9970, -0.9861],\n",
            "         [-0.1973,  0.5031, -0.1601, -1.2636]]])\n"
          ]
        }
      ],
      "source": [
        "# solution\n",
        "import random\n",
        "\n",
        "def nested_list(shape, value = 0):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [value for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
        "\n",
        "def random_nested_list(shape, sample_from, *args):\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [sample_from(*args) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:], sample_from, *args) for _ in range(l)]\n",
        "'''\n",
        "def random_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.random() for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
        "'''\n",
        "def randomn_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.gauss(0, 1) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
        "\n",
        "def zeros(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 0))\n",
        "def ones(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 1))\n",
        "def rand(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.random))\n",
        "def randn(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.gauss, 0, 1))\n",
        "def eyes(n):\n",
        "    return torch.tensor([[0 if i != j else 1 for j in range(n)] for i in range(n)])\n",
        "\n",
        "print(nested_list((2, 3, 4), value = 0))\n",
        "print(zeros((2,3,4)))\n",
        "print(ones((2,3,4)))\n",
        "print(rand((2,3,4)))\n",
        "print(randn((2,3,4)))\n",
        "# print(eyes(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpAlToLo7wq"
      },
      "source": [
        "### torch.tensor끼리의 연산\n",
        "\n",
        "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨.  \n",
        "엘레멘트 와이즈? 와이드로 하면 행렬?의 차원이 같으면 원소간 연산이 기본"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "JsvDLKcKkd2e"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0]) # tensor([1., 2., 3.]) \n",
        "b = torch.tensor([4.0, 5.0, 6.0]) # tensor([4., 5., 6.])\n",
        "\n",
        "add = a + b  # tensor([5., 7., 9.])\n",
        "sub = a - b  # tensor([-3., -3., -3.])\n",
        "\n",
        "mul = a * b  # tensor([ 4., 10., 18.])\n",
        "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
        "\n",
        "exp = a ** 2  # tensor([1., 4., 9.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAa8J66qkjhp"
      },
      "outputs": [],
      "source": [
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "'''\n",
        "1 2  5 6\n",
        "3 4  7 8\n",
        "\n",
        "1 3\n",
        "2 4\n",
        "'''\n",
        "'''\n",
        "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
        "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
        "'''\n",
        "matmul = torch.matmul(matrix_a, matrix_b)\n",
        "\n",
        "# tensor([[19, 22],\n",
        "#         [43, 50]])\n",
        "\n",
        "elem_mul = matrix_a * matrix_b\n",
        "# tensor([[ 5, 12],\n",
        "#         [21, 32]])\n",
        "\n",
        "transposed = torch.transpose(matrix_a, 0, 1)\n",
        "# tensor([[1, 3],\n",
        "#         [2, 4]])\n",
        "\n",
        "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n",
        "# 잡학: torch.transpose() 텐서의 차원을 바꾸는 메서드(행렬 전치)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Mjhn93pEQ7"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
        "\n",
        "브로드캐스팅 규칙:\n",
        "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
        "\n",
        "예를 들어서,\n",
        "\n",
        "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "VolcxkK_krYh"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])    # Shape: (2, 3,)\n",
        "b = torch.tensor([1, 2, 3])                 # Shape: (3,)\n",
        "b = torch.tensor([[1, 2, 3]])               # Shape: (1, 3,)\n",
        "b = torch.tensor([[1, 2, 3], [1, 2, 3]])    # Shape: (2, 3,)\n",
        "\n",
        "broadcast_add = a + b  # Shape: (2, 3)\n",
        "# tensor([[2, 4, 6],\n",
        "#         [5, 7, 9]])\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])                   # Shape: (3, 1)\n",
        "b = torch.tensor([4, 5, 6])                         # Shape: (3,)\n",
        "b = torch.tensor([[4, 5, 6]])                       # Shape: (1, 3,)\n",
        "b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]]) # Shape: (3, 3,)\n",
        "a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) # Shape: (3, 3)\n",
        "a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\n",
        "# To make shapes compatible:\n",
        "# a: (3, 1) -> (3, 3)\n",
        "# b: (3,)   -> (1, 3) -> (3, 3)\n",
        "\n",
        "broadcast_mul = a * b  # Shape: (3, 3)\n",
        "# tensor([[ 4,  5,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [12, 15, 18]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN5jjYflr1oE"
      },
      "source": [
        "### 이 외 tensor operation들\n",
        "\n",
        "- Slicing / Indexing\n",
        "- Reshaping\n",
        "- Concatenation / Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz0GLy02kt-Y"
      },
      "outputs": [],
      "source": [
        "# slicing / indexing\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Basic indexing\n",
        "element = tensor[1, 2]  # tensor(6)\n",
        "\n",
        "# Slicing\n",
        "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
        "                            #         [5, 6],\n",
        "                            #         [8, 9]])\n",
        "\n",
        "# Advanced indexing with masks\n",
        "mask = tensor > 5\n",
        "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiCkGFvjsnBq"
      },
      "outputs": [],
      "source": [
        "# reshaping\n",
        "\n",
        "tensor = torch.arange(0, 12)\n",
        "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
        "                                   #         [ 4,  5,  6,  7],\n",
        "                                   #         [ 8,  9, 10, 11]])\n",
        "\n",
        "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
        "\n",
        "# tensor.permute\n",
        "tensor = torch.randn(2, 3, 4)\n",
        "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
        "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIHxbxPTs4Yx"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Concatenate along existing dimension\n",
        "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Stack along a new dimension\n",
        "stack = torch.stack((a, b), dim=0)\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 5, 6]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCXr7r3Kr__Z"
      },
      "source": [
        "### 수학적 함수들\n",
        "\n",
        "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
        "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
        "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "UsFdYgJws8kL"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([-1.0, -2.0, 3.0])\n",
        "\n",
        "abs_a = torch.abs(a)               # tensor([1., 2., 3.])\n",
        "# sqrt_a = torch.sqrt(a)           # tensor([   nan,    nan, 1.7321])\n",
        "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
        "exp_a = torch.exp(a)               # tensor([0.3679, 0.1353, 20.0855])\n",
        "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "EYIgfoa7s-gX"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
        "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmMzGqgOs_kk"
      },
      "outputs": [],
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2,3)\n",
        "\n",
        "sum_all = torch.sum(tensor)          # tensor(10)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6, 8]) (3,)\n",
        "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([6, 12]) (2,)\n",
        "\n",
        "mean_all = torch.mean(tensor.float(), dim=1)  # tensor(2.5000)\n",
        "print(mean_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLsS1TKhtAn2"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([2, 2, 2])\n",
        "\n",
        "greater = a > b  # tensor([False, False, True])\n",
        "equal = a == b   # tensor([False, True, False])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5udXG_t6Jv"
      },
      "source": [
        "## Pytorch로 다시 해 보는 선형회귀\n",
        "\n",
        "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
        "\n",
        "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
        "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
        "\n",
        "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeaB0-MIwM3h",
        "outputId": "45447762-89bf-4b28-d390-c732ae2db2f5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m true_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# 실제 바이어스\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generate data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 100 samples, single feature # 평균이 0이고 표준편차가 10인 정규 분포에서 샘플링\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m true_w \u001b[38;5;241m*\u001b[39m X \u001b[38;5;241m+\u001b[39m true_b \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# 노이즈를 추가하여 y 값을 생성\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# w,b 2개의 파라미터 텐서 2개 생성\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# y값 생성 과정\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 이 텐서가 학습 가능하게 requires_grad = True로 설정\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0  # 실제 가중치\n",
        "true_b = 1.0  # 실제 바이어스\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature # 평균이 0이고 표준편차가 10인 정규 분포에서 샘플링\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 0.2  # 노이즈를 추가하여 y 값을 생성\n",
        "# w,b 2개의 파라미터 텐서 2개 생성\n",
        "\n",
        "# y값 생성 과정\n",
        "# y[0] = true_w * X[0] + true_b + torch.randn(1, 1) * 2  # 샘플*가중치 + (노이즈)\n",
        "# y[1] = true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
        "# ...\n",
        "# y[99] = true_w * X[99] + true_b + torch.randn(1, 1) * 2\n",
        "\n",
        "# 이 텐서가 학습 가능하게 requires_grad = True로 설정\n",
        "w = torch.randn(1, 1, requires_grad=True) #  requires_grad=True, 가중치 w 무작위로 초기화, 기울기 계산할 수 있도록 설정\n",
        "b = torch.randn(1, requires_grad=True) # 바이어스 b 무작위로 초기화, 기울기 계산할 수 있도록 설정\n",
        "\n",
        "learning_rate = 0.009 # 학습률 설정 # 이걸 올리면 보폭이 커진다? 가장 적당한 값을 찾아야 한다\n",
        "epochs = 5000 # 총 학습 에폭 수 설정\n",
        "\n",
        "for epoch in range(epochs): # 학습횟수 에폭스\n",
        "    # Forward pass: compute predicted y (예측된 y계산)\n",
        "    # 100,1 / 1 -> 100, 1 / 1, 1 -> 100, 1 /100, 1 # 100,1 곱하기 1,1 은 shape가\n",
        "    y_pred = X * w + b # y_pred: 100,1 # # 입력X * 가중치w + 바이어스b = 예측값\n",
        "\n",
        "    # Compute and print loss # 손실계산\n",
        "    loss = torch.mean((y_pred - y) ** 2) # 평균 제곱 오차(MSE) 손실을 계산 # 100, 1\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward() # 손실에 대한 기울기 계산\n",
        "\n",
        "    # Update parameters using gradient descent # 경량 경량화\n",
        "    with torch.no_grad(): # 기울기 업데이트 시 기울기를 추적하지 않도록 설정 기울기\n",
        "        w -= learning_rate * w.grad # w 업데이트\n",
        "        b -= learning_rate * b.grad # b 업데이트\n",
        "    # 보법을 옵티마이저?라 한다? 잘하기 쉽지않다 가져다 쓴다?\n",
        "    \n",
        "    # Zero gradients after updating # 업데이트 후  w,b의 그레디언트 기울기를 0으로 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # 100에폭마다 현재 파라미터와 손실을 출력.\n",
        "    if epoch % 200 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "        \n",
        "    # 로컬미니마이즈: 손실함수의 특정 지점, 주변보다 손실값이 낮지만, 전체 함수에서 최저점(글로벌미니마이즈)이 아닌 곳\n",
        "    #   그래디언트가 로컬미니마이즈에 빠지면 더이상 파라미터를 업데이트 하지못해 최적화가 안되는 구조\n",
        "    #   특히 비선형함수에서 발생 가능\n",
        "    #   해결법:\n",
        "    #       모멘텀(Momentum): 이전 기울기를 반영, 업데이트 방향을 조정함으로써 로컬 미니마를 피하도록 유도\n",
        "    #       학습률 조정: 학습률을 조금씩 감소시키거나, 동적으로 조정하는 기법으로 최적화 성능 개선가능\n",
        "    #       다양한 초기화: 파라미터를 여러 번 다른 초기값으로 초기화 하여 최적화 과정을 반복함으로 로컬미니마에 빠질 가능성을 감소"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2IbgzEsuXLv"
      },
      "source": [
        "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDFD6NNUuW5S"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlzOMd44rinI"
      },
      "source": [
        "### 선형회귀 조금 더 해보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgue321swjEE"
      },
      "source": [
        "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
        "\n",
        "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Pe5Uqs91xjtC",
        "outputId": "cee17ffe-63b4-4764-f0d6-5aa7aa01c70f"
      },
      "outputs": [],
      "source": [
        "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROZ96vR1wI5h"
      },
      "source": [
        "### 뒤에서 할 내용 미리 살짝 엿보기 - Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR7URmNB0e7n",
        "outputId": "84328edb-722a-45bf-e4e1-1354937da925"
      },
      "outputs": [],
      "source": [
        "# 이번에는 adam optimizer를 한번 사용해보자.\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhIwbEiQ6meH"
      },
      "source": [
        "자꾸 local minima 어딘가에 빠지는 것 같다. 이걸 수정하기 위해서, 일정 횟수 이상 바뀌지 않으면 noise를 주는 방식을 생각해보자.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-JPhYZV4PXg",
        "outputId": "dc154f03-4847-471f-c6a2-0d8f8446cd3e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "torch.manual_seed(42)  # 재현성을 위해 시드 설정\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 2  # Add noise\n",
        "\n",
        "# requires_grad = True로 해야 학습이 가능\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "# 아담 옵티마이저 하이퍼파라미터 설정\n",
        "learning_rate = 0.005\n",
        "epochs = 10000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# 아담 옵티마이저를 위한 모멘트 변수 초기화\n",
        "m_w = torch.zeros_like(w)\n",
        "v_w = torch.zeros_like(w)\n",
        "m_b = torch.zeros_like(b)\n",
        "v_b = torch.zeros_like(b)\n",
        "\n",
        "# 아담 옵티마이저를 위한 시간 스텝 변수 초기화\n",
        "t = 0\n",
        "\n",
        "\n",
        "patience = 300  # 손실과 파라미터 변화가 임계값 이하로 유지되는 에포크 수\n",
        "threshold_loss = 1e-4  # 손실 변화 임계값\n",
        "threshold_w = 1e-4     # w 변화 임계값\n",
        "threshold_b = 1e-4     # b 변화 임계값\n",
        "\n",
        "\n",
        "loss_history = []\n",
        "w_history = []\n",
        "b_history = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass: compute predicted y\n",
        "    y_pred = X * w + b\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # 아담 옵티마이저 업데이트\n",
        "    with torch.no_grad():\n",
        "        t += 1  # 시간 스텝 증가\n",
        "\n",
        "        # w 파라미터 업데이트\n",
        "        m_w = beta1 * m_w + (1 - beta1) * w.grad\n",
        "        v_w = beta2 * v_w + (1 - beta2) * (w.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_w_hat = m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = v_w / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        w -= learning_rate * m_w_hat / (torch.sqrt(v_w_hat) + epsilon)\n",
        "\n",
        "        # b 파라미터 업데이트\n",
        "        m_b = beta1 * m_b + (1 - beta1) * b.grad\n",
        "        v_b = beta2 * v_b + (1 - beta2) * (b.grad ** 2)\n",
        "        # 편향 보정\n",
        "        m_b_hat = m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = v_b / (1 - beta2 ** t)\n",
        "        # 파라미터 업데이트\n",
        "        b -= learning_rate * m_b_hat / (torch.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "    # Gradients 초기화\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # 손실과 파라미터 값을 기록\n",
        "    loss_history.append(loss.item())\n",
        "    w_history.append(w.item())\n",
        "    b_history.append(b.item())\n",
        "\n",
        "    # Patience에 도달했는지 확인\n",
        "    if epoch >= patience :\n",
        "        # 최근 'patience' 에포크의 손실 변화 계산\n",
        "        recent_losses = loss_history[-patience:]\n",
        "        loss_deltas = [abs(recent_losses[i] - recent_losses[i-1]) for i in range(1, patience)]\n",
        "        max_loss_delta = max(loss_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 w 변화 계산\n",
        "        recent_ws = w_history[-patience:]\n",
        "        w_deltas = [abs(recent_ws[i] - recent_ws[i-1]) for i in range(1, patience)]\n",
        "        max_w_delta = max(w_deltas)\n",
        "\n",
        "        # 최근 'patience' 에포크의 b 변화 계산\n",
        "        recent_bs = b_history[-patience:]\n",
        "        b_deltas = [abs(recent_bs[i] - recent_bs[i-1]) for i in range(1, patience)]\n",
        "        max_b_delta = max(b_deltas)\n",
        "\n",
        "        # 변화가 모두 임계값 이하인 경우 노이즈 추가\n",
        "        if (max_loss_delta < threshold_loss) and (max_w_delta < threshold_w) and (max_b_delta < threshold_b):\n",
        "            print(f'\\nEpoch {epoch}: No significant updates in the last {patience} epochs. Adding noise to parameters.')\n",
        "            # 파라미터에 노이즈 추가\n",
        "            noise_w = torch.randn_like(w) * 0.1\n",
        "            noise_b = torch.randn_like(b) * 0.1\n",
        "            w.data += noise_w\n",
        "            b.data += noise_b\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "\n",
        "# 최종 파라미터 출력\n",
        "print(f'\\nFinal Parameters: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcrBxNp7LCB"
      },
      "source": [
        "## 딥러닝 들어가기\n",
        "\n",
        "\n",
        "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUTgBrCI7k8X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuEAYWDf7h6d"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "input_features = 20\n",
        "\n",
        "# Features: random numbers\n",
        "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "# Labels: sum of features > 0 => class 1, else class 0\n",
        "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
        "\n",
        "# Create a dataset and data loader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXe436j_7Uda",
        "outputId": "755e69c5-24e2-4a74-e012-cfdf1670df50"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.ReLU()                         # ReLU activation\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = input_features\n",
        "hidden_size = 64\n",
        "output_size = 1  # Binary classification\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()  # Combines a sigmoid layer and the BCELoss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update weights\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
        "    predicted_classes = (predictions >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5n0jb4vxWg"
      },
      "source": [
        "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YxkejqAw8--"
      },
      "outputs": [],
      "source": [
        "# write your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
