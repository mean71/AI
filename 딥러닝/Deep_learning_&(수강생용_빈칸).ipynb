{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvVeD4rs-uoN"
      },
      "source": [
        "## ① 인공 신경망에 대하여 🧠\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IOrbgYFKmFk"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-9.png' width = '600' border='0'></a>\n",
        "\n",
        "\n",
        "✏️ **학습 방법에 따라서 인공지능 → 머신러닝 → 딥러닝으로 구체화됩니다.**\n",
        "> **`인공지능`** </br>\n",
        "> 인공지능은 인간의 지능이 갖고 있는 기능을 갖춘 컴퓨터 시스템이며, 인간의 지능을 기계 등에 인공적으로 시연(구현)한 가장 큰 범주에 해당합니다. </br>일반적으로 범용 컴퓨터에 적용한다고 가정합니다.\n",
        "\n",
        "> **`머신러닝`** </br>기계 학습 또는 머신 러닝은 인공지능의 한 분야로, 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야를 말합니다.\n",
        "\n",
        "> **`딥러닝(심층학습)`** </br> 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화를 시도하는 기계 학습 알고리즘의 집합으로 정의됩니다. </br>큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있습니다. </br>딥 러닝은 특징 추출부터 패턴까지 모든 과정을 사람의 개입 없이 심층인공신경망을 토대로 학습방식을 구현하는 기술입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFamzTOZK_m"
      },
      "source": [
        "**인공 신경망(artificial neural network)**은 알고리즘의 플로우 차트(순서도)와 매우 유사합니다.\n",
        "\n",
        "다만, 훨씬 더 단순하죠. 오직 동그라미와 화살표만으로 이루어져 있고, 대칭적입니다.\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-10.jpeg' width='600' border='0'></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRwitWQJo9-"
      },
      "source": [
        "```\n",
        "🔎 인공 신경망의 하나의 동그라미를 확대해 봅시다.  \n",
        "\n",
        "```\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-11.png' width='600' border='0'></a>\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-12.png' border='0'></a>\n",
        "\n",
        "입력 x와 딥러닝을 통해 찾아지는 숫자 w가 서로 곱해지고 더해져서 하나의 숫자를 만들고 단순한 함수 f를 통과하여 다음 동그라미의 입력으로 넘어갑니다.\n",
        "\n",
        "이러한 구조를 **퍼셉트론(perceptron)**이라고 합니다.\n",
        "\n",
        "모든 인공지능 알고리즘은 이러한 구조로 이뤄져 있고,\n",
        "\n",
        "이러한 신경망을 통해 학습(learn)을 하는 것을 **딥러닝**이라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-Ju-SsEj3_"
      },
      "source": [
        "### 딥러닝이란?\n",
        "```\n",
        "🧐 이제 본격적으로 딥러닝에 대해 탐구해봅시다!\n",
        "```\n",
        "\n",
        "앞서, **인공 신경망을 통해 학습하는 과정**이 딥러닝이라는 것을 배웠습니다.\n",
        "\n",
        "기존 프로그램의 한계는 바로 사람이 짜기 때문에 생긴다는 것도 배웠죠!\n",
        "\n",
        "따라서 딥러닝은 기존 프로그래밍처럼 프로그래머가 정해진 규칙에 기반한 알고리즘을 지정해주지 않고    \n",
        "\n",
        "가장 단순한 구조의 플로우 차트에서 출발하여 **데이터 기반으로 스스로 배우는 과정**을 의미합니다.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJMKBGVXNREy"
      },
      "source": [
        "```\n",
        "🗂 데이터를 기반으로 스스로 학습하는 과정을 구체적으로 살펴볼까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGofK9F9Ivmh"
      },
      "source": [
        "<img src='https://datadiving.dothome.co.kr/Deep_1-13.jpeg' border='0'></a>\n",
        "\n",
        "임의의 w (weight) 로 구성된 인공 신경망에서 각 w 값을 키웠다가, 줄였다가 변화시키면서\n",
        "\n",
        "**최종 결과값이 기대하는 값에 가까워지는 방향**으로 w 를 학습하게 됩니다.  \n",
        "\n",
        "우리가 아는 알파고, 자율 주행, 아무리 복잡한 딥러닝 알고리즘도 이러한 단순 구조의 퍼셉트론들의 집합으로 이뤄져 있습니다.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R1ybKJ7JUK7"
      },
      "source": [
        "## Pytorch 설치 및 확인\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC7h8bMATYj",
        "outputId": "9ad8f36b-15bf-4d4c-9c27-39ec4f9a5425"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_C7Xg6KJjm7",
        "outputId": "0e01c475-5df1-4a66-ec6c-36f2b0b92ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAIaC3rWAp3O"
      },
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
        "\n",
        "\n",
        "### Tensor?\n",
        "\n",
        "먼저, Tensor에 대해 알아봅시다.\n",
        "\n",
        "```Tensor란?```  **다차원의 배열**을 뜻하는 말입니다.\n",
        "\n",
        "*배열의 차원에 따라 불리는 이름이 달라진다는 것, 모두 알고 계시죠?</br>\n",
        "0차원은 스칼라, 1차원은 벡터, 2차원은 메트릭스, 그 이상의 다차원은 아래 차원의 것을 모아놓은 배열인 것이라고 할 수 있는데요!</br> tensor는 스칼라, 벡터, 매트릭스 등의 데이터와 그 이상의 고차원 데이터도 포함하는 개념입니다.\n",
        "\n",
        "참고로 tensor에서 Rank는 그 데이터가 몇 차원의 배열인지를 의미합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzM9rYH8hPx"
      },
      "source": [
        "<img src='https://datadiving.dothome.co.kr/Deep_1-63.jpeg' width='900'  border='0'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYzdxecs8eY9"
      },
      "source": [
        "### Tensor 만드는 법 1\n",
        "\n",
        "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZPgQV3KoVgz",
        "outputId": "9069c33e-8689-4c64-f788-56850dfc94dd"
      },
      "outputs": [],
      "source": [
        "# 0-D Tensor\n",
        "# 텐서를 생성하는 함수를 작성하시오.\n",
        "scalar = torch.tensor(5.0)\n",
        "number = torch.tensor(1.0)\n",
        "print(scalar)  # tensor(5.)\n",
        "print(number) # tensor(1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTYsy2Uno0Av",
        "outputId": "76f10ab7-28b2-421f-f966-f921e2b301c6"
      },
      "outputs": [],
      "source": [
        "# 텐서를 생성하는 함수를 작성하시오.\n",
        "vector = torch.tensor([1.0, 2.0, 3.0])\n",
        "tuple_vector = torch.tensor((1, 2, 3))\n",
        "print(vector)  # tensor([1., 2., 3.])\n",
        "print(tuple_vector) # tensor([1, 2, 3,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJA3LCr1pbRU",
        "outputId": "5843b817-e3ee-43c0-8294-cda210e3467a"
      },
      "outputs": [],
      "source": [
        "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "matrix2 = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
        "print(matrix.shape)\n",
        "print(matrix2.shape)\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOILb3NGJrx",
        "outputId": "b8ba54ad-23e1-4741-ed58-d06c97e00ed5"
      },
      "outputs": [],
      "source": [
        "matrix2 = [[1, 2, 3], [3, 4, 5]]\n",
        "lst = [matrix2, matrix2, matrix2, matrix2]\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0]), ....\n",
        "print(tensor_3d_2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMM5BWTmTSF"
      },
      "source": [
        "### torch.tensor의 주요 속성들\n",
        "\n",
        "- tensor.shape\n",
        "- tensor.size()\n",
        "- tensor.dtype\n",
        "- device: gpu에 있는지, cpu에 있는지\n",
        "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "LJg4nrPAMt7t",
        "outputId": "d34998f0-5530-49e0-ea06-2a9dcf03499d"
      },
      "outputs": [],
      "source": [
        "#텐서의 크기를 출력해보는 코드를 작성하시오\n",
        "print(vector.shape)    # torch.Size([3])\n",
        "print(matrix.size())   # torch.Size([2, 2])\n",
        "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "BrqNxnpxMwE3",
        "outputId": "0ed75509-409b-4d1c-f136-5e6ada8516b6"
      },
      "outputs": [],
      "source": [
        "#텐서의 데이터 타입을 출력하는 코드를 작성하시오\n",
        "print(vector.dtype)    # torch.float32\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "print(int_tensor.dtype)  # torch.int32\n",
        "\n",
        "#텐서가 현재 gpu에 위치해 있는지 확인하는 코드를 작성하시오\n",
        "device = 'cuda'*torch.cuda.is_available() or 'cpu'\n",
        "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "print(tensor_gpu.device)  # cuda:0 or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXHR4BemnJ1"
      },
      "source": [
        "### torch.tensor 만드는 방법 2\n",
        "\n",
        "- torch.tensor(data)\n",
        "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
        "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
        "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
        "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴.\n",
        "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬.\n",
        "- 이외에도 많이 쓰이는 함수들\n",
        "  * torch.arange: range() 함수와 매우 비슷하다.\n",
        "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUTrq-ekUmh",
        "outputId": "03b06850-6dc8-4b77-963b-4180a818c2f5"
      },
      "outputs": [],
      "source": [
        "list_tensor = torch.tensor([1, 2, 3])\n",
        "tuple_tensor = torch.tensor((4, 5, 6))\n",
        "\n",
        "#값이 0인 텐서를 생성하는 코드를 작성하시오\n",
        "zeros = torch.zeros((2, 3))\n",
        "#값이 1인 텐서를 생성하는 코드를 작성하시오\n",
        "ones = torch.ones((2, 3))\n",
        "#값이 랜덤인 텐서를 생성하는 코드를 작성하시오\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)  # 3x3 Identity matrix\n",
        "print(zeros)\n",
        "print(ones)\n",
        "print(rand)\n",
        "print(eye)\n",
        "#랜덤값의 분포가 정규분포인 텐서를 생성하는 코드를 작성하시오\n",
        "normal = torch.randn((2, 3))  # Normal distribution\n",
        "\n",
        " # 값이 range(0, 10, 2) 인 텐서를 생성하는 코드를 작성하시오\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2)\n",
        " # # 0 0.25 0.5 0.75 1 처럼 등분으로 나뉘는 텐서를 생성하는 코드를 작성하시오\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
        "print(arange_tensor)\n",
        "print(linspace_tensor)\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Pf1-B3vOVS"
      },
      "source": [
        "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUiK8UuXpHfE",
        "outputId": "1ef5eeb6-f385-4cd5-c126-c640370d95f4"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "import random\n",
        "\n",
        "def nested_list(shape, value = 0):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [value for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
        "\n",
        "def random_nested_list(shape, sample_from, *args):\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [sample_from(*args) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:], sample_from, *args) for _ in range(l)]\n",
        "'''\n",
        "def random_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.random() for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
        "'''\n",
        "def randomn_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.gauss(0, 1) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
        "\n",
        "def zeros(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 0))\n",
        "\n",
        "def ones(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 1))\n",
        "\n",
        "def rand(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.random))\n",
        "\n",
        "def randn(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.gauss, 0, 1))\n",
        "\n",
        "print(nested_list((2, 3, 4), value = 0))\n",
        "print(zeros((2,3,4)) == torch.zeros((2,3,4)))\n",
        "print(ones((2,3,4)))\n",
        "print(rand((2,3,4)))\n",
        "print(randn((2,3,4)))\n",
        "\n",
        "def eyes(n):\n",
        "    lst = [[0 for i in range(n)] for j in range(n)]\n",
        "\n",
        "    lst = []\n",
        "    for i in range(n):\n",
        "        tmp = []\n",
        "        for j in range(n):\n",
        "            tmp.append(0)\n",
        "        lst.append(tmp)\n",
        "\n",
        "    for i in range(n):\n",
        "        lst[i][i] = 1\n",
        "\n",
        "    return torch.tensor(lst)\n",
        "\n",
        "print(eyes(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpAlToLo7wq"
      },
      "source": [
        "### torch.tensor끼리의 연산\n",
        "\n",
        "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsvDLKcKkd2e"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "add = a + b  # tensor([5., 7., 9.])\n",
        "sub = a - b  # tensor([-3., -3., -3.])\n",
        "\n",
        "mul = a * b  # tensor([ 4., 10., 18.])\n",
        "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
        "\n",
        "exp = a ** 2  # tensor([1., 4., 9.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAa8J66qkjhp",
        "outputId": "2f80b9fe-facb-43ae-a9ef-b5993d510666"
      },
      "outputs": [],
      "source": [
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "'''\n",
        "1 2  5 6\n",
        "3 4  7 8\n",
        "\n",
        "1 3\n",
        "2 4\n",
        "'''\n",
        "'''\n",
        "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
        "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
        "'''\n",
        "\n",
        "#행렬곱을 수행하는 코드\n",
        "matmul = torch.????(matrix_a, matrix_b)\n",
        "\n",
        "# tensor([[19, 22],\n",
        "#         [43, 50]])\n",
        "\n",
        "elem_mul = matrix_a * matrix_b\n",
        "# tensor([[ 5, 12],\n",
        "#         [21, 32]])\n",
        "\n",
        "# 축을 바꾸는 전치를 수행하는 코드\n",
        "transposed = torch.????(matrix_a, 0, 1)\n",
        "# 주로 딥러닝이나 배열 연산에서 사용되며, 다차원 배열 또는 텐서의 축을 바꿔주는 역할\n",
        "# tensor([[1, 3],\n",
        "#         [2, 4]])\n",
        "\n",
        "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Mjhn93pEQ7"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
        "\n",
        "브로드캐스팅 규칙:\n",
        "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
        "\n",
        "예를 들어서,\n",
        "\n",
        "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lumy89RRi-6p"
      },
      "outputs": [],
      "source": [
        "# shape (1, 3) / (3, 1)\n",
        "# dim = 0\n",
        "# 3 / 3\n",
        "# dim = 1\n",
        "# 3 / 3\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "VolcxkK_krYh",
        "outputId": "b451e523-4f87-4d88-edc1-1d3085dbae50"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])    # Shape: (2, 3,)\n",
        "b = torch.tensor([1, 2, 3])                 # Shape: (3,)\n",
        "b = torch.tensor([[1, 2, 3]])               # Shape: (1, 3,)\n",
        "b = torch.tensor([[1, 2, 3], [1, 2, 3]])    # Shape: (2, 3,)\n",
        "\n",
        "broadcast_add = a + b  # Shape: (2, 3)\n",
        "# tensor([[2, 4, 6],\n",
        "#         [5, 7, 9]])\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
        "b = torch.tensor([4, 5, 6])        # Shape: (3,)\n",
        "b = torch.tensor([[4, 5, 6]])        # Shape: (1, 3,)\n",
        "b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])        # Shape: (3, 3,)\n",
        "a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])  # Shape: (3, 3)\n",
        "a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\n",
        "# To make shapes compatible:\n",
        "# a: (3, 1) -> (3, 3)\n",
        "# b: (3,)   -> (1, 3) -> (3, 3)\n",
        "\n",
        "broadcast_mul = a * b  # Shape: (3, 3)\n",
        "# tensor([[ 4,  5,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [12, 15, 18]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rQSNtHjms5"
      },
      "source": [
        "### 오늘의 복습용 문제\n",
        "\n",
        "1. l = list of list of .... 를 받아서, 하는 함수 get_shape를 작성하세요.\n",
        "  - 여기서 torch.tensor(l).shape 과 같은 값을 가지는 리스트를 반환\n",
        "  - 만약 torch.tensor가 불가능하다면 False를 리턴\n",
        "2. l = list of list of ..., r = list of list of ... 두 input을 받아서, 아래와 같이 동작하는 함수 broadcasting을 작성하세요\n",
        "  - 브로드캐스팅이 될 때 각 l, r이 바뀌어야 하는 형태를 리턴\n",
        "  - 브로드캐스팅이 되지 않으면 False를 리턴\n",
        "\n",
        "[[1,2,3], [1,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAEIj1mbjmI-"
      },
      "outputs": [],
      "source": [
        "def get_shape(lst):\n",
        "    res = []\n",
        "    cur = lst\n",
        "\n",
        "    while isinstance(cur, list):\n",
        "        res.append(len(cur))\n",
        "        cur = cur[0]\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_shape(lst):\n",
        "    if not isinstance(lst, list):\n",
        "        return []\n",
        "    else:\n",
        "        shapes = []\n",
        "        for elem in lst:\n",
        "            shape = get_shape(elem)\n",
        "            if shape not in shapes:\n",
        "                shapes.append(shape)\n",
        "        if len(shapes) == 1:\n",
        "            return [len(lst)] + get_shape(lst[0])\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def fill(l, r):\n",
        "    \"\"\"If len(l) > len(r), fill 1 to r's front, so that len(l) == len(r),\n",
        "    If len(r) < len(l), do the opposite.\n",
        "    \"\"\"\n",
        "    if len(l) > len(r):\n",
        "        diff = len(l) - len(r)\n",
        "        r = [1 for _ in range(diff)] + r\n",
        "        return l, r\n",
        "    elif len(l) < len(r):\n",
        "        diff = len(r) - len(l)\n",
        "        l = [1 for _ in range(diff)] + l\n",
        "        return l, r\n",
        "    return l, r\n",
        "\n",
        "def expand_dimension(l, dim_idx, r_s):\n",
        "    \"\"\"\n",
        "    l = [[1,2,3,]] (shape 1, 3)\n",
        "    dim_idx = 0\n",
        "    r_s = 4\n",
        "    expand_dimension(l, 0, 4)\n",
        "    >> [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
        "\n",
        "    l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]] (shape 3, 1, 3 -> 3, 2, 3)\n",
        "    l[0] = [[1,2,3]], l[1], l[2] (shape 2, 3)\n",
        "    dim_idx = 1\n",
        "    r_s = 2\n",
        "    expand_dimension(l, 0, 2)\n",
        "    >> [[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]] (shape 3, 2, 3)\n",
        "\n",
        "    l / shape 4, 3, 2, 1, 2 -> 4, 3, 2, 5, 2\n",
        "    expand_dimension(l, 3, 5)\n",
        "    l[0], l[1], l[2], l[3] / shape 3, 2, 1, 2 -> 3, 2, 5, 2\n",
        "    expand_dimension(l[0], 2, 5)\n",
        "    expand_dimension(l[1], 2, 5)\n",
        "    expand_dimension(l[2], 2, 5)\n",
        "    expand_dimension(l[3], 2, 5)\n",
        "    \"\"\"\n",
        "    assert get_shape(l)[dim_idx] == 1, (get_shape(l), dim_idx)\n",
        "\n",
        "    if dim_idx == 0:\n",
        "        return [l[0] for _ in range(r_s)]\n",
        "    else:\n",
        "        return [expand_dimension(e, dim_idx - 1, r_s) for e in l]\n",
        "\n",
        "\n",
        "def broadcasting(l, r):\n",
        "    shape_l = get_shape(l)\n",
        "    shape_r = get_shape(r)\n",
        "\n",
        "    assert shape_l and shape_r\n",
        "\n",
        "    # 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때,\n",
        "    # 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "\n",
        "    # (2, 3) / (4, 5, 2, 3) -> (1, 1, 2, 3) / (4, 5, 2, 3)\n",
        "\n",
        "    l_is_bigger = False\n",
        "    r_is_bigger = False\n",
        "    diff = abs(len(shape_l) - len(shape_r))\n",
        "\n",
        "    if len(shape_l) > len(shape_r):\n",
        "        l_is_bigger = True\n",
        "    elif len(shape_l) < len(shape_r):\n",
        "        r_is_bigger = True\n",
        "\n",
        "    shape_l, shape_r = fill(shape_l, shape_r)\n",
        "\n",
        "    for _ in range(diff):\n",
        "        if l_is_bigger:\n",
        "            r = [r]  # r.shape: a1, a2, ... , an / [r].shape : 1, a1, a2, ... , an\n",
        "        elif r_is_bigger:\n",
        "            l = [l]\n",
        "\n",
        "    assert shape_l == get_shape(l)\n",
        "    assert shape_r == get_shape(r)\n",
        "\n",
        "    # 크기 맞추기: 각 차원에서 크기가 1인 텐서는\n",
        "    # 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "\n",
        "    dim_idx = 0\n",
        "\n",
        "    for l_s, r_s in zip(shape_l, shape_r):\n",
        "        if l_s != r_s:\n",
        "            if min(l_s, r_s) == 1:\n",
        "                if l_s == 1: #\n",
        "                    l = expand_dimension(l, dim_idx, r_s)\n",
        "                else: # r_s == 1\n",
        "                    r = expand_dimension(r, dim_idx, l_s)\n",
        "            else:\n",
        "                return False\n",
        "        dim_idx += 1\n",
        "\n",
        "    return l, r\n",
        "\n",
        "l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]]\n",
        "print(\"get_shape(I) :\", get_shape(l))\n",
        "\n",
        "r = [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
        "print(get_shape(r))\n",
        "\n",
        "# 3 1 3 / 4 3 -> 3 1 3 / 1 4 3 -> 3 1 3 / 3 4 3 -> 3 4 3 / 3 4 3\n",
        "# r = [[[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
        "r_ans = [[\n",
        "            [1,2,3,],\n",
        "            [1,2,3,],\n",
        "            [1,2,3,],\n",
        "            [1,2,3,]\n",
        "          ],\n",
        "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]],\n",
        "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
        "l_ans = [[[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]]]\n",
        "l, r = broadcasting(l, r)\n",
        "print(get_shape(l), get_shape(r))\n",
        "print(l_ans == l, r_ans == r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN5jjYflr1oE"
      },
      "source": [
        "### 이 외 tensor operation들\n",
        "\n",
        "- Slicing / Indexing\n",
        "- Reshaping\n",
        "- Concatenation / Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz0GLy02kt-Y"
      },
      "outputs": [],
      "source": [
        "# slicing / indexing\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Basic indexing\n",
        "element = tensor[1, 2]  # tensor(6)\n",
        "\n",
        "# Slicing\n",
        "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
        "                            #         [5, 6],\n",
        "                            #         [8, 9]])\n",
        "\n",
        "# Advanced indexing with masks\n",
        "mask = tensor > 5\n",
        "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiCkGFvjsnBq",
        "outputId": "76d39b6c-0a1a-4280-f827-97a19f263cd2"
      },
      "outputs": [],
      "source": [
        "# reshaping\n",
        "\n",
        "tensor = torch.arange(0, 12)\n",
        "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
        "                                   #         [ 4,  5,  6,  7],\n",
        "                                   #         [ 8,  9, 10, 11]])\n",
        "\n",
        "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
        "\n",
        "# tensor.permute\n",
        "tensor = torch.randn(2, 3, 4)\n",
        "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
        "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLZc1-Bo1xjv",
        "outputId": "73973602-592d-4fa0-8c40-f3da55c4bf51"
      },
      "outputs": [],
      "source": [
        "tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VIHxbxPTs4Yx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6]) \n",
            " tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Concatenate along existing dimension\n",
        "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Stack along a new dimension\n",
        "stack = torch.stack((a, b), dim=0)\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 5, 6]])\n",
        "\n",
        "stack = torch.stack((a, b), dim=1)\n",
        "# tensor([[1, 4],\n",
        "#         [2, 5],\n",
        "#         [3, 6]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCXr7r3Kr__Z"
      },
      "source": [
        "### 수학적 함수들\n",
        "\n",
        "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
        "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
        "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsFdYgJws8kL"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([-1.0, -2.0, 3.0])\n",
        "\n",
        "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
        "# sqrt_a = torch.sqrt(a)\n",
        "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
        "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
        "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYIgfoa7s-gX"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
        "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmMzGqgOs_kk"
      },
      "outputs": [],
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2,3)\n",
        "\n",
        "sum_all = torch.sum(tensor)          # tensor(10)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6, 8]) (3,)\n",
        "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([6, 12]) (2,)\n",
        "\n",
        "mean_all = torch.mean(tensor.float(), dim = 1)  # tensor(2.5000)\n",
        "print(mean_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLsS1TKhtAn2"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([2, 2, 2])\n",
        "\n",
        "greater = a > b  # tensor([False, False, True])\n",
        "equal = a == b   # tensor([False, True, False])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5udXG_t6Jv"
      },
      "source": [
        "## Pytorch로 다시 해 보는 선형회귀\n",
        "\n",
        "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
        "\n",
        "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
        "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
        "\n",
        "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rWCse3hg-8Vs"
      },
      "outputs": [],
      "source": [
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "# 표준 정규분포 텐서 랜덤 생성하는 코드가 뭐였죠?\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 0.2  # Add noise\n",
        "\n",
        "# y[0]= true_w * X[0] + true_b + torch.randn(1, 1) * 2\n",
        "# y[1]= true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
        "# ...\n",
        "# y[99]= true_w * X[99] + true_b + torch.randn(1, 1) * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMhLRtPy_SED"
      },
      "source": [
        "### requires_grad = True ??\n",
        "\n",
        "- 다음과 같이 경사하강법을 통해 학습을 할때 미분을 통해 나오는 '기울기' 값을 저장을 해야합니다.\n",
        "\n",
        "- requires_grad = True 를 해야 기울기가 저장됩니다.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-16-gradient_descent/pic1.png' width='500'  border='0'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nOG1PrLJ_OlA"
      },
      "outputs": [],
      "source": [
        "# requires_grad = True로 해야 학습이 가능\n",
        "\n",
        "# 기울기를 기록하는 코드 작성\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9gaSZBtBTyJ"
      },
      "source": [
        "### learning_rate??? 뭐였지??\n",
        "\n",
        "- lr (learning_rate): 학습율, 경사하강법에서 파라미터를 업데이트하는 정도를 조절하기 위한 변수,</br> *보통 0.0001~0.001의 학습율을 사용합니다.\n",
        "\n",
        "✨학습율이 크다고 항상 좋은 것이 아닙니다!\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%202-1_6.png\" width=900>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XE_drc9XBTOA"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.009\n",
        "epochs = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeaB0-MIwM3h"
      },
      "outputs": [],
      "source": [
        "#epoch은 이 데이터를 몇바퀴 돌려서 학습을 시킬 것이냐! 라고 볼 수 있겠죠? 기억나시나요?\n",
        "for epoch in range(epochs):\n",
        "    #우리는 y을 예측하고 싶습니다. y에 영향을 주는 변수는 X하나 라고 생각하고 간단한 선형식을 만들어 보았습니다!\n",
        "    y_pred = X * w + b # y_pred: 100, 1\n",
        "\n",
        "\n",
        "    # Compute and print loss\n",
        "    # loss 함수로 우리는 MSE 평균제곱오차의 식을 사용합니다.\n",
        "    # 모듈을 불러올 수 있겠지만 수학식으로 작성하면 다음과 같습니다.\n",
        "    # 예상한 값 - 실제값의 제곱의 평균\n",
        "    loss = torch.mean((y_pred-y)**2)# 100, 1\n",
        "\n",
        "\n",
        "    # 100 에폭당 우리는 지금 학습이 어느정도 되고 있는지 수치로 보고 싶어요!\n",
        "    # 그래서 if%100이 0일때 print 해보는 코드를 짜보았습니다.\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "    # Forward pass: compute predicted y\n",
        "    # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
        "\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    # 역전파를 통해 기울기를 계산합니다.\n",
        "    # loss 는 우리가 위에 정의했던 'torch.mean((y_pred - y) ** 2)' 이었습니다!\n",
        "    # 여기서 기울기를 계산하게 되면 독립변수(X) 에 영향을 주었던 w, b 에 대해서 기울기 값이 나오게 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    # 잠깐 ! 여기서 왜 - 를 할까요?\n",
        "    # 위 그림에서 우리는 기울기가 음수이면 \\ 양수이면 / 모양을 띄는 것을 확인할 수 있었습니다.\n",
        "    # 우리는 경사하강법 그래프에서 볼 수 있든 최저점을 찾아가야 하는데\n",
        "    # 그럼 기울기가 음수이면 오른쪽 양수이면 왼쪽으로 가야하겠죠?\n",
        "    # 즉 음수이면 그래프상 X축 기준으로 + 양수이면 X축 기준으로 - 를 해야하는 것 입니다.이걸 어떻게 할까요?\n",
        "    # 바로 -연산을 하면 됩니다. -에 -를 하면 +가 되는 것은 아시죠?\n",
        "    # 그래서 기울기가 음수일 경우 빼주면 +가 되어 오른쪽으로 이동하고 기울기가 양수일때 빼주면 왼쪽으로 이동하게 되는 것이죠!\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Zero gradients after updating\n",
        "    # 그리고 이후 학습을 위해 기울기를 초기화 시켜줍니다.\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "    if epoch % 200 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2IbgzEsuXLv"
      },
      "source": [
        "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlzOMd44rinI"
      },
      "source": [
        "### 선형회귀 조금 더 해보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgue321swjEE"
      },
      "source": [
        "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
        "\n",
        "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe5Uqs91xjtC"
      },
      "outputs": [],
      "source": [
        "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcrBxNp7LCB"
      },
      "source": [
        "## 딥러닝 들어가기\n",
        "\n",
        "\n",
        "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUTgBrCI7k8X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "# 랜덤 시드를 고정하는 이유는 고정하지 않으면 매번 다른 값이 나와 모델을 튜닝할때 적절하지 않습니다.\n",
        "# 모델 튜닝을 같은 기준, 환경에서 튜닝을 하기 위함입니다.\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBf3IhLwOYVa"
      },
      "source": [
        "원래대로라면 실제 데이터를 쓰는 것이 좋지만, 실험의 편리함을 위해 인공적으로 만든 합성 데이터를 사용하자.\n",
        "\n",
        "입력 feature는 20개다. 1시간 후에 비가 올지 안 올지 알고 싶은데, 현재 가지고 있는 데이터가 서울 각지의 습도 데이터라고 하자. 20곳에서 동시에 각각 습도를 잰 것이다. 이 데이터를 가지고 비가 오면 1, 비가 오지 않으면 0이라고 하자. 실제 데이터를 안 가지고 있기 때문에, 적당히 수식을 써서 비가 오는 경우와 안 오는 경우를 임의로 구분하여 합성 데이터를 만들자. 이러한 데이터 포인트 1000개를 만들자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AuEAYWDf7h6d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "input_features = 20 #20개의 데이터를 통해 비가 올지(1) 비가 안올지(0) 을 예측 할것\n",
        "\n",
        "# Features: random numbers\n",
        "# 20개의 feature(특성, 독립변수) 랜덤 생성하는 코드를 작성하시오\n",
        "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "# Labels: sum of features > 0 => class 1, else class 0\n",
        "# Please convince yourself this is the real data, while actuall not the case\n",
        "\n",
        "# Label데이터 생성 비가 올때는 1 비가 안올 때는 0이라는 더미데이터 생성하는 코드\n",
        "# 아래 코드는 X의 20개의 데이터의 합이 양수면 1 음수면 0이라는 더미데이터를 생성하는 코드이다.\n",
        "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKaFl-Y8fIAO"
      },
      "outputs": [],
      "source": [
        "print(X[0])\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJm3f3k1ZyA6"
      },
      "source": [
        "$x_0 + x_1 + ... + x_{19} >0 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQCWFuWZQikB"
      },
      "source": [
        "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
        "\n",
        "비가 오는지 오지 않는지 어떻게 추정할 수 있을까? 위 수식을 계산하여 결정한다. 비가 오면 값이 커지고, 비가 오지 않으면 값이 작아지도록 한다. 0을 기준으로 양수이면 비가 오는 것으로 판단, 음수면 비가 오지 않는 것으로 판단한다.\n",
        "\n",
        "`X`는 주어진 데이터이니, 우리는 원하는 값을 계산할 수 있도록 해 주는 `w`와 `b`의 값을 구하면 된다.\n",
        "\n",
        "일단 랜덤으로 값을 초기화해 놓고, 이후 경사하강법으로 좋은 값을 찾아간다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "a_h2Shu8TRu3"
      },
      "outputs": [],
      "source": [
        "# 위에서 말했든 실제 딥러닝에서도 초기 가중치와 편향 값은 랜덤으로 찍고 시작을 합니다.\n",
        "# 그리고 기울기 값을 기록하기 위해 requires_grad=True ! 필수!!\n",
        "# 가중치의 개수는 특성의 개수와 같아야 겠죠? 아래 빈칸을 채워볼까요?\n",
        "w = torch.randn(input_features, requires_grad=True) \n",
        "b = torch.randn(1, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-a0lSLgYY6C"
      },
      "source": [
        "- w를 출력해보면 20개의 랜덤값이 있습니다.\n",
        "- Why?? X의 특성 개수 20개에 각각 다른 가중치를 적용하기 위해 w를 생성할 때 파라미터로 input_features(20)을 넣어주었기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "WTHZasM_YUJK",
        "outputId": "6f8bc179-14e5-49ec-c7ec-13b34df486af"
      },
      "outputs": [],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yk-utOrT9EJ"
      },
      "source": [
        "본격적으로 답을 찾기 전에, 랜덤으로 만든 값들이 얼마나 유용한지 테스트해보자.\n",
        "\n",
        "혹시라도 랜덤으로 만들었는데 이미 잘 맞춘다면 우리가 이 고생을 할 필요가 없다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qKcEAZwnULNJ",
        "outputId": "c98cc813-5e49-4f64-f4bd-5e7beb72d0cc"
      },
      "outputs": [],
      "source": [
        "# 일반적으로 이진분류(O or X)에서 찍는 다고 생각하면 50%의 확률이다\n",
        "# 아래 코드는 지금 당장은 이해할 필요없고 일단 랜덤으로 찍었을 때 얼마정도의 확률이 나오는지 재미삼아 계산을 해본 것 입니다.\n",
        "cnt_total = 0\n",
        "cnt_correct = 0\n",
        "for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "    with torch.no_grad():\n",
        "        score = torch.sum(x_i*w) + b\n",
        "    predict_flag = score > 0\n",
        "    answer_flag = y_i == 1\n",
        "\n",
        "    if predict_flag == answer_flag:\n",
        "        cnt_correct += 1\n",
        "    cnt_total += 1\n",
        "\n",
        "print(f'Accuracy: {cnt_correct/cnt_total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8wtWG0fTqzS"
      },
      "source": [
        "다음은 놀랍게도 실제 뉴럴네트워크의 학습 과정이다.\n",
        "```\n",
        "면접관: 당신의 장점은?\n",
        "나: 저는 머신러닝 전문가입니다.\n",
        "면접관: 9+10은?\n",
        "나: 3 입니다.\n",
        "면접관: 틀렸네. 전혀 달라. 답은 19일세.\n",
        "나: 16 입니다.\n",
        "면접관: 틀렸네. 답은 19일세.\n",
        "나: 18 입니다.\n",
        "면접관: 틀렸네. 답은 19일세.\n",
        "나: 19 입니다.\n",
        "면접관: 자넨 합격일세.\n",
        "```\n",
        "\n",
        "다만 아래와 같이 좀 더 정확하게 고칠 수 있다.\n",
        "\n",
        "```\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
        "신경망: 수식을 계산한 결과 8이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
        "훈련교관: 답은 비가 오지 않는다네.\n",
        "신경망: 명심하겠습니다.\n",
        "\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
        "신경망: 수식을 계산한 결과 3이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
        "훈련교관: 답은 비가 온다네.\n",
        "신경망: 명심하겠습니다.\n",
        "\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가? (1. 학습 데이터 샘플링)\n",
        "신경망: 수식을 계산한 결과 -10이 나왔습니다. 0보다 작으니 비가 안 올 것입니다. (2. 추론)\n",
        "훈련교관: 답은 비가 오지 않는다네.\n",
        "신경망: 명심하겠습니다. (3. 손실함수에서 역전파된 그래디언트를 바탕으로 파라미터 조정)\n",
        "```\n",
        "위에서 이미 1단계와 2단계를 끝냈다.\n",
        "이제 위 코드에 약간 추가해 3단계를 수행하자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mDSr6JRaTwu"
      },
      "source": [
        "손실함수를 계산하기 이전에 손실함수에 대해 짚고 넘어가는 것이 좋겠다. `torch.nn.BCEWithLogitsLoss()`가 우리가 사용할 손실함수다. 이 함수의 수학적 구조에 대해 논하는 것은 이론 시간의 역할이고, 실습 시간에는 이 손실함수가 우리가 원하는 성질을 만족시키는지만 간단히 체크하고 넘어가자.\n",
        "\n",
        "손실함수는 말 그대로의 의미를 지닌다. 손실이다. 크면 안 된다. 우리는 손실을 줄여야 한다. 좀 더 정확히는, 절대적인 크기보다는 상대적인 크기가 중요하다.\n",
        "\n",
        "신경망은 학습을 통해 나쁜 상태에서 좋은 상태로 나아간다. 나쁜 상태는 비가 오는지 오지 않는지 잘 예측하지 못하는 상태이고, 좋은 상태는 그 반대다. 현재 신경망이 처한 상태가 얼마나 좋은지, 또는 얼마나 나쁜지 알려주는 가이드가 손실함수다.\n",
        "\n",
        "우리에게 $s_1$과 $s_2$라는 두 가지의 상태가 있고, $s_1$이 더 바람직한 상태라고 하자. 손실함수를 $f$라고 하면, 다음을 만족해야 한다.\n",
        "\n",
        "$$f(s_1) < f(s_2)$$\n",
        "\n",
        "(상태란 단적으로 말하면 `w`와 `b`의 값을 말한다.)\n",
        "\n",
        "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
        "\n",
        "A. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 온다면 현재 신경망은 좋은 상태에 있는 것이다.\n",
        "\n",
        "B. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 오지 않는다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
        "\n",
        "C. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 온다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
        "\n",
        "D. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 오지 않는다면 현재 신경망은 좋은 상태에 있는 것이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmPIevn-a9qm",
        "outputId": "e6a40d8a-7f01-4bee-ef69-be0e3febb95b"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss() # 분류문제는 이걸쓰는게 좋다\n",
        "# criterion = torch.nn.MSELoss\n",
        "# 비가 오면 1, 안 오면 0\n",
        "\n",
        "# A\n",
        "a = criterion(torch.tensor([10.]), torch.tensor([1.]))\n",
        "print(a)\n",
        "# B\n",
        "b = criterion(torch.tensor([10.]), torch.tensor([0.]))\n",
        "print(b)\n",
        "# C\n",
        "c = criterion(torch.tensor([-6.]), torch.tensor([1.]))\n",
        "print(c)\n",
        "# D\n",
        "d = criterion(torch.tensor([-6.]), torch.tensor([0.]))\n",
        "print(d)\n",
        "\n",
        "assert a < b\n",
        "assert a < c\n",
        "assert d < b\n",
        "assert d < c\n",
        "# 위 코드에 대한 이해가 어려우시다면 BCEWithLogitsLoss 랴는 손실함수는 숫자가 높으면 yes에 가깝고 숫자가 낮으면 no를\n",
        "# 뜻한다고 추상적으로 이해를 해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRwK8PvZ9a-"
      },
      "source": [
        "손실함수가 원하는 성질을 가진다는 것을 확인했으니 이제 신경망의 학습과정을 구현하자. 학습이 진행되며 손실함수가 점점 줄어들어야 정상이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "3tJQUKTKhKsK",
        "outputId": "195b1269-9fac-4cf6-c9a2-ca1c742e90f6"
      },
      "outputs": [],
      "source": [
        "# 손실함수 정의\n",
        "# 이진 분류를 수행하는 손실함수 코드를 작성해봅시다.\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 데이터를 몇번 돌려서 학습을 할지 정의\n",
        "num_epoch = 300\n",
        "\n",
        "# 아까 봤던\n",
        "'''\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "'''\n",
        "#위 코드 처럼 가중치와 편향의 기울기에 얼만큼의 learning_rate(학습률)을 적용해 얼만큼 많이, 또는 적게 반영할 것인지 정의\n",
        "learning_rate = 0.003\n",
        "\n",
        "# 가중치, 편향 초기화\n",
        "w.grad = torch.zeros_like(w) # Initialize gradient\n",
        "b.grad = torch.zeros_like(b) # Initialize gradient\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    loss_sum = 0\n",
        "    # 자 아까 X_tensor, y_tensor가 뭐였죠? 랜덤하게 생성된 20개의 특성 X_tensor\n",
        "    # 그리고 랜덤하게 생성된 1(yes), 0(no)의 값 y_tensor 입니다.\n",
        "    # 아래와 같이 for 문을 작성하면 x_i에는 20개의 특성, y_i에는 0,1중 하나의 값이 들어가겠죠?\n",
        "    # 아래 ????에 들어가야 할 값은 데이터 입니다!\n",
        "    for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "        # Evaluation code is commented out because not needed\n",
        "        # predict_flag = (x_i*w).sum() + b > 0\n",
        "        # answer_flag = y_i == 1\n",
        "\n",
        "        #가중치의 기울기 초기화하는 코드를 작성해보세요\n",
        "        w.grad.zero_() # Clear gradient\n",
        "        b.grad.zero_() # Clear gradient\n",
        "\n",
        "        # score = x_1 * w_1 + x_2 * w_2 + ... + x_19 * w_19 + b\n",
        "        score = torch.sum(x_i*w) + b\n",
        "        # 손실함수 계산을 위해 squeeze() 함수로 차원을 맞춥니다.\n",
        "        loss = criterion(score.squeeze(), y_i.squeeze())\n",
        "        # 아래는 역전파 코드입니다.\n",
        "        loss.backward()\n",
        "        # X_tensor 는 20개의 특성데이터 num_samples(1000개)가 들어가 있습니다.\n",
        "        # 1000개의 loss를 다 더한 후 1000개로 나누어 평균 loss를 구할 예정입니다.\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        #가중치를 조정할때는  기울기의 계산이 필요하지 않기 때문에 with 구문을 실행\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epoch}, Loss: {loss_sum/num_samples:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYcNr1USoMj6"
      },
      "source": [
        "학습이 어느정도 진행되었으면 정확도를 다시 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "uXw8OHoGoS-i",
        "outputId": "99029a79-5954-4ccc-fbb2-5b80464e9991"
      },
      "outputs": [],
      "source": [
        "cnt_total = 0\n",
        "cnt_correct = 0\n",
        "for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "\n",
        "    # 평가를 할 때에도 학습을 하는 것이 아니기 때문에 기울기를 기록할 필요 없습니다. 어떤 코드를 작성해야 하죠??\n",
        "    with torch.no_grad():\n",
        "        score = torch.sum(x_i*w) + b\n",
        "    predict_flag = score > 0\n",
        "    answer_flag = y_i == 1\n",
        "\n",
        "    if predict_flag == answer_flag:\n",
        "        cnt_correct += 1\n",
        "    cnt_total += 1\n",
        "\n",
        "print(f'Accuracy: {cnt_correct/cnt_total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2jJ4Gglg7st"
      },
      "outputs": [],
      "source": [
        "#데이터 첫번째 샘플을 다음과 같이 출력하면 값이 나온다.\n",
        "print(X_tensor[0])\n",
        "print(y_tensor[0])\n",
        "#위 딥러닝 아키텍쳐를 그려보자면 아래와 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeSGLZA3e8OI"
      },
      "source": [
        "### 위 X_tensor 와 y_tensor 의 첫번째 값을 출력하면\n",
        "\n",
        "    tensor([ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676, -0.9773,  0.9501, -0.1514,\n",
        "            -0.1032,  0.4106,  0.1440,  1.4543,  0.7610,  0.1217,  0.4439,  0.3337,\n",
        "            1.4941, -0.2052,  0.3131, -0.8541])\n",
        "            \n",
        "    tensor([1.])\n",
        "\n",
        "위와 같이 나왔다\n",
        "위 데이터를 바탕으로 딥러닝 아키텍쳐를 그려보자면 다음과 같은\n",
        "\"히든레이어가 없는\" 단일 레이어로 구성된 신경망이라고 볼 수 있다.\n",
        "\n",
        "<img src='https://drive.google.com/drive-viewer/AKGpihaD8lib2wGDNwkjxbQ9bBGGGG2o-KYN0z_HNIJVzNNSUwVgMLkrlFtpnUQ1WPfc7GPGb5pEPkrMBjVd6m5MKfWNjlYnLlNWIZ0=w2880-h1428-rw-v1' width='500'  border='0'></a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyArh-UHrPYn"
      },
      "source": [
        "아래는 같은 문제를 다층 퍼셉트론, `torch.nn.Module`, `torch.optim` 등을 통해 해결한 것이다. 위는 원리를 설명하기 위한 실습이고, 현업에서는 아래와 같은 코드를 작성하는 것이 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ba9azyGIjsKP",
        "outputId": "14eb704c-267d-471b-a8e3-46bd57589b89"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "      #파이토치의 뉴럴넷의 기본은 ! 클래스 상속 기능입니다 ! 아래 코드를 작성해주세요!\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "\n",
        "        # 레이어를 쌓는곳 위에 예시는 단일레이어 였지만 실제로는 여러개의 hidden 레이어를 쌓아서 사용한다.\n",
        "        # 아래에는 두개의 레이어를 쌓는 것을 볼 수 있다.\n",
        "        # 선형 레이어를 쌓는 코드\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        # 활성화 함수를 이용해 선형식의 한계를 극복해 봅시다 !\n",
        "        self.relu = nn.ReLU()                 # ReLU activation\n",
        "        # 두번째 선형 레이어\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x 가 들어와서 첫번째 히든레이어 통과\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        # 히든레이어가 통과한 값이 들어와서 ReLU 함수 적용\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        # ReLU 함수가 적용된 값이 다시 두번째 히든레이어에 들어가고 최종 output인\n",
        "        # out으로 값을 return\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyu8iWdzjtAZ"
      },
      "source": [
        "### 잠깐 ! ReLU 뭐였지??\n",
        "\n",
        " - 은닉계층의 활성함수로는 **ReLU** 함수를 주로 사용합니다.\n",
        "\n",
        "    <img src=\"https://datadiving.dothome.co.kr/Deep%202-1_3.png\" width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXe436j_7Uda",
        "outputId": "58f70acb-1c53-4fd6-a7c4-cbf1fe5dc378"
      },
      "outputs": [],
      "source": [
        "# Create a dataset and data loader\n",
        "# DataLoader 모듈을 사용하여 데이터를 배치 단위로 나누기 위해 TensorDataset 모듈로 데이터를 묶어줌\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "batch_size = 32\n",
        "# 예를들어 X_tensor의 개수가 1000개면 1~32,33~64 이런식으로 나누어서 들어갑니다 !\n",
        "# 어떤 모듈을 사용해야 할까요?\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# dataloader = create_dataset()\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = input_features\n",
        "# 좀 위에 정의했던 단일레이어보다 좀 더 복잡한 연산을 수행하기 위한 히든 사이즈(히든 레이어의 노드개수) 설정\n",
        "hidden_size = 64\n",
        "output_size = 1  # Binary classification (1,0 둘 하나가 나오기 때문에 사이즈는 1개)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss(input)  # Combines a sigmoid layer and the BCELoss\n",
        "# optimizer 를 아래와 같이 정의하면 optimizer 가 model 의 parameters 즉 모든 가중치와 편향을 저장한다.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(????)\n",
        "        loss = ????(????, ????)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.????()  # Clear gradients 저장된 모든 파라미터의 기울기 초기화\n",
        "        loss.????()        # Compute gradients 모든 파라미터의 기울기 계산\n",
        "        optimizer.????()       # Update weights 기울기를 바탕으로 가중치 업데이트\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# train()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    predictions = torch.????(outputs)  # Apply sigmoid to get probabilities\n",
        "    predicted_classes = (predictions >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2DJDAxMn0Hq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.input_size = input_size\n",
        "        self.fc1 = nn.????(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.????()             # ReLU activation\n",
        "        self.fc2 = nn.????(hidden_size, 1) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out\n",
        "\n",
        "    def create_dataset(self, num_samples = 1000, batch_size = 32):\n",
        "        # Features: random numbers\n",
        "        input_features = self.input_size\n",
        "        X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "        # Labels: sum of features > 0 => class 1, else class 0\n",
        "        # Please convince yourself this is the real data, while actuall not the case\n",
        "        y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        X_tensor = torch.from_numpy(X)\n",
        "        y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def train(self, train_data, num_epochs = 20, learning_rate = 0.001, criterion = nn.BCEWithLogitsLoss):\n",
        "        criterion = criterion()  # Combines a sigmoid layer and the BCELoss\n",
        "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch_X, batch_y in train_data:\n",
        "                # Forward pass\n",
        "                outputs = self(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                optimizer.zero_grad()  # Clear gradients\n",
        "                loss.backward()        # Compute gradients\n",
        "                optimizer.step()       # Update weights\n",
        "\n",
        "            # Print loss for every epoch\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            accuracy_history = []\n",
        "            for batch_X, batch_y in test_data:\n",
        "                outputs = self(batch_X)\n",
        "                predictions = torch.sigmoid(outputs)\n",
        "                predicted_classes = (predictions >= 0.5).float()\n",
        "                accuracy = (predicted_classes == batch_y).float().mean()\n",
        "                accuracy_history.append(accuracy)\n",
        "            accuracy = sum(accuracy_history) / len(accuracy_history)\n",
        "            print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "            return accuracy\n",
        "\n",
        "\n",
        "model = ????(20, 64)\n",
        "train_dataset = model.????()\n",
        "test_dataset = model.????()\n",
        "print(train_dataset)\n",
        "model.????(train_dataset)\n",
        "model.????(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5n0jb4vxWg"
      },
      "source": [
        "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVlORyey0VDi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIv5SMtX0U_C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Rl4ddJZPpKDo",
        "outputId": "3cf3def6-8567-4207-ea0d-8de5afb8d48c"
      },
      "outputs": [],
      "source": [
        "def hidden_size_candidates():\n",
        "    return [2**n for n in range(7)]\n",
        "\n",
        "def learning_rate_candidates():\n",
        "    return [0.001]\n",
        "\n",
        "def train_and_eval(hidden_size, learning_rate):\n",
        "    model = ????(20, hidden_size)\n",
        "    train_dataset = model.????(1000)\n",
        "    test_dataset = model.????(100)\n",
        "\n",
        "    model.????(????, learning_rate = learning_rate)\n",
        "    evaluation_result = model.????(test_dataset)\n",
        "\n",
        "    return evaluation_result\n",
        "\n",
        "def find_hyperparameter():\n",
        "    hidden_sizes = ????()\n",
        "    learning_rates = ????()\n",
        "    result_history = []\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for learning_rate in learning_rates:\n",
        "            eval_result = ????(hidden_size, learning_rate)\n",
        "            result_history.append((hidden_size, learning_rate, eval_result))\n",
        "            print(hidden_size, learning_rate, eval_result)\n",
        "    return max(result_history, key = lambda x:x[2])\n",
        "\n",
        "find_hyperparameter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "6YxkejqAw8--",
        "outputId": "4c083cde-6d59-4240-ffbc-b2e49961d8ef"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "# 함수 생성\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [1,2,3,4]\n",
        "y = [2,4,6,8]\n",
        "plt.scatter(X, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAmP1Nzr01JZ"
      },
      "source": [
        "딥러닝으로\n",
        "\n",
        "\n",
        "1.   선형 회귀 y = 4 + 3x를 해볼 것\n",
        "  - 맨 처음에는 torch.tensor이용해서 아예 바닥부터 구현\n",
        "  - 그 이후에 nn.Linear 써서 구현\n",
        "  - 그 이후에 dataloader 써서 구현\n",
        "  - batching 구현해보기\n",
        "  - optimizer 써보기\n",
        "\n",
        "2. 다 하고 나서 $ y = w_1 x_1  + w_2 x_2  + w_3 x_3 + b$  해볼 것\n",
        "\n",
        "$ y = (w_1, w_2, w_3) * (x_1, x_2, x_3)^T +b $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24rvHPi-0ZuG",
        "outputId": "62bd20f7-acca-400d-c70c-4f79096ad111"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "0rl_S88Y2DYV",
        "outputId": "12875346-7a44-45d5-8bcf-3e2653385879"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        self.b = torch.randn(1, requires_grad = True)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return ????\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = ????(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = self.????(y_pred, y)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.????()\n",
        "\n",
        "            with torch.????():\n",
        "                self.w -= learning_rate * self.w.grad\n",
        "                self.b -= learning_rate * self.b.grad\n",
        "\n",
        "            self.????.grad.zero_()\n",
        "            self.????.grad.zero_()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate_model(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return self.loss(y_pred, y).item()\n",
        "\n",
        "X, y = ????(noise_amplitude = 1)\n",
        "test_X, test_y = ????(noise_amplitude = 1)\n",
        "\n",
        "model = ????()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.w.item(), model.b.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-oOrtaXQE2"
      },
      "source": [
        "Optimizer를 써서 gradient descent를 해보기 (SGD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEMMb0CVvro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.????):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.w = torch.randn(????, ????, requires_grad = True)\n",
        "        self.b = torch.randn(????, requires_grad = True)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.w + self.b\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = ????(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = ????.loss(y_pred, y)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.????()\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            #     # self.w -= learning_rate * self.w.grad\n",
        "            #     # self.b -= learning_rate * self.b.grad\n",
        "            optimizer.????()\n",
        "\n",
        "            # self.w.grad.zero_()\n",
        "            # self.b.grad.zero_()\n",
        "            optimizer.????()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return self.loss(y_pred, y).item()\n",
        "\n",
        "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz-aUPJ9XUvH"
      },
      "source": [
        "torch.tensor 대신 nn.Linear 써서 해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gucWqihWXL3y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        # self.b = torch.randn(1, requires_grad = True)\n",
        "        self.layer = nn.????(1, 1)\n",
        "        self.loss = nn.????()\n",
        "        self.optimizer = optim.????\n",
        "\n",
        "    def forward(self, X):\n",
        "        # return X @ self.w + self.b\n",
        "        return self.layer(X)\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        optimizer = self.????(????, lr = learning_rate)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = ????\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            #     # self.w -= learning_rate * self.w.grad\n",
        "            #     # self.b -= learning_rate * self.b.grad\n",
        "            optimizer.step()\n",
        "\n",
        "            # self.w.grad.zero_()\n",
        "            # self.b.grad.zero_()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return ????\n",
        "\n",
        "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR3C2dVVYsNL"
      },
      "source": [
        "torch DataLoader 써서 데이터 다루기 (자동으로 알아서 batching 해줌!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7fQvJddYrtg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1, batch_size = 32):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        # self.b = torch.randn(1, requires_grad = True)\n",
        "        self.layer = ????\n",
        "        self.loss = ????\n",
        "        self.optimizer = ????\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X.shape: (batch_size, n_feature)\n",
        "        # self.layer.shape: (n_feature, hidden_layer)\n",
        "        # X.shape @ self.layer: (batch_size, hidden_layer)\n",
        "        # m, n @ n, p -> m, p\n",
        "        # return X @ self.w + self.b\n",
        "        return self.????\n",
        "\n",
        "    def train_model(self, data, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        optimizer = ????\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in data:\n",
        "                y_pred = self(x)\n",
        "\n",
        "                loss = ????\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                # compute the gradient of the loss\n",
        "                loss.????\n",
        "                optimizer.????\n",
        "                optimizer.????\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        with torch.no_grad():\n",
        "            loss_list = []\n",
        "\n",
        "            for x, y in data:\n",
        "                y_pred = ????\n",
        "                loss = ????\n",
        "                loss_list.append(loss)\n",
        "\n",
        "            return sum(loss_list) / len(loss_list)\n",
        "\n",
        "train_data = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_data = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(train_data, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_data))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBqAOBE5hbG5"
      },
      "source": [
        "1. $x+y<1$인지 판별하는\n",
        "  - 데이터셋을 만들고\n",
        "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
        "  - 학습/평가하고\n",
        "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
        "2. $x^2+y^2<1$인지 판별하는\n",
        "  - 데이터셋을 만들고\n",
        "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
        "  - 학습/평가하고\n",
        "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqpXFgRbe8QI",
        "outputId": "9f3f84d7-dd24-4d7a-d700-8e2cfc00042b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def generate_artificial_data(n_samples, batch_size = 32):\n",
        "    x = torch.randn(n_samples, 2)\n",
        "    y = torch.sum(x**2, dim = 1).unsqueeze(dim = 1)\n",
        "\n",
        "    y = (y > 1).float()\n",
        "\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.layer1 = ????\n",
        "        self.activation = ????\n",
        "        self.layer2 = ????\n",
        "        self.sigmoid = ????\n",
        "        self.loss = ????\n",
        "        self.optimizer = ????\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.???? # output.shape: (batch_size, hidden_size)\n",
        "        output = self.????\n",
        "        output = self.???? # output.shape: (batch_size, 1)\n",
        "        output = self.???? # output.shape: (batch_size, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_data, valid_data, epochs = 20, learning_rate = 0.002):\n",
        "        optimizer = ????\n",
        "        train_loss_history = []\n",
        "        valid_loss_history = []\n",
        "        valid_acc_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in train_data:\n",
        "                y_pred = ????\n",
        "                loss = ????\n",
        "                train_loss_history.append(loss.item())\n",
        "\n",
        "                loss.????\n",
        "                optimizer.????\n",
        "                optimizer.????\n",
        "\n",
        "                acc, valid_loss = self.evaluate_model(valid_data)\n",
        "                self.train()\n",
        "                valid_acc_history.append(acc)\n",
        "                valid_loss_history.append(valid_loss)\n",
        "\n",
        "            # print(f'{epoch+1}/{epochs} Epoch, train loss = {loss}, valid loss = {valid_loss}, acc = {acc}')\n",
        "\n",
        "        return train_loss_history, valid_loss_history, valid_acc_history\n",
        "\n",
        "    def evaluate_model(self, data):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            loss_history = []\n",
        "            for x, y in data:\n",
        "                y_pred = ????\n",
        "                loss_history.append(self.loss(y, y_pred).item())\n",
        "                correct += torch.sum(((y_pred > 0.5).float() == y).float())\n",
        "                total += 32\n",
        "\n",
        "            return correct / total, sum(loss_history) / len(loss_history)\n",
        "\n",
        "model = BinaryClassifier(32) #\n",
        "train_data = generate_artificial_data(1000)\n",
        "valid_data = generate_artificial_data(100)\n",
        "test_data = generate_artificial_data(100)\n",
        "\n",
        "train_loss_history, valid_loss_history, valid_acc_history = model.train_model(train_data, valid_data, epochs = 100)\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(valid_loss_history)\n",
        "plot_loss_history(valid_acc_history)\n",
        "\n",
        "model.evaluate_model(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqBOCaUGGKiR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xN5jjYflr1oE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
