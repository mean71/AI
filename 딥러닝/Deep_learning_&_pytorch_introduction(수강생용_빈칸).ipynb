{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvVeD4rs-uoN"
      },
      "source": [
        "## ① 인공 신경망에 대하여 🧠\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IOrbgYFKmFk"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-9.png' width = '600' border='0'></a>\n",
        "\n",
        "\n",
        "✏️ **학습 방법에 따라서 인공지능 → 머신러닝 → 딥러닝으로 구체화됩니다.**\n",
        "> **`인공지능`** </br>\n",
        "> 인공지능은 인간의 지능이 갖고 있는 기능을 갖춘 컴퓨터 시스템이며, 인간의 지능을 기계 등에 인공적으로 시연(구현)한 가장 큰 범주에 해당합니다. </br>일반적으로 범용 컴퓨터에 적용한다고 가정합니다.\n",
        "\n",
        "> **`머신러닝`** </br>기계 학습 또는 머신 러닝은 인공지능의 한 분야로, 컴퓨터가 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 분야를 말합니다.\n",
        "\n",
        "> **`딥러닝(심층학습)`** </br> 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화를 시도하는 기계 학습 알고리즘의 집합으로 정의됩니다. </br>큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있습니다. </br>딥 러닝은 특징 추출부터 패턴까지 모든 과정을 사람의 개입 없이 심층인공신경망을 토대로 학습방식을 구현하는 기술입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsFamzTOZK_m"
      },
      "source": [
        "**인공 신경망(artificial neural network)**은 알고리즘의 플로우 차트(순서도)와 매우 유사합니다.\n",
        "\n",
        "다만, 훨씬 더 단순하죠. 오직 동그라미와 화살표만으로 이루어져 있고, 대칭적입니다.\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-10.jpeg' width='600' border='0'></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRwitWQJo9-"
      },
      "source": [
        "```\n",
        "🔎 인공 신경망의 하나의 동그라미를 확대해 봅시다.  \n",
        "\n",
        "```\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-11.png' width='600' border='0'></a>\n",
        "\n",
        "<img src='https://datadiving.dothome.co.kr/Deep_1-12.png' border='0'></a>\n",
        "\n",
        "입력 x와 딥러닝을 통해 찾아지는 숫자 w가 서로 곱해지고 더해져서 하나의 숫자를 만들고 단순한 함수 f를 통과하여 다음 동그라미의 입력으로 넘어갑니다.\n",
        "\n",
        "이러한 구조를 **퍼셉트론(perceptron)**이라고 합니다.\n",
        "\n",
        "모든 인공지능 알고리즘은 이러한 구조로 이뤄져 있고,\n",
        "\n",
        "이러한 신경망을 통해 학습(learn)을 하는 것을 **딥러닝**이라고 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-Ju-SsEj3_"
      },
      "source": [
        "### 딥러닝이란?\n",
        "```\n",
        "🧐 이제 본격적으로 딥러닝에 대해 탐구해봅시다!\n",
        "```\n",
        "\n",
        "앞서, **인공 신경망을 통해 학습하는 과정**이 딥러닝이라는 것을 배웠습니다.\n",
        "\n",
        "기존 프로그램의 한계는 바로 사람이 짜기 때문에 생긴다는 것도 배웠죠!\n",
        "\n",
        "따라서 딥러닝은 기존 프로그래밍처럼 프로그래머가 정해진 규칙에 기반한 알고리즘을 지정해주지 않고    \n",
        "\n",
        "가장 단순한 구조의 플로우 차트에서 출발하여 **데이터 기반으로 스스로 배우는 과정**을 의미합니다.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJMKBGVXNREy"
      },
      "source": [
        "```\n",
        "🗂 데이터를 기반으로 스스로 학습하는 과정을 구체적으로 살펴볼까요?\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGofK9F9Ivmh"
      },
      "source": [
        "<img src='https://datadiving.dothome.co.kr/Deep_1-13.jpeg' border='0'></a>\n",
        "\n",
        "임의의 w (weight) 로 구성된 인공 신경망에서 각 w 값을 키웠다가, 줄였다가 변화시키면서\n",
        "\n",
        "**최종 결과값이 기대하는 값에 가까워지는 방향**으로 w 를 학습하게 됩니다.  \n",
        "\n",
        "우리가 아는 알파고, 자율 주행, 아무리 복잡한 딥러닝 알고리즘도 이러한 단순 구조의 퍼셉트론들의 집합으로 이뤄져 있습니다.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R1ybKJ7JUK7"
      },
      "source": [
        "## Pytorch 설치 및 확인\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYC7h8bMATYj",
        "outputId": "9ad8f36b-15bf-4d4c-9c27-39ec4f9a5425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_C7Xg6KJjm7",
        "outputId": "0e01c475-5df1-4a66-ec6c-36f2b0b92ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cpu\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAIaC3rWAp3O"
      },
      "source": [
        "## Pytorch Tensor\n",
        "\n",
        "pytorch의 가장 근본이 되는 Tensor들에 대해서 배워보겠습니다.\n",
        "\n",
        "\n",
        "### Tensor?\n",
        "\n",
        "먼저, Tensor에 대해 알아봅시다.\n",
        "\n",
        "```Tensor란?```  **다차원의 배열**을 뜻하는 말입니다.\n",
        "\n",
        "*배열의 차원에 따라 불리는 이름이 달라진다는 것, 모두 알고 계시죠?</br>\n",
        "0차원은 스칼라, 1차원은 벡터, 2차원은 메트릭스, 그 이상의 다차원은 아래 차원의 것을 모아놓은 배열인 것이라고 할 수 있는데요!</br> tensor는 스칼라, 벡터, 매트릭스 등의 데이터와 그 이상의 고차원 데이터도 포함하는 개념입니다.\n",
        "\n",
        "참고로 tensor에서 Rank는 그 데이터가 몇 차원의 배열인지를 의미합니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZzM9rYH8hPx"
      },
      "source": [
        "<img src='https://datadiving.dothome.co.kr/Deep_1-63.jpeg' width='900'  border='0'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYzdxecs8eY9"
      },
      "source": [
        "### Tensor 만드는 법 1\n",
        "\n",
        "torch.tensor(data): data는 튜플, 리스트, numpy 배열 등등\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZPgQV3KoVgz",
        "outputId": "9069c33e-8689-4c64-f788-56850dfc94dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(5.)\n",
            "tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# 0-D Tensor\n",
        "# 텐서를 생성하는 함수를 작성하시오.\n",
        "scalar = torch.tensor(5.0)\n",
        "number = torch.tensor(1.0)\n",
        "print(scalar)  # tensor(5.)\n",
        "print(number) # tensor(1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTYsy2Uno0Av",
        "outputId": "76f10ab7-28b2-421f-f966-f921e2b301c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# 텐서를 생성하는 함수를 작성하시오.\n",
        "vector = torch.tensor([1.0, 2.0, 3.0])\n",
        "tuple_vector = torch.tensor((1, 2, 3))\n",
        "print(vector)  # tensor([1., 2., 3.])\n",
        "print(tuple_vector) # tensor([1, 2, 3,])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJA3LCr1pbRU",
        "outputId": "5843b817-e3ee-43c0-8294-cda210e3467a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "matrix2 = torch.tensor([[1, 2, 3], [3, 4, 5]])\n",
        "print(matrix.shape)\n",
        "print(matrix2.shape)\n",
        "# tensor([[1., 2.],\n",
        "#         [3., 4.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnOILb3NGJrx",
        "outputId": "b8ba54ad-23e1-4741-ed58-d06c97e00ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "matrix2 = [[1, 2, 3], [3, 4, 5]]\n",
        "lst = [matrix2, matrix2, matrix2, matrix2]\n",
        "\n",
        "tensor_3d = torch.tensor([[[1.0], [2.0]], [[3.0], [4.0]]])\n",
        "tensor_3d_2 = torch.tensor(lst) # len(lst), len(lst[0]), len(lst[0][0]), ....\n",
        "print(tensor_3d_2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cMM5BWTmTSF"
      },
      "source": [
        "### torch.tensor의 주요 속성들\n",
        "\n",
        "- tensor.shape\n",
        "- tensor.size()\n",
        "- tensor.dtype\n",
        "- device: gpu에 있는지, cpu에 있는지\n",
        "- requires_grad: 이게 True면 미분값을 계산함. 아니면 하지 않음.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "LJg4nrPAMt7t",
        "outputId": "d34998f0-5530-49e0-ea06-2a9dcf03499d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 1])\n"
          ]
        }
      ],
      "source": [
        "#텐서의 크기를 출력해보는 코드를 작성하시오\n",
        "print(vector.shape)    # torch.Size([3])\n",
        "print(matrix.size())   # torch.Size([2, 2])\n",
        "print(tensor_3d.shape) # torch.Size([2, 2, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "BrqNxnpxMwE3",
        "outputId": "0ed75509-409b-4d1c-f136-5e6ada8516b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.int32\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "#텐서의 데이터 타입을 출력하는 코드를 작성하시오\n",
        "print(vector.dtype)    # torch.float32\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
        "print(int_tensor.dtype)  # torch.int32\n",
        "\n",
        "#텐서가 현재 gpu에 위치해 있는지 확인하는 코드를 작성하시오\n",
        "device = 'cuda'*torch.cuda.is_available() or 'cpu'\n",
        "tensor_gpu = torch.tensor([1.0, 2.0, 3.0]).to(device)\n",
        "print(tensor_gpu.device)  # cuda:0 or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GXHR4BemnJ1"
      },
      "source": [
        "### torch.tensor 만드는 방법 2\n",
        "\n",
        "- torch.tensor(data)\n",
        "- 자주 쓰는 텐서들은 만드는 함수가 있음.\n",
        "  * torch.zeros(size): size 형태로 된, 0으로 된 텐서를 만듬.\n",
        "  * torch.ones(size): size 형태로 된, 1로 된 텐서를 만듬.\n",
        "  * torch.rand(size) / torch.randn(size) : 랜덤한 숫자로 된 텐서를 만듬. rand는 0과 1 사이에서 랜덤하게, randn은 표준정규분포(평균 0, 표준편차 1)에서 뽑아옴.\n",
        "  * torch.eye(n): 대각선만 1이고 이외에는 0인 2D 텐서(행렬)을 만듬.\n",
        "- 이외에도 많이 쓰이는 함수들\n",
        "  * torch.arange: range() 함수와 매우 비슷하다.\n",
        "  * torch.linspace(start, end, steps): start부터, end까지, steps개의 숫자를 가지는 텐서를 만듬. 이 때, 숫자들은 등간격으로 만들어짐.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUTrq-ekUmh",
        "outputId": "03b06850-6dc8-4b77-963b-4180a818c2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.1996, 0.6840, 0.0431],\n",
            "        [0.7720, 0.7741, 0.3451]])\n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "tensor([0, 2, 4, 6, 8])\n",
            "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "list_tensor = torch.tensor([1, 2, 3])\n",
        "tuple_tensor = torch.tensor((4, 5, 6))\n",
        "\n",
        "#값이 0인 텐서를 생성하는 코드를 작성하시오\n",
        "zeros = torch.zeros((2, 3))\n",
        "#값이 1인 텐서를 생성하는 코드를 작성하시오\n",
        "ones = torch.ones((2, 3))\n",
        "#값이 랜덤인 텐서를 생성하는 코드를 작성하시오\n",
        "rand = torch.rand((2, 3))\n",
        "eye = torch.eye(3)  # 3x3 Identity matrix\n",
        "print(zeros)\n",
        "print(ones)\n",
        "print(rand)\n",
        "print(eye)\n",
        "#랜덤값의 분포가 정규분포인 텐서를 생성하는 코드를 작성하시오\n",
        "normal = torch.randn((2, 3))  # Normal distribution\n",
        "\n",
        " # 값이 range(0, 10, 2) 인 텐서를 생성하는 코드를 작성하시오\n",
        "arange_tensor = torch.arange(start=0, end=10, step=2)\n",
        " # # 0 0.25 0.5 0.75 1 처럼 등분으로 나뉘는 텐서를 생성하는 코드를 작성하시오\n",
        "linspace_tensor = torch.linspace(start=0, end=1, steps=5) # 0 0.25 0.5 0.75 1\n",
        "print(arange_tensor)\n",
        "print(linspace_tensor)\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
        "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Pf1-B3vOVS"
      },
      "source": [
        "TODO: 위 각 함수들을 torch.tensor와 파이썬 리스트 operation들을 이용하여 재구현해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUiK8UuXpHfE",
        "outputId": "1ef5eeb6-f385-4cd5-c126-c640370d95f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n",
            "tensor([[[True, True, True, True],\n",
            "         [True, True, True, True],\n",
            "         [True, True, True, True]],\n",
            "\n",
            "        [[True, True, True, True],\n",
            "         [True, True, True, True],\n",
            "         [True, True, True, True]]])\n",
            "tensor([[[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]],\n",
            "\n",
            "        [[1, 1, 1, 1],\n",
            "         [1, 1, 1, 1],\n",
            "         [1, 1, 1, 1]]])\n",
            "tensor([[[0.5341, 0.4354, 0.1798, 0.4856],\n",
            "         [0.3508, 0.0708, 0.8588, 0.0859],\n",
            "         [0.8033, 0.5117, 0.7220, 0.0189]],\n",
            "\n",
            "        [[0.7584, 0.8280, 0.8258, 0.9981],\n",
            "         [0.9683, 0.4683, 0.7839, 0.0389],\n",
            "         [0.1764, 0.1719, 0.7552, 0.8354]]])\n",
            "tensor([[[-0.1770,  1.2226,  1.1086,  2.2657],\n",
            "         [ 2.1264, -0.6837, -1.0616, -0.3432],\n",
            "         [-0.5916,  0.7842,  0.7029, -1.2494]],\n",
            "\n",
            "        [[ 0.0852, -1.1278,  1.8988, -1.8883],\n",
            "         [ 2.0508, -0.9514, -0.5559,  2.1520],\n",
            "         [-0.8841, -0.8260, -0.4556,  0.9966]]])\n",
            "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
          ]
        }
      ],
      "source": [
        "# write your code here\n",
        "import random\n",
        "\n",
        "def nested_list(shape, value = 0):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [value for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [nested_list(shape[1:], value = value) for _ in range(l)]\n",
        "\n",
        "def random_nested_list(shape, sample_from, *args):\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [sample_from(*args) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:], sample_from, *args) for _ in range(l)]\n",
        "'''\n",
        "def random_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.random() for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [random_nested_list(shape[1:]) for _ in range(l)]\n",
        "'''\n",
        "def randomn_nested_list(shape):\n",
        "    \"\"\"Accepts tuple/list of int, denoting shape of the nested list.\n",
        "    \"\"\"\n",
        "    if len(shape) == 1:\n",
        "        l = shape[0]\n",
        "        return [random.gauss(0, 1) for _ in range(l)]\n",
        "    else:\n",
        "        l = shape[0]\n",
        "        return [randomn_nested_list(shape[1:]) for _ in range(l)]\n",
        "\n",
        "def zeros(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 0))\n",
        "\n",
        "def ones(shape):\n",
        "    return torch.tensor(nested_list(shape, value = 1))\n",
        "\n",
        "def rand(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.random))\n",
        "\n",
        "def randn(shape):\n",
        "    return torch.tensor(random_nested_list(shape, random.gauss, 0, 1))\n",
        "\n",
        "print(nested_list((2, 3, 4), value = 0))\n",
        "print(zeros((2,3,4)) == torch.zeros((2,3,4)))\n",
        "print(ones((2,3,4)))\n",
        "print(rand((2,3,4)))\n",
        "print(randn((2,3,4)))\n",
        "\n",
        "def eyes(n):\n",
        "    lst = [[0 for i in range(n)] for j in range(n)]\n",
        "\n",
        "    lst = []\n",
        "    for i in range(n):\n",
        "        tmp = []\n",
        "        for j in range(n):\n",
        "            tmp.append(0)\n",
        "        lst.append(tmp)\n",
        "\n",
        "    for i in range(n):\n",
        "        lst[i][i] = 1\n",
        "\n",
        "    return torch.tensor(lst)\n",
        "\n",
        "print(eyes(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWpAlToLo7wq"
      },
      "source": [
        "### torch.tensor끼리의 연산\n",
        "\n",
        "일반적인 사칙연산, 행렬 곱(matmul), 원소간 곱 등등이 다 적용됨."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsvDLKcKkd2e"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "add = a + b  # tensor([5., 7., 9.])\n",
        "sub = a - b  # tensor([-3., -3., -3.])\n",
        "\n",
        "mul = a * b  # tensor([ 4., 10., 18.])\n",
        "div = b / a  # tensor([4.0000, 2.5000, 2.0000])\n",
        "\n",
        "exp = a ** 2  # tensor([1., 4., 9.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAa8J66qkjhp",
        "outputId": "2f80b9fe-facb-43ae-a9ef-b5993d510666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 4,  5,  6],\n",
            "        [ 8, 10, 12],\n",
            "        [12, 15, 18]])\n"
          ]
        }
      ],
      "source": [
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "'''\n",
        "1 2  5 6\n",
        "3 4  7 8\n",
        "\n",
        "1 3\n",
        "2 4\n",
        "'''\n",
        "'''\n",
        "1*5 + 2*7 = 19   1*6 + 2*8 = 22\n",
        "3*5 + 4*7 = 43   3*6 + 4*8 = 50\n",
        "'''\n",
        "\n",
        "#행렬곱을 수행하는 코드\n",
        "matmul = torch.????(matrix_a, matrix_b)\n",
        "\n",
        "# tensor([[19, 22],\n",
        "#         [43, 50]])\n",
        "\n",
        "elem_mul = matrix_a * matrix_b\n",
        "# tensor([[ 5, 12],\n",
        "#         [21, 32]])\n",
        "\n",
        "# 축을 바꾸는 전치를 수행하는 코드\n",
        "transposed = torch.????(matrix_a, 0, 1)\n",
        "# 주로 딥러닝이나 배열 연산에서 사용되며, 다차원 배열 또는 텐서의 축을 바꿔주는 역할\n",
        "# tensor([[1, 3],\n",
        "#         [2, 4]])\n",
        "\n",
        "print(torch.matmul(torch.tensor([[1],[2],[3]]), torch.tensor([[4,5,6]])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Mjhn93pEQ7"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "브로드캐스팅은 서로 다른 크기를 가진 텐서들 간에 연산을 수행할 때, 자동으로 크기를 맞춰주는 PyTorch(및 NumPy)의 기능입니다. 이 기능은 명시적으로 텐서의 크기를 변환하지 않아도, 작은 크기의 텐서를 큰 크기의 텐서와 함께 연산할 수 있도록 해줍니다. Pandas나 Numpy 등에서도 자주 활용되기 때문에 알아두면 좋습니다.\n",
        "\n",
        "브로드캐스팅 규칙:\n",
        "1. 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때, 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "2. 크기 맞추기: 각 차원에서 크기가 1인 텐서는 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "3. 불가능한 경우: 두 텐서가 특정 차원에서 서로 다른 크기를 가지며, 그중 하나가 1이 아니면 브로드캐스팅이 불가능하고 오류가 발생합니다.\n",
        "\n",
        "예를 들어서,\n",
        "\n",
        "- (2,3) 크기의 텐서에 (3,) 크기의 텐서를 더하면, (2,3) 크기의 텐서가 됩니다. 이 때 (3,) 크기의 텐서들은 첫 번째 차원에 대해서 다 더해집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lumy89RRi-6p"
      },
      "outputs": [],
      "source": [
        "# shape (1, 3) / (3, 1)\n",
        "# dim = 0\n",
        "# 3 / 3\n",
        "# dim = 1\n",
        "# 3 / 3\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "VolcxkK_krYh",
        "outputId": "b451e523-4f87-4d88-edc1-1d3085dbae50"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (1537353623.py, line 15)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[26], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6]])    # Shape: (2, 3,)\n",
        "b = torch.tensor([1, 2, 3])                 # Shape: (3,)\n",
        "b = torch.tensor([[1, 2, 3]])               # Shape: (1, 3,)\n",
        "b = torch.tensor([[1, 2, 3], [1, 2, 3]])    # Shape: (2, 3,)\n",
        "\n",
        "broadcast_add = a + b  # Shape: (2, 3)\n",
        "# tensor([[2, 4, 6],\n",
        "#         [5, 7, 9]])\n",
        "\n",
        "a = torch.tensor([[1], [2], [3]])  # Shape: (3, 1)\n",
        "b = torch.tensor([4, 5, 6])        # Shape: (3,)\n",
        "b = torch.tensor([[4, 5, 6]])        # Shape: (1, 3,)\n",
        "b = torch.tensor([[4, 5, 6], [4, 5, 6], [4, 5, 6]])        # Shape: (3, 3,)\n",
        "a = torch.tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]])  # Shape: (3, 3)\n",
        "a*b = [[4, 5, 6], [8, 10, 12], [12, 15, 18]]\n",
        "# To make shapes compatible:\n",
        "# a: (3, 1) -> (3, 3)\n",
        "# b: (3,)   -> (1, 3) -> (3, 3)\n",
        "\n",
        "broadcast_mul = a * b  # Shape: (3, 3)\n",
        "# tensor([[ 4,  5,  6],\n",
        "#         [ 8, 10, 12],\n",
        "#         [12, 15, 18]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_rQSNtHjms5"
      },
      "source": [
        "### 오늘의 복습용 문제\n",
        "\n",
        "1. l = list of list of .... 를 받아서, 하는 함수 get_shape를 작성하세요.\n",
        "  - 여기서 torch.tensor(l).shape 과 같은 값을 가지는 리스트를 반환\n",
        "  - 만약 torch.tensor가 불가능하다면 False를 리턴\n",
        "2. l = list of list of ..., r = list of list of ... 두 input을 받아서, 아래와 같이 동작하는 함수 broadcasting을 작성하세요\n",
        "  - 브로드캐스팅이 될 때 각 l, r이 바뀌어야 하는 형태를 리턴\n",
        "  - 브로드캐스팅이 되지 않으면 False를 리턴\n",
        "\n",
        "[[1,2,3], [1,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAEIj1mbjmI-"
      },
      "outputs": [],
      "source": [
        "def get_shape(lst):\n",
        "    res = []\n",
        "    cur = lst\n",
        "\n",
        "    while isinstance(cur, list):\n",
        "        res.append(len(cur))\n",
        "        cur = cur[0]\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_shape(lst):\n",
        "    if not isinstance(lst, list):\n",
        "        return []\n",
        "    else:\n",
        "        shapes = []\n",
        "        for elem in lst:\n",
        "            shape = get_shape(elem)\n",
        "            if shape not in shapes:\n",
        "                shapes.append(shape)\n",
        "        if len(shapes) == 1:\n",
        "            return [len(lst)] + get_shape(lst[0])\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "def fill(l, r):\n",
        "    \"\"\"If len(l) > len(r), fill 1 to r's front, so that len(l) == len(r),\n",
        "    If len(r) < len(l), do the opposite.\n",
        "    \"\"\"\n",
        "    if len(l) > len(r):\n",
        "        diff = len(l) - len(r)\n",
        "        r = [1 for _ in range(diff)] + r\n",
        "        return l, r\n",
        "    elif len(l) < len(r):\n",
        "        diff = len(r) - len(l)\n",
        "        l = [1 for _ in range(diff)] + l\n",
        "        return l, r\n",
        "    return l, r\n",
        "\n",
        "def expand_dimension(l, dim_idx, r_s):\n",
        "    \"\"\"\n",
        "    l = [[1,2,3,]] (shape 1, 3)\n",
        "    dim_idx = 0\n",
        "    r_s = 4\n",
        "    expand_dimension(l, 0, 4)\n",
        "    >> [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
        "\n",
        "    l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]] (shape 3, 1, 3 -> 3, 2, 3)\n",
        "    l[0] = [[1,2,3]], l[1], l[2] (shape 2, 3)\n",
        "    dim_idx = 1\n",
        "    r_s = 2\n",
        "    expand_dimension(l, 0, 2)\n",
        "    >> [[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]] (shape 3, 2, 3)\n",
        "\n",
        "    l / shape 4, 3, 2, 1, 2 -> 4, 3, 2, 5, 2\n",
        "    expand_dimension(l, 3, 5)\n",
        "    l[0], l[1], l[2], l[3] / shape 3, 2, 1, 2 -> 3, 2, 5, 2\n",
        "    expand_dimension(l[0], 2, 5)\n",
        "    expand_dimension(l[1], 2, 5)\n",
        "    expand_dimension(l[2], 2, 5)\n",
        "    expand_dimension(l[3], 2, 5)\n",
        "    \"\"\"\n",
        "    assert get_shape(l)[dim_idx] == 1, (get_shape(l), dim_idx)\n",
        "\n",
        "    if dim_idx == 0:\n",
        "        return [l[0] for _ in range(r_s)]\n",
        "    else:\n",
        "        return [expand_dimension(e, dim_idx - 1, r_s) for e in l]\n",
        "\n",
        "\n",
        "def broadcasting(l, r):\n",
        "    shape_l = get_shape(l)\n",
        "    shape_r = get_shape(r)\n",
        "\n",
        "    assert shape_l and shape_r\n",
        "\n",
        "    # 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때,\n",
        "    # 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
        "\n",
        "    # (2, 3) / (4, 5, 2, 3) -> (1, 1, 2, 3) / (4, 5, 2, 3)\n",
        "\n",
        "    l_is_bigger = False\n",
        "    r_is_bigger = False\n",
        "    diff = abs(len(shape_l) - len(shape_r))\n",
        "\n",
        "    if len(shape_l) > len(shape_r):\n",
        "        l_is_bigger = True\n",
        "    elif len(shape_l) < len(shape_r):\n",
        "        r_is_bigger = True\n",
        "\n",
        "    shape_l, shape_r = fill(shape_l, shape_r)\n",
        "\n",
        "    for _ in range(diff):\n",
        "        if l_is_bigger:\n",
        "            r = [r]  # r.shape: a1, a2, ... , an / [r].shape : 1, a1, a2, ... , an\n",
        "        elif r_is_bigger:\n",
        "            l = [l]\n",
        "\n",
        "    assert shape_l == get_shape(l)\n",
        "    assert shape_r == get_shape(r)\n",
        "\n",
        "    # 크기 맞추기: 각 차원에서 크기가 1인 텐서는\n",
        "    # 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
        "\n",
        "    dim_idx = 0\n",
        "\n",
        "    for l_s, r_s in zip(shape_l, shape_r):\n",
        "        if l_s != r_s:\n",
        "            if min(l_s, r_s) == 1:\n",
        "                if l_s == 1: #\n",
        "                    l = expand_dimension(l, dim_idx, r_s)\n",
        "                else: # r_s == 1\n",
        "                    r = expand_dimension(r, dim_idx, l_s)\n",
        "            else:\n",
        "                return False\n",
        "        dim_idx += 1\n",
        "\n",
        "    return l, r\n",
        "\n",
        "l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]]\n",
        "print(\"get_shape(I) :\", get_shape(l))\n",
        "\n",
        "r = [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
        "print(get_shape(r))\n",
        "\n",
        "# 3 1 3 / 4 3 -> 3 1 3 / 1 4 3 -> 3 1 3 / 3 4 3 -> 3 4 3 / 3 4 3\n",
        "# r = [[[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
        "r_ans = [[\n",
        "            [1,2,3,],\n",
        "            [1,2,3,],\n",
        "            [1,2,3,],\n",
        "            [1,2,3,]\n",
        "          ],\n",
        "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]],\n",
        "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
        "l_ans = [[[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
        "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]]]\n",
        "l, r = broadcasting(l, r)\n",
        "print(get_shape(l), get_shape(r))\n",
        "print(l_ans == l, r_ans == r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN5jjYflr1oE"
      },
      "source": [
        "### 이 외 tensor operation들\n",
        "\n",
        "- Slicing / Indexing\n",
        "- Reshaping\n",
        "- Concatenation / Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz0GLy02kt-Y"
      },
      "outputs": [],
      "source": [
        "# slicing / indexing\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "\n",
        "# Basic indexing\n",
        "element = tensor[1, 2]  # tensor(6)\n",
        "\n",
        "# Slicing\n",
        "sub_tensor = tensor[:, 1:]  # tensor([[2, 3],\n",
        "                            #         [5, 6],\n",
        "                            #         [8, 9]])\n",
        "\n",
        "# Advanced indexing with masks\n",
        "mask = tensor > 5\n",
        "filtered = tensor[mask]  # tensor([6, 7, 8, 9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiCkGFvjsnBq",
        "outputId": "76d39b6c-0a1a-4280-f827-97a19f263cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ],
      "source": [
        "# reshaping\n",
        "\n",
        "tensor = torch.arange(0, 12)\n",
        "reshaped_view = tensor.view(3, 4)  # tensor([[ 0,  1,  2,  3],\n",
        "                                   #         [ 4,  5,  6,  7],\n",
        "                                   #         [ 8,  9, 10, 11]])\n",
        "\n",
        "reshaped_reshape = tensor.reshape(2, 6)  # tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "                                         #         [ 6,  7,  8,  9, 10, 11]])\n",
        "\n",
        "# tensor.permute\n",
        "tensor = torch.randn(2, 3, 4)\n",
        "permuted = tensor.permute(2, 0, 1)  # Changes the order of dimensions\n",
        "print(permuted.shape)  # torch.Size([4, 2, 3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLZc1-Bo1xjv",
        "outputId": "73973602-592d-4fa0-8c40-f3da55c4bf51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.5915,  0.1231, -0.6552, -0.9403],\n",
              "         [ 0.7802,  0.1173, -0.5841, -0.3675],\n",
              "         [-1.4575,  1.5500,  0.5738, -2.0250]],\n",
              "\n",
              "        [[-0.6440, -1.1911, -0.6119,  1.3835],\n",
              "         [-0.0964, -1.5181,  0.6562, -1.5485],\n",
              "         [ 0.3178,  1.2326, -1.3016,  0.0969]]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIHxbxPTs4Yx"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Concatenate along existing dimension\n",
        "concat = torch.cat((a, b), dim=0)  # tensor([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Stack along a new dimension\n",
        "stack = torch.stack((a, b), dim=0)\n",
        "# tensor([[1, 2, 3],\n",
        "#         [4, 5, 6]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCXr7r3Kr__Z"
      },
      "source": [
        "### 수학적 함수들\n",
        "\n",
        "- abs, sqrt, exp, log 등 unary 함수들 (텐서 하나만을 input으로 받음): torch.abs, torch.sqrt, torch.exp, torch.log\n",
        "- max, min 등 binary 함수들 (텐서 2개를 input으로 받음): torch.max, torch.min\n",
        "- 차원을 하나 혹은 여럿 낮추는 Reduction Operation들: torch.sum(tensor, dim = n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsFdYgJws8kL"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([-1.0, -2.0, 3.0])\n",
        "\n",
        "abs_a = torch.abs(a)          # tensor([1., 2., 3.])\n",
        "# sqrt_a = torch.sqrt(a)\n",
        "sqrt_a = torch.sqrt(torch.abs(a))  # tensor([1., 1.4142, 1.7321])\n",
        "exp_a = torch.exp(a)          # tensor([0.3679, 0.1353, 20.0855])\n",
        "log_a = torch.log(torch.abs(a))    # tensor([0.0000, 0.6931, 1.0986])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYIgfoa7s-gX"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "max_ab = torch.max(a, b)  # tensor([4., 5., 6.])\n",
        "min_ab = torch.min(a, b)  # tensor([1., 2., 3.])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmMzGqgOs_kk"
      },
      "outputs": [],
      "source": [
        "tensor = torch.tensor([[1, 2, 3], [3, 4, 5]]) # (2,3)\n",
        "\n",
        "sum_all = torch.sum(tensor)          # tensor(10)\n",
        "sum_dim0 = torch.sum(tensor, dim=0)  # tensor([4, 6, 8]) (3,)\n",
        "sum_dim1 = torch.sum(tensor, dim=1)  # tensor([6, 12]) (2,)\n",
        "\n",
        "mean_all = torch.mean(tensor.float(), dim = 1)  # tensor(2.5000)\n",
        "print(mean_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLsS1TKhtAn2"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([2, 2, 2])\n",
        "\n",
        "greater = a > b  # tensor([False, False, True])\n",
        "equal = a == b   # tensor([False, True, False])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5udXG_t6Jv"
      },
      "source": [
        "## Pytorch로 다시 해 보는 선형회귀\n",
        "\n",
        "주어진 데이터 $(x_i, y_i)$ 에 대해서 $y=wx+b$에서, 가장 적절한 w와 b를 찾는 것이 선형회귀였음.\n",
        "\n",
        "y = wx + b 에서, w와 b는 parameter이고 x는 입력, y는 출력임.\n",
        "이 때 w랑 b를 구하기 위해서, 다음의 loss function을 최소화하는 방향으로 학습하고 싶다고 하자.\n",
        "\n",
        "$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $\n",
        "\n",
        "원래는 저 값을 그냥 바로 식으로 계산할 수 있었지만, 언제나 그렇지는 않기 때문에 (선형회귀 외의 다른 모델들에서) 수치적으로 계산해보자.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rWCse3hg-8Vs"
      },
      "outputs": [],
      "source": [
        "# 임의로 데이터를 한번 만들어 보자.\n",
        "# True parameters\n",
        "true_w = 2.0\n",
        "true_b = 1.0\n",
        "\n",
        "# Generate data\n",
        "# 표준 정규분포 텐서 랜덤 생성하는 코드가 뭐였죠?\n",
        "X = torch.randn(100, 1) * 10  # 100 samples, single feature\n",
        "y = true_w * X + true_b + torch.randn(100, 1) * 0.2  # Add noise\n",
        "\n",
        "# y[0]= true_w * X[0] + true_b + torch.randn(1, 1) * 2\n",
        "# y[1]= true_w * X[1] + true_b + torch.randn(1, 1) * 2\n",
        "# ...\n",
        "# y[99]= true_w * X[99] + true_b + torch.randn(1, 1) * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMhLRtPy_SED"
      },
      "source": [
        "### requires_grad = True ??\n",
        "\n",
        "- 다음과 같이 경사하강법을 통해 학습을 할때 미분을 통해 나오는 '기울기' 값을 저장을 해야합니다.\n",
        "\n",
        "- requires_grad = True 를 해야 기울기가 저장됩니다.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-16-gradient_descent/pic1.png' width='500'  border='0'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nOG1PrLJ_OlA"
      },
      "outputs": [],
      "source": [
        "# requires_grad = True로 해야 학습이 가능\n",
        "\n",
        "# 기울기를 기록하는 코드 작성\n",
        "w = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9gaSZBtBTyJ"
      },
      "source": [
        "### learning_rate??? 뭐였지??\n",
        "\n",
        "- lr (learning_rate): 학습율, 경사하강법에서 파라미터를 업데이트하는 정도를 조절하기 위한 변수,</br> *보통 0.0001~0.001의 학습율을 사용합니다.\n",
        "\n",
        "✨학습율이 크다고 항상 좋은 것이 아닙니다!\n",
        "\n",
        "<img src=\"https://datadiving.dothome.co.kr/Deep%202-1_6.png\" width=900>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XE_drc9XBTOA"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.009\n",
        "epochs = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UeaB0-MIwM3h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: w = 3.3063, b = 1.3651, loss = 163.7342\n",
            "Epoch 0: w = 4.1392, b = 1.3528, loss = 163.7342\n",
            "Epoch 100: w = 2.0021, b = 1.0674, loss = 0.0406\n",
            "Epoch 200: w = 2.0021, b = 1.0202, loss = 0.0375\n",
            "Epoch 200: w = 2.0021, b = 1.0201, loss = 0.0375\n",
            "Epoch 300: w = 2.0021, b = 1.0125, loss = 0.0374\n",
            "Epoch 400: w = 2.0021, b = 1.0113, loss = 0.0374\n",
            "Epoch 400: w = 2.0021, b = 1.0113, loss = 0.0374\n",
            "Epoch 500: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 700: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 900: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1100: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1300: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1500: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1700: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 1900: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2100: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2300: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2500: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2700: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 2900: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3100: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3300: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3500: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3700: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 3900: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4000: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4100: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4200: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4300: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4400: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4500: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4600: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4700: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4800: w = 2.0021, b = 1.0111, loss = 0.0374\n",
            "Epoch 4900: w = 2.0021, b = 1.0111, loss = 0.0374\n"
          ]
        }
      ],
      "source": [
        "#epoch은 이 데이터를 몇바퀴 돌려서 학습을 시킬 것이냐! 라고 볼 수 있겠죠? 기억나시나요?\n",
        "for epoch in range(epochs):\n",
        "    #우리는 y을 예측하고 싶습니다. y에 영향을 주는 변수는 X하나 라고 생각하고 간단한 선형식을 만들어 보았습니다!\n",
        "    y_pred = X * w + b # y_pred: 100, 1\n",
        "\n",
        "\n",
        "    # Compute and print loss\n",
        "    # loss 함수로 우리는 MSE 평균제곱오차의 식을 사용합니다.\n",
        "    # 모듈을 불러올 수 있겠지만 수학식으로 작성하면 다음과 같습니다.\n",
        "    # 예상한 값 - 실제값의 제곱의 평균\n",
        "    loss = torch.mean((y_pred-y)**2)# 100, 1\n",
        "\n",
        "\n",
        "    # 100 에폭당 우리는 지금 학습이 어느정도 되고 있는지 수치로 보고 싶어요!\n",
        "    # 그래서 if%100이 0일때 print 해보는 코드를 짜보았습니다.\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "    # Forward pass: compute predicted y\n",
        "    # 100, 1 / 1 -> 100, 1 / 1, 1 -> 100, 1 / 100, 1\n",
        "\n",
        "\n",
        "    # Backward pass: compute gradients\n",
        "    # 역전파를 통해 기울기를 계산합니다.\n",
        "    # loss 는 우리가 위에 정의했던 'torch.mean((y_pred - y) ** 2)' 이었습니다!\n",
        "    # 여기서 기울기를 계산하게 되면 독립변수(X) 에 영향을 주었던 w, b 에 대해서 기울기 값이 나오게 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters using gradient descent\n",
        "    # 잠깐 ! 여기서 왜 - 를 할까요?\n",
        "    # 위 그림에서 우리는 기울기가 음수이면 \\ 양수이면 / 모양을 띄는 것을 확인할 수 있었습니다.\n",
        "    # 우리는 경사하강법 그래프에서 볼 수 있든 최저점을 찾아가야 하는데\n",
        "    # 그럼 기울기가 음수이면 오른쪽 양수이면 왼쪽으로 가야하겠죠?\n",
        "    # 즉 음수이면 그래프상 X축 기준으로 + 양수이면 X축 기준으로 - 를 해야하는 것 입니다.이걸 어떻게 할까요?\n",
        "    # 바로 -연산을 하면 됩니다. -에 -를 하면 +가 되는 것은 아시죠?\n",
        "    # 그래서 기울기가 음수일 경우 빼주면 +가 되어 오른쪽으로 이동하고 기울기가 양수일때 빼주면 왼쪽으로 이동하게 되는 것이죠!\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # Zero gradients after updating\n",
        "    # 그리고 이후 학습을 위해 기울기를 초기화 시켜줍니다.\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "    \n",
        "    if epoch % 200 == 0:\n",
        "        print(f'Epoch {epoch}: w = {w.item():.4f}, b = {b.item():.4f}, loss = {loss.item():.4f}')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2IbgzEsuXLv"
      },
      "source": [
        "TODO: 위 선형회귀 부분을 함수로 만들고, 다양한 하이퍼파라미터 (여기서의 hyperparameter은 learning rate 뿐임)를 바꿔가며 최적의 모델을 찾아보세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlzOMd44rinI"
      },
      "source": [
        "### 선형회귀 조금 더 해보기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgue321swjEE"
      },
      "source": [
        "TODO: 이번에는 비슷하게, 입력이 3개이고 출력이 1개인 선형회귀를 해 보자.\n",
        "\n",
        "$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe5Uqs91xjtC"
      },
      "outputs": [],
      "source": [
        "# y = w1*x1 + w2 * x2 + w3 * x3 + b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcrBxNp7LCB"
      },
      "source": [
        "## 딥러닝 들어가기\n",
        "\n",
        "\n",
        "아래 코드는 pytorch 에서 딥러닝 모델을 짤 때, 가장 일반적인 형식이라고 할 수 있다. 각 부분에서 쓰일 함수들은 문제에 따라서 다르지만, 대개의 경우 위 내용이 크게 바뀌지 않을 것임."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUTgBrCI7k8X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "# 랜덤 시드를 고정하는 이유는 고정하지 않으면 매번 다른 값이 나와 모델을 튜닝할때 적절하지 않습니다.\n",
        "# 모델 튜닝을 같은 기준, 환경에서 튜닝을 하기 위함입니다.\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBf3IhLwOYVa"
      },
      "source": [
        "원래대로라면 실제 데이터를 쓰는 것이 좋지만, 실험의 편리함을 위해 인공적으로 만든 합성 데이터를 사용하자.\n",
        "\n",
        "입력 feature는 20개다. 1시간 후에 비가 올지 안 올지 알고 싶은데, 현재 가지고 있는 데이터가 서울 각지의 습도 데이터라고 하자. 20곳에서 동시에 각각 습도를 잰 것이다. 이 데이터를 가지고 비가 오면 1, 비가 오지 않으면 0이라고 하자. 실제 데이터를 안 가지고 있기 때문에, 적당히 수식을 써서 비가 오는 경우와 안 오는 경우를 임의로 구분하여 합성 데이터를 만들자. 이러한 데이터 포인트 1000개를 만들자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AuEAYWDf7h6d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Generate synthetic data\n",
        "num_samples = 1000\n",
        "input_features = 20 #20개의 데이터를 통해 비가 올지(1) 비가 안올지(0) 을 예측 할것\n",
        "\n",
        "# Features: random numbers\n",
        "# 20개의 feature(특성, 독립변수) 랜덤 생성하는 코드를 작성하시오\n",
        "X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "# Labels: sum of features > 0 => class 1, else class 0\n",
        "# Please convince yourself this is the real data, while actuall not the case\n",
        "\n",
        "# Label데이터 생성 비가 올때는 1 비가 안올 때는 0이라는 더미데이터 생성하는 코드\n",
        "# 아래 코드는 X의 20개의 데이터의 합이 양수면 1 음수면 0이라는 더미데이터를 생성하는 코드이다.\n",
        "y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UKaFl-Y8fIAO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.2185198  -0.2550827  -0.2541623   2.1748464  -1.0501808  -2.1677716\n",
            "  1.5362862   0.5310896  -1.0833567  -1.3366777   1.1937752   0.43172172\n",
            "  0.8398959  -0.2646188  -1.0072836  -0.11322445 -0.28038144 -1.3940833\n",
            " -1.6579418   0.55816686]\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(X[0])\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJm3f3k1ZyA6"
      },
      "source": [
        "$x_0 + x_1 + ... + x_{19} >0 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQCWFuWZQikB"
      },
      "source": [
        "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
        "\n",
        "비가 오는지 오지 않는지 어떻게 추정할 수 있을까? 위 수식을 계산하여 결정한다. 비가 오면 값이 커지고, 비가 오지 않으면 값이 작아지도록 한다. 0을 기준으로 양수이면 비가 오는 것으로 판단, 음수면 비가 오지 않는 것으로 판단한다.\n",
        "\n",
        "`X`는 주어진 데이터이니, 우리는 원하는 값을 계산할 수 있도록 해 주는 `w`와 `b`의 값을 구하면 된다.\n",
        "\n",
        "일단 랜덤으로 값을 초기화해 놓고, 이후 경사하강법으로 좋은 값을 찾아간다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "a_h2Shu8TRu3"
      },
      "outputs": [],
      "source": [
        "# 위에서 말했든 실제 딥러닝에서도 초기 가중치와 편향 값은 랜덤으로 찍고 시작을 합니다.\n",
        "# 그리고 기울기 값을 기록하기 위해 requires_grad=True ! 필수!!\n",
        "# 가중치의 개수는 특성의 개수와 같아야 겠죠? 아래 빈칸을 채워볼까요?\n",
        "w = torch.randn(input_features, requires_grad=True) \n",
        "b = torch.randn(1, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-a0lSLgYY6C"
      },
      "source": [
        "- w를 출력해보면 20개의 랜덤값이 있습니다.\n",
        "- Why?? X의 특성 개수 20개에 각각 다른 가중치를 적용하기 위해 w를 생성할 때 파라미터로 input_features(20)을 넣어주었기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "WTHZasM_YUJK",
        "outputId": "6f8bc179-14e5-49ec-c7ec-13b34df486af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-2.0314, -0.3655, -0.6214,  0.4122, -0.1437, -0.2074,  0.0540, -0.2917,\n",
              "        -1.0980, -0.2547, -0.0073,  1.9353, -0.9218,  0.3507,  1.3779,  0.7322,\n",
              "         0.2002, -0.3371,  0.1876, -0.8091], requires_grad=True)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yk-utOrT9EJ"
      },
      "source": [
        "본격적으로 답을 찾기 전에, 랜덤으로 만든 값들이 얼마나 유용한지 테스트해보자.\n",
        "\n",
        "혹시라도 랜덤으로 만들었는데 이미 잘 맞춘다면 우리가 이 고생을 할 필요가 없다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "qKcEAZwnULNJ",
        "outputId": "c98cc813-5e49-4f64-f4bd-5e7beb72d0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.484\n"
          ]
        }
      ],
      "source": [
        "# 일반적으로 이진분류(O or X)에서 찍는 다고 생각하면 50%의 확률이다\n",
        "# 아래 코드는 지금 당장은 이해할 필요없고 일단 랜덤으로 찍었을 때 얼마정도의 확률이 나오는지 재미삼아 계산을 해본 것 입니다.\n",
        "cnt_total = 0\n",
        "cnt_correct = 0\n",
        "for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "    with torch.no_grad():\n",
        "        score = torch.sum(x_i*w) + b\n",
        "    predict_flag = score > 0\n",
        "    answer_flag = y_i == 1\n",
        "\n",
        "    if predict_flag == answer_flag:\n",
        "        cnt_correct += 1\n",
        "    cnt_total += 1\n",
        "\n",
        "print(f'Accuracy: {cnt_correct/cnt_total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8wtWG0fTqzS"
      },
      "source": [
        "다음은 놀랍게도 실제 뉴럴네트워크의 학습 과정이다.\n",
        "```\n",
        "면접관: 당신의 장점은?\n",
        "나: 저는 머신러닝 전문가입니다.\n",
        "면접관: 9+10은?\n",
        "나: 3 입니다.\n",
        "면접관: 틀렸네. 전혀 달라. 답은 19일세.\n",
        "나: 16 입니다.\n",
        "면접관: 틀렸네. 답은 19일세.\n",
        "나: 18 입니다.\n",
        "면접관: 틀렸네. 답은 19일세.\n",
        "나: 19 입니다.\n",
        "면접관: 자넨 합격일세.\n",
        "```\n",
        "\n",
        "다만 아래와 같이 좀 더 정확하게 고칠 수 있다.\n",
        "\n",
        "```\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
        "신경망: 수식을 계산한 결과 8이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
        "훈련교관: 답은 비가 오지 않는다네.\n",
        "신경망: 명심하겠습니다.\n",
        "\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가?\n",
        "신경망: 수식을 계산한 결과 3이 나왔습니다. 0보다 크니 비가 올 것입니다.\n",
        "훈련교관: 답은 비가 온다네.\n",
        "신경망: 명심하겠습니다.\n",
        "\n",
        "훈련교관: 현재 서울 곳곳의 습도는 이러하다. 1시간 후에 비가 오겠는가? (1. 학습 데이터 샘플링)\n",
        "신경망: 수식을 계산한 결과 -10이 나왔습니다. 0보다 작으니 비가 안 올 것입니다. (2. 추론)\n",
        "훈련교관: 답은 비가 오지 않는다네.\n",
        "신경망: 명심하겠습니다. (3. 손실함수에서 역전파된 그래디언트를 바탕으로 파라미터 조정)\n",
        "```\n",
        "위에서 이미 1단계와 2단계를 끝냈다.\n",
        "이제 위 코드에 약간 추가해 3단계를 수행하자.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mDSr6JRaTwu"
      },
      "source": [
        "손실함수를 계산하기 이전에 손실함수에 대해 짚고 넘어가는 것이 좋겠다. `torch.nn.BCEWithLogitsLoss()`가 우리가 사용할 손실함수다. 이 함수의 수학적 구조에 대해 논하는 것은 이론 시간의 역할이고, 실습 시간에는 이 손실함수가 우리가 원하는 성질을 만족시키는지만 간단히 체크하고 넘어가자.\n",
        "\n",
        "손실함수는 말 그대로의 의미를 지닌다. 손실이다. 크면 안 된다. 우리는 손실을 줄여야 한다. 좀 더 정확히는, 절대적인 크기보다는 상대적인 크기가 중요하다.\n",
        "\n",
        "신경망은 학습을 통해 나쁜 상태에서 좋은 상태로 나아간다. 나쁜 상태는 비가 오는지 오지 않는지 잘 예측하지 못하는 상태이고, 좋은 상태는 그 반대다. 현재 신경망이 처한 상태가 얼마나 좋은지, 또는 얼마나 나쁜지 알려주는 가이드가 손실함수다.\n",
        "\n",
        "우리에게 $s_1$과 $s_2$라는 두 가지의 상태가 있고, $s_1$이 더 바람직한 상태라고 하자. 손실함수를 $f$라고 하면, 다음을 만족해야 한다.\n",
        "\n",
        "$$f(s_1) < f(s_2)$$\n",
        "\n",
        "(상태란 단적으로 말하면 `w`와 `b`의 값을 말한다.)\n",
        "\n",
        "$$x_0 \\cdot w_0 + \\cdots + x_{19} \\cdot w_{19} + b$$\n",
        "\n",
        "A. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 온다면 현재 신경망은 좋은 상태에 있는 것이다.\n",
        "\n",
        "B. 만약 신경망이 수식을 계산한 결과가 10이고, 이 때 비가 오지 않는다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
        "\n",
        "C. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 온다면 현재 신경망은 나쁜 상태에 있는 것이다.\n",
        "\n",
        "D. 만약 신경망이 수식을 계산한 결과가 -6이고, 이 때 비가 오지 않는다면 현재 신경망은 좋은 상태에 있는 것이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmPIevn-a9qm",
        "outputId": "e6a40d8a-7f01-4bee-ef69-be0e3febb95b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.5399e-05)\n",
            "tensor(10.0000)\n",
            "tensor(6.0025)\n",
            "tensor(0.0025)\n"
          ]
        }
      ],
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss() # 분류문제는 이걸쓰는게 좋다\n",
        "# criterion = torch.nn.MSELoss\n",
        "# 비가 오면 1, 안 오면 0\n",
        "\n",
        "# A\n",
        "a = criterion(torch.tensor([10.]), torch.tensor([1.]))\n",
        "print(a)\n",
        "# B\n",
        "b = criterion(torch.tensor([10.]), torch.tensor([0.]))\n",
        "print(b)\n",
        "# C\n",
        "c = criterion(torch.tensor([-6.]), torch.tensor([1.]))\n",
        "print(c)\n",
        "# D\n",
        "d = criterion(torch.tensor([-6.]), torch.tensor([0.]))\n",
        "print(d)\n",
        "\n",
        "assert a < b\n",
        "assert a < c\n",
        "assert d < b\n",
        "assert d < c\n",
        "# 위 코드에 대한 이해가 어려우시다면 BCEWithLogitsLoss 랴는 손실함수는 숫자가 높으면 yes에 가깝고 숫자가 낮으면 no를\n",
        "# 뜻한다고 추상적으로 이해를 해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puRwK8PvZ9a-"
      },
      "source": [
        "손실함수가 원하는 성질을 가진다는 것을 확인했으니 이제 신경망의 학습과정을 구현하자. 학습이 진행되며 손실함수가 점점 줄어들어야 정상이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "3tJQUKTKhKsK",
        "outputId": "195b1269-9fac-4cf6-c9a2-ca1c742e90f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300, Loss: 0.039628\n",
            "Epoch 2/300, Loss: 0.039602\n",
            "Epoch 3/300, Loss: 0.039576\n",
            "Epoch 4/300, Loss: 0.039551\n",
            "Epoch 5/300, Loss: 0.039525\n",
            "Epoch 6/300, Loss: 0.039499\n",
            "Epoch 7/300, Loss: 0.039474\n",
            "Epoch 8/300, Loss: 0.039448\n",
            "Epoch 9/300, Loss: 0.039423\n",
            "Epoch 10/300, Loss: 0.039397\n",
            "Epoch 11/300, Loss: 0.039372\n",
            "Epoch 12/300, Loss: 0.039347\n",
            "Epoch 13/300, Loss: 0.039322\n",
            "Epoch 14/300, Loss: 0.039296\n",
            "Epoch 15/300, Loss: 0.039271\n",
            "Epoch 16/300, Loss: 0.039246\n",
            "Epoch 17/300, Loss: 0.039221\n",
            "Epoch 18/300, Loss: 0.039196\n",
            "Epoch 19/300, Loss: 0.039171\n",
            "Epoch 20/300, Loss: 0.039147\n",
            "Epoch 21/300, Loss: 0.039122\n",
            "Epoch 22/300, Loss: 0.039097\n",
            "Epoch 23/300, Loss: 0.039072\n",
            "Epoch 24/300, Loss: 0.039048\n",
            "Epoch 25/300, Loss: 0.039023\n",
            "Epoch 26/300, Loss: 0.038999\n",
            "Epoch 27/300, Loss: 0.038974\n",
            "Epoch 28/300, Loss: 0.038950\n",
            "Epoch 29/300, Loss: 0.038925\n",
            "Epoch 30/300, Loss: 0.038901\n",
            "Epoch 31/300, Loss: 0.038877\n",
            "Epoch 32/300, Loss: 0.038853\n",
            "Epoch 33/300, Loss: 0.038829\n",
            "Epoch 34/300, Loss: 0.038804\n",
            "Epoch 35/300, Loss: 0.038780\n",
            "Epoch 36/300, Loss: 0.038756\n",
            "Epoch 37/300, Loss: 0.038732\n",
            "Epoch 38/300, Loss: 0.038709\n",
            "Epoch 39/300, Loss: 0.038685\n",
            "Epoch 40/300, Loss: 0.038661\n",
            "Epoch 41/300, Loss: 0.038637\n",
            "Epoch 42/300, Loss: 0.038614\n",
            "Epoch 43/300, Loss: 0.038590\n",
            "Epoch 44/300, Loss: 0.038566\n",
            "Epoch 45/300, Loss: 0.038543\n",
            "Epoch 46/300, Loss: 0.038519\n",
            "Epoch 47/300, Loss: 0.038496\n",
            "Epoch 48/300, Loss: 0.038472\n",
            "Epoch 49/300, Loss: 0.038449\n",
            "Epoch 50/300, Loss: 0.038426\n",
            "Epoch 51/300, Loss: 0.038402\n",
            "Epoch 52/300, Loss: 0.038379\n",
            "Epoch 53/300, Loss: 0.038356\n",
            "Epoch 54/300, Loss: 0.038333\n",
            "Epoch 55/300, Loss: 0.038310\n",
            "Epoch 56/300, Loss: 0.038287\n",
            "Epoch 57/300, Loss: 0.038264\n",
            "Epoch 58/300, Loss: 0.038241\n",
            "Epoch 59/300, Loss: 0.038218\n",
            "Epoch 60/300, Loss: 0.038195\n",
            "Epoch 61/300, Loss: 0.038173\n",
            "Epoch 62/300, Loss: 0.038150\n",
            "Epoch 63/300, Loss: 0.038127\n",
            "Epoch 64/300, Loss: 0.038104\n",
            "Epoch 65/300, Loss: 0.038082\n",
            "Epoch 66/300, Loss: 0.038059\n",
            "Epoch 67/300, Loss: 0.038037\n",
            "Epoch 68/300, Loss: 0.038014\n",
            "Epoch 69/300, Loss: 0.037992\n",
            "Epoch 70/300, Loss: 0.037970\n",
            "Epoch 71/300, Loss: 0.037947\n",
            "Epoch 72/300, Loss: 0.037925\n",
            "Epoch 73/300, Loss: 0.037903\n",
            "Epoch 74/300, Loss: 0.037881\n",
            "Epoch 75/300, Loss: 0.037858\n",
            "Epoch 76/300, Loss: 0.037836\n",
            "Epoch 77/300, Loss: 0.037814\n",
            "Epoch 78/300, Loss: 0.037792\n",
            "Epoch 79/300, Loss: 0.037770\n",
            "Epoch 80/300, Loss: 0.037748\n",
            "Epoch 81/300, Loss: 0.037726\n",
            "Epoch 82/300, Loss: 0.037705\n",
            "Epoch 83/300, Loss: 0.037683\n",
            "Epoch 84/300, Loss: 0.037661\n",
            "Epoch 85/300, Loss: 0.037639\n",
            "Epoch 86/300, Loss: 0.037618\n",
            "Epoch 87/300, Loss: 0.037596\n",
            "Epoch 88/300, Loss: 0.037574\n",
            "Epoch 89/300, Loss: 0.037553\n",
            "Epoch 90/300, Loss: 0.037531\n",
            "Epoch 91/300, Loss: 0.037510\n",
            "Epoch 92/300, Loss: 0.037489\n",
            "Epoch 93/300, Loss: 0.037467\n",
            "Epoch 94/300, Loss: 0.037446\n",
            "Epoch 95/300, Loss: 0.037425\n",
            "Epoch 96/300, Loss: 0.037403\n",
            "Epoch 97/300, Loss: 0.037382\n",
            "Epoch 98/300, Loss: 0.037361\n",
            "Epoch 99/300, Loss: 0.037340\n",
            "Epoch 100/300, Loss: 0.037319\n",
            "Epoch 101/300, Loss: 0.037298\n",
            "Epoch 102/300, Loss: 0.037277\n",
            "Epoch 103/300, Loss: 0.037256\n",
            "Epoch 104/300, Loss: 0.037235\n",
            "Epoch 105/300, Loss: 0.037214\n",
            "Epoch 106/300, Loss: 0.037193\n",
            "Epoch 107/300, Loss: 0.037172\n",
            "Epoch 108/300, Loss: 0.037151\n",
            "Epoch 109/300, Loss: 0.037131\n",
            "Epoch 110/300, Loss: 0.037110\n",
            "Epoch 111/300, Loss: 0.037089\n",
            "Epoch 112/300, Loss: 0.037069\n",
            "Epoch 113/300, Loss: 0.037048\n",
            "Epoch 114/300, Loss: 0.037028\n",
            "Epoch 115/300, Loss: 0.037007\n",
            "Epoch 116/300, Loss: 0.036987\n",
            "Epoch 117/300, Loss: 0.036966\n",
            "Epoch 118/300, Loss: 0.036946\n",
            "Epoch 119/300, Loss: 0.036926\n",
            "Epoch 120/300, Loss: 0.036905\n",
            "Epoch 121/300, Loss: 0.036885\n",
            "Epoch 122/300, Loss: 0.036865\n",
            "Epoch 123/300, Loss: 0.036845\n",
            "Epoch 124/300, Loss: 0.036824\n",
            "Epoch 125/300, Loss: 0.036804\n",
            "Epoch 126/300, Loss: 0.036784\n",
            "Epoch 127/300, Loss: 0.036764\n",
            "Epoch 128/300, Loss: 0.036744\n",
            "Epoch 129/300, Loss: 0.036724\n",
            "Epoch 130/300, Loss: 0.036704\n",
            "Epoch 131/300, Loss: 0.036685\n",
            "Epoch 132/300, Loss: 0.036665\n",
            "Epoch 133/300, Loss: 0.036645\n",
            "Epoch 134/300, Loss: 0.036625\n",
            "Epoch 135/300, Loss: 0.036605\n",
            "Epoch 136/300, Loss: 0.036586\n",
            "Epoch 137/300, Loss: 0.036566\n",
            "Epoch 138/300, Loss: 0.036546\n",
            "Epoch 139/300, Loss: 0.036527\n",
            "Epoch 140/300, Loss: 0.036507\n",
            "Epoch 141/300, Loss: 0.036488\n",
            "Epoch 142/300, Loss: 0.036468\n",
            "Epoch 143/300, Loss: 0.036449\n",
            "Epoch 144/300, Loss: 0.036429\n",
            "Epoch 145/300, Loss: 0.036410\n",
            "Epoch 146/300, Loss: 0.036391\n",
            "Epoch 147/300, Loss: 0.036371\n",
            "Epoch 148/300, Loss: 0.036352\n",
            "Epoch 149/300, Loss: 0.036333\n",
            "Epoch 150/300, Loss: 0.036314\n",
            "Epoch 151/300, Loss: 0.036294\n",
            "Epoch 152/300, Loss: 0.036275\n",
            "Epoch 153/300, Loss: 0.036256\n",
            "Epoch 154/300, Loss: 0.036237\n",
            "Epoch 155/300, Loss: 0.036218\n",
            "Epoch 156/300, Loss: 0.036199\n",
            "Epoch 157/300, Loss: 0.036180\n",
            "Epoch 158/300, Loss: 0.036161\n",
            "Epoch 159/300, Loss: 0.036142\n",
            "Epoch 160/300, Loss: 0.036123\n",
            "Epoch 161/300, Loss: 0.036105\n",
            "Epoch 162/300, Loss: 0.036086\n",
            "Epoch 163/300, Loss: 0.036067\n",
            "Epoch 164/300, Loss: 0.036048\n",
            "Epoch 165/300, Loss: 0.036030\n",
            "Epoch 166/300, Loss: 0.036011\n",
            "Epoch 167/300, Loss: 0.035992\n",
            "Epoch 168/300, Loss: 0.035974\n",
            "Epoch 169/300, Loss: 0.035955\n",
            "Epoch 170/300, Loss: 0.035936\n",
            "Epoch 171/300, Loss: 0.035918\n",
            "Epoch 172/300, Loss: 0.035899\n",
            "Epoch 173/300, Loss: 0.035881\n",
            "Epoch 174/300, Loss: 0.035863\n",
            "Epoch 175/300, Loss: 0.035844\n",
            "Epoch 176/300, Loss: 0.035826\n",
            "Epoch 177/300, Loss: 0.035808\n",
            "Epoch 178/300, Loss: 0.035789\n",
            "Epoch 179/300, Loss: 0.035771\n",
            "Epoch 180/300, Loss: 0.035753\n",
            "Epoch 181/300, Loss: 0.035735\n",
            "Epoch 182/300, Loss: 0.035716\n",
            "Epoch 183/300, Loss: 0.035698\n",
            "Epoch 184/300, Loss: 0.035680\n",
            "Epoch 185/300, Loss: 0.035662\n",
            "Epoch 186/300, Loss: 0.035644\n",
            "Epoch 187/300, Loss: 0.035626\n",
            "Epoch 188/300, Loss: 0.035608\n",
            "Epoch 189/300, Loss: 0.035590\n",
            "Epoch 190/300, Loss: 0.035572\n",
            "Epoch 191/300, Loss: 0.035554\n",
            "Epoch 192/300, Loss: 0.035537\n",
            "Epoch 193/300, Loss: 0.035519\n",
            "Epoch 194/300, Loss: 0.035501\n",
            "Epoch 195/300, Loss: 0.035483\n",
            "Epoch 196/300, Loss: 0.035465\n",
            "Epoch 197/300, Loss: 0.035448\n",
            "Epoch 198/300, Loss: 0.035430\n",
            "Epoch 199/300, Loss: 0.035412\n",
            "Epoch 200/300, Loss: 0.035395\n",
            "Epoch 201/300, Loss: 0.035377\n",
            "Epoch 202/300, Loss: 0.035360\n",
            "Epoch 203/300, Loss: 0.035342\n",
            "Epoch 204/300, Loss: 0.035325\n",
            "Epoch 205/300, Loss: 0.035307\n",
            "Epoch 206/300, Loss: 0.035290\n",
            "Epoch 207/300, Loss: 0.035272\n",
            "Epoch 208/300, Loss: 0.035255\n",
            "Epoch 209/300, Loss: 0.035238\n",
            "Epoch 210/300, Loss: 0.035220\n",
            "Epoch 211/300, Loss: 0.035203\n",
            "Epoch 212/300, Loss: 0.035186\n",
            "Epoch 213/300, Loss: 0.035169\n",
            "Epoch 214/300, Loss: 0.035151\n",
            "Epoch 215/300, Loss: 0.035134\n",
            "Epoch 216/300, Loss: 0.035117\n",
            "Epoch 217/300, Loss: 0.035100\n",
            "Epoch 218/300, Loss: 0.035083\n",
            "Epoch 219/300, Loss: 0.035066\n",
            "Epoch 220/300, Loss: 0.035049\n",
            "Epoch 221/300, Loss: 0.035032\n",
            "Epoch 222/300, Loss: 0.035015\n",
            "Epoch 223/300, Loss: 0.034998\n",
            "Epoch 224/300, Loss: 0.034981\n",
            "Epoch 225/300, Loss: 0.034964\n",
            "Epoch 226/300, Loss: 0.034947\n",
            "Epoch 227/300, Loss: 0.034930\n",
            "Epoch 228/300, Loss: 0.034913\n",
            "Epoch 229/300, Loss: 0.034897\n",
            "Epoch 230/300, Loss: 0.034880\n",
            "Epoch 231/300, Loss: 0.034863\n",
            "Epoch 232/300, Loss: 0.034846\n",
            "Epoch 233/300, Loss: 0.034830\n",
            "Epoch 234/300, Loss: 0.034813\n",
            "Epoch 235/300, Loss: 0.034796\n",
            "Epoch 236/300, Loss: 0.034780\n",
            "Epoch 237/300, Loss: 0.034763\n",
            "Epoch 238/300, Loss: 0.034747\n",
            "Epoch 239/300, Loss: 0.034730\n",
            "Epoch 240/300, Loss: 0.034714\n",
            "Epoch 241/300, Loss: 0.034697\n",
            "Epoch 242/300, Loss: 0.034681\n",
            "Epoch 243/300, Loss: 0.034664\n",
            "Epoch 244/300, Loss: 0.034648\n",
            "Epoch 245/300, Loss: 0.034632\n",
            "Epoch 246/300, Loss: 0.034615\n",
            "Epoch 247/300, Loss: 0.034599\n",
            "Epoch 248/300, Loss: 0.034583\n",
            "Epoch 249/300, Loss: 0.034567\n",
            "Epoch 250/300, Loss: 0.034550\n",
            "Epoch 251/300, Loss: 0.034534\n",
            "Epoch 252/300, Loss: 0.034518\n",
            "Epoch 253/300, Loss: 0.034502\n",
            "Epoch 254/300, Loss: 0.034486\n",
            "Epoch 255/300, Loss: 0.034469\n",
            "Epoch 256/300, Loss: 0.034453\n",
            "Epoch 257/300, Loss: 0.034437\n",
            "Epoch 258/300, Loss: 0.034421\n",
            "Epoch 259/300, Loss: 0.034405\n",
            "Epoch 260/300, Loss: 0.034389\n",
            "Epoch 261/300, Loss: 0.034373\n",
            "Epoch 262/300, Loss: 0.034357\n",
            "Epoch 263/300, Loss: 0.034342\n",
            "Epoch 264/300, Loss: 0.034326\n",
            "Epoch 265/300, Loss: 0.034310\n",
            "Epoch 266/300, Loss: 0.034294\n",
            "Epoch 267/300, Loss: 0.034278\n",
            "Epoch 268/300, Loss: 0.034262\n",
            "Epoch 269/300, Loss: 0.034247\n",
            "Epoch 270/300, Loss: 0.034231\n",
            "Epoch 271/300, Loss: 0.034215\n",
            "Epoch 272/300, Loss: 0.034200\n",
            "Epoch 273/300, Loss: 0.034184\n",
            "Epoch 274/300, Loss: 0.034168\n",
            "Epoch 275/300, Loss: 0.034153\n",
            "Epoch 276/300, Loss: 0.034137\n",
            "Epoch 277/300, Loss: 0.034122\n",
            "Epoch 278/300, Loss: 0.034106\n",
            "Epoch 279/300, Loss: 0.034090\n",
            "Epoch 280/300, Loss: 0.034075\n",
            "Epoch 281/300, Loss: 0.034060\n",
            "Epoch 282/300, Loss: 0.034044\n",
            "Epoch 283/300, Loss: 0.034029\n",
            "Epoch 284/300, Loss: 0.034013\n",
            "Epoch 285/300, Loss: 0.033998\n",
            "Epoch 286/300, Loss: 0.033983\n",
            "Epoch 287/300, Loss: 0.033967\n",
            "Epoch 288/300, Loss: 0.033952\n",
            "Epoch 289/300, Loss: 0.033937\n",
            "Epoch 290/300, Loss: 0.033921\n",
            "Epoch 291/300, Loss: 0.033906\n",
            "Epoch 292/300, Loss: 0.033891\n",
            "Epoch 293/300, Loss: 0.033876\n",
            "Epoch 294/300, Loss: 0.033861\n",
            "Epoch 295/300, Loss: 0.033845\n",
            "Epoch 296/300, Loss: 0.033830\n",
            "Epoch 297/300, Loss: 0.033815\n",
            "Epoch 298/300, Loss: 0.033800\n",
            "Epoch 299/300, Loss: 0.033785\n",
            "Epoch 300/300, Loss: 0.033770\n"
          ]
        }
      ],
      "source": [
        "# 손실함수 정의\n",
        "# 이진 분류를 수행하는 손실함수 코드를 작성해봅시다.\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 데이터를 몇번 돌려서 학습을 할지 정의\n",
        "num_epoch = 300\n",
        "\n",
        "# 아까 봤던\n",
        "'''\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "'''\n",
        "#위 코드 처럼 가중치와 편향의 기울기에 얼만큼의 learning_rate(학습률)을 적용해 얼만큼 많이, 또는 적게 반영할 것인지 정의\n",
        "learning_rate = 0.003\n",
        "\n",
        "# 가중치, 편향 초기화\n",
        "w.grad = torch.zeros_like(w) # Initialize gradient\n",
        "b.grad = torch.zeros_like(b) # Initialize gradient\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    loss_sum = 0\n",
        "    # 자 아까 X_tensor, y_tensor가 뭐였죠? 랜덤하게 생성된 20개의 특성 X_tensor\n",
        "    # 그리고 랜덤하게 생성된 1(yes), 0(no)의 값 y_tensor 입니다.\n",
        "    # 아래와 같이 for 문을 작성하면 x_i에는 20개의 특성, y_i에는 0,1중 하나의 값이 들어가겠죠?\n",
        "    # 아래 ????에 들어가야 할 값은 데이터 입니다!\n",
        "    for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "        # Evaluation code is commented out because not needed\n",
        "        # predict_flag = (x_i*w).sum() + b > 0\n",
        "        # answer_flag = y_i == 1\n",
        "\n",
        "        #가중치의 기울기 초기화하는 코드를 작성해보세요\n",
        "        w.grad.zero_() # Clear gradient\n",
        "        b.grad.zero_() # Clear gradient\n",
        "\n",
        "        # score = x_1 * w_1 + x_2 * w_2 + ... + x_19 * w_19 + b\n",
        "        score = torch.sum(x_i*w) + b\n",
        "        # 손실함수 계산을 위해 squeeze() 함수로 차원을 맞춥니다.\n",
        "        loss = criterion(score.squeeze(), y_i.squeeze())\n",
        "        # 아래는 역전파 코드입니다.\n",
        "        loss.backward()\n",
        "        # X_tensor 는 20개의 특성데이터 num_samples(1000개)가 들어가 있습니다.\n",
        "        # 1000개의 loss를 다 더한 후 1000개로 나누어 평균 loss를 구할 예정입니다.\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        #가중치를 조정할때는  기울기의 계산이 필요하지 않기 때문에 with 구문을 실행\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epoch}, Loss: {loss_sum/num_samples:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYcNr1USoMj6"
      },
      "source": [
        "학습이 어느정도 진행되었으면 정확도를 다시 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "uXw8OHoGoS-i",
        "outputId": "99029a79-5954-4ccc-fbb2-5b80464e9991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.993\n"
          ]
        }
      ],
      "source": [
        "cnt_total = 0\n",
        "cnt_correct = 0\n",
        "for x_i, y_i in zip(X_tensor, y_tensor):\n",
        "\n",
        "    # 평가를 할 때에도 학습을 하는 것이 아니기 때문에 기울기를 기록할 필요 없습니다. 어떤 코드를 작성해야 하죠??\n",
        "    with torch.no_grad():\n",
        "        score = torch.sum(x_i*w) + b\n",
        "    predict_flag = score > 0\n",
        "    answer_flag = y_i == 1\n",
        "\n",
        "    if predict_flag == answer_flag:\n",
        "        cnt_correct += 1\n",
        "    cnt_total += 1\n",
        "\n",
        "print(f'Accuracy: {cnt_correct/cnt_total}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "L2jJ4Gglg7st"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 1.2185, -0.2551, -0.2542,  2.1748, -1.0502, -2.1678,  1.5363,  0.5311,\n",
            "        -1.0834, -1.3367,  1.1938,  0.4317,  0.8399, -0.2646, -1.0073, -0.1132,\n",
            "        -0.2804, -1.3941, -1.6579,  0.5582])\n",
            "tensor([0.])\n"
          ]
        }
      ],
      "source": [
        "#데이터 첫번째 샘플을 다음과 같이 출력하면 값이 나온다.\n",
        "print(X_tensor[0])\n",
        "print(y_tensor[0])\n",
        "#위 딥러닝 아키텍쳐를 그려보자면 아래와 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeSGLZA3e8OI"
      },
      "source": [
        "### 위 X_tensor 와 y_tensor 의 첫번째 값을 출력하면\n",
        "\n",
        "    tensor([ 1.7641,  0.4002,  0.9787,  2.2409,  1.8676, -0.9773,  0.9501, -0.1514,\n",
        "            -0.1032,  0.4106,  0.1440,  1.4543,  0.7610,  0.1217,  0.4439,  0.3337,\n",
        "            1.4941, -0.2052,  0.3131, -0.8541])\n",
        "            \n",
        "    tensor([1.])\n",
        "\n",
        "위와 같이 나왔다\n",
        "위 데이터를 바탕으로 딥러닝 아키텍쳐를 그려보자면 다음과 같은\n",
        "\"히든레이어가 없는\" 단일 레이어로 구성된 신경망이라고 볼 수 있다.\n",
        "\n",
        "<img src='https://drive.google.com/drive-viewer/AKGpihaD8lib2wGDNwkjxbQ9bBGGGG2o-KYN0z_HNIJVzNNSUwVgMLkrlFtpnUQ1WPfc7GPGb5pEPkrMBjVd6m5MKfWNjlYnLlNWIZ0=w2880-h1428-rw-v1' width='500'  border='0'></a>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyArh-UHrPYn"
      },
      "source": [
        "아래는 같은 문제를 다층 퍼셉트론, `torch.nn.Module`, `torch.optim` 등을 통해 해결한 것이다. 위는 원리를 설명하기 위한 실습이고, 현업에서는 아래와 같은 코드를 작성하는 것이 좋다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ba9azyGIjsKP",
        "outputId": "14eb704c-267d-471b-a8e3-46bd57589b89"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleMLP\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, output_size):\n\u001b[0;32m      3\u001b[0m       \u001b[38;5;66;03m#파이토치의 뉴럴넷의 기본은 ! 클래스 상속 기능입니다 ! 아래 코드를 작성해주세요!\u001b[39;00m\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(SimpleMLP, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "      #파이토치의 뉴럴넷의 기본은 ! 클래스 상속 기능입니다 ! 아래 코드를 작성해주세요!\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "\n",
        "        # 레이어를 쌓는곳 위에 예시는 단일레이어 였지만 실제로는 여러개의 hidden 레이어를 쌓아서 사용한다.\n",
        "        # 아래에는 두개의 레이어를 쌓는 것을 볼 수 있다.\n",
        "        # 선형 레이어를 쌓는 코드\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
        "        # 활성화 함수를 이용해 선형식의 한계를 극복해 봅시다 !\n",
        "        self.relu = nn.ReLU()                 # ReLU activation\n",
        "        # 두번째 선형 레이어\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x 가 들어와서 첫번째 히든레이어 통과\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        # 히든레이어가 통과한 값이 들어와서 ReLU 함수 적용\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        # ReLU 함수가 적용된 값이 다시 두번째 히든레이어에 들어가고 최종 output인\n",
        "        # out으로 값을 return\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyu8iWdzjtAZ"
      },
      "source": [
        "### 잠깐 ! ReLU 뭐였지??\n",
        "\n",
        " - 은닉계층의 활성함수로는 **ReLU** 함수를 주로 사용합니다.\n",
        "\n",
        "    <img src=\"https://datadiving.dothome.co.kr/Deep%202-1_3.png\" width=600>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXe436j_7Uda",
        "outputId": "58f70acb-1c53-4fd6-a7c4-cbf1fe5dc378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.5969\n",
            "Epoch [2/20], Loss: 0.5278\n",
            "Epoch [3/20], Loss: 0.4573\n",
            "Epoch [4/20], Loss: 0.3623\n",
            "Epoch [5/20], Loss: 0.2727\n",
            "Epoch [6/20], Loss: 0.2250\n",
            "Epoch [7/20], Loss: 0.2524\n",
            "Epoch [8/20], Loss: 0.1101\n",
            "Epoch [9/20], Loss: 0.2015\n",
            "Epoch [10/20], Loss: 0.1142\n",
            "Epoch [11/20], Loss: 0.1070\n",
            "Epoch [12/20], Loss: 0.3423\n",
            "Epoch [13/20], Loss: 0.0497\n",
            "Epoch [14/20], Loss: 0.1490\n",
            "Epoch [15/20], Loss: 0.1160\n",
            "Epoch [16/20], Loss: 0.0885\n",
            "Epoch [17/20], Loss: 0.0397\n",
            "Epoch [18/20], Loss: 0.0154\n",
            "Epoch [19/20], Loss: 0.0482\n",
            "Epoch [20/20], Loss: 0.0436\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Create a dataset and data loader\n",
        "# DataLoader 모듈을 사용하여 데이터를 배치 단위로 나누기 위해 TensorDataset 모듈로 데이터를 묶어줌\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "batch_size = 32\n",
        "# 예를들어 X_tensor의 개수가 1000개면 1~32,33~64 이런식으로 나누어서 들어갑니다 !\n",
        "# 어떤 모듈을 사용해야 할까요?\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# dataloader = create_dataset()\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = input_features\n",
        "# 좀 위에 정의했던 단일레이어보다 좀 더 복잡한 연산을 수행하기 위한 히든 사이즈(히든 레이어의 노드개수) 설정\n",
        "hidden_size = 64\n",
        "output_size = 1  # Binary classification (1,0 둘 하나가 나오기 때문에 사이즈는 1개)\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss(input)  # Combines a sigmoid layer and the BCELoss\n",
        "# optimizer 를 아래와 같이 정의하면 optimizer 가 model 의 parameters 즉 모든 가중치와 편향을 저장한다.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = model(????)\n",
        "        loss = ????(????, ????)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.????()  # Clear gradients 저장된 모든 파라미터의 기울기 초기화\n",
        "        loss.????()        # Compute gradients 모든 파라미터의 기울기 계산\n",
        "        optimizer.????()       # Update weights 기울기를 바탕으로 가중치 업데이트\n",
        "\n",
        "    # Print loss for every epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# train()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_tensor)\n",
        "    predictions = torch.????(outputs)  # Apply sigmoid to get probabilities\n",
        "    predicted_classes = (predictions >= 0.5).float()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted_classes == y_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2DJDAxMn0Hq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.input_size = input_size\n",
        "        self.fc1 = nn.????(input_size, hidden_size)  # First fully connected layer\n",
        "        self.relu = nn.????()             # ReLU activation\n",
        "        self.fc2 = nn.????(hidden_size, 1) # Second fully connected layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)      # Input to first layer\n",
        "        out = self.relu(out)   # Apply ReLU\n",
        "        out = self.fc2(out)    # Output layer\n",
        "        return out\n",
        "\n",
        "    def create_dataset(self, num_samples = 1000, batch_size = 32):\n",
        "        # Features: random numbers\n",
        "        input_features = self.input_size\n",
        "        X = np.random.randn(num_samples, input_features).astype(np.float32)\n",
        "\n",
        "        # Labels: sum of features > 0 => class 1, else class 0\n",
        "        # Please convince yourself this is the real data, while actuall not the case\n",
        "        y = (X.sum(axis=1) > 0).astype(np.float32)\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        X_tensor = torch.from_numpy(X)\n",
        "        y_tensor = torch.from_numpy(y).unsqueeze(1)  # Add dimension for compatibility\n",
        "\n",
        "        dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def train(self, train_data, num_epochs = 20, learning_rate = 0.001, criterion = nn.BCEWithLogitsLoss):\n",
        "        criterion = criterion()  # Combines a sigmoid layer and the BCELoss\n",
        "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            for batch_X, batch_y in train_data:\n",
        "                # Forward pass\n",
        "                outputs = self(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                optimizer.zero_grad()  # Clear gradients\n",
        "                loss.backward()        # Compute gradients\n",
        "                optimizer.step()       # Update weights\n",
        "\n",
        "            # Print loss for every epoch\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # self.eval()\n",
        "        with torch.no_grad():\n",
        "            accuracy_history = []\n",
        "            for batch_X, batch_y in test_data:\n",
        "                outputs = self(batch_X)\n",
        "                predictions = torch.sigmoid(outputs)\n",
        "                predicted_classes = (predictions >= 0.5).float()\n",
        "                accuracy = (predicted_classes == batch_y).float().mean()\n",
        "                accuracy_history.append(accuracy)\n",
        "            accuracy = sum(accuracy_history) / len(accuracy_history)\n",
        "            print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "            return accuracy\n",
        "\n",
        "\n",
        "model = ????(20, 64)\n",
        "train_dataset = model.????()\n",
        "test_dataset = model.????()\n",
        "print(train_dataset)\n",
        "model.????(train_dataset)\n",
        "model.????(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq5n0jb4vxWg"
      },
      "source": [
        "TODO: input_size, hidden_size, learning_rate 등의 하이퍼파라미터를 바꿔 가며 최적의 모델을 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVlORyey0VDi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIv5SMtX0U_C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Rl4ddJZPpKDo",
        "outputId": "3cf3def6-8567-4207-ea0d-8de5afb8d48c"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SimpleMLP' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7398daeddbfa>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mfind_hyperparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-7398daeddbfa>\u001b[0m in \u001b[0;36mfind_hyperparameter\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mresult_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-7398daeddbfa>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(hidden_size, learning_rate)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleMLP' is not defined"
          ]
        }
      ],
      "source": [
        "def hidden_size_candidates():\n",
        "    return [2**n for n in range(7)]\n",
        "\n",
        "def learning_rate_candidates():\n",
        "    return [0.001]\n",
        "\n",
        "def train_and_eval(hidden_size, learning_rate):\n",
        "    model = ????(20, hidden_size)\n",
        "    train_dataset = model.????(1000)\n",
        "    test_dataset = model.????(100)\n",
        "\n",
        "    model.????(????, learning_rate = learning_rate)\n",
        "    evaluation_result = model.????(test_dataset)\n",
        "\n",
        "    return evaluation_result\n",
        "\n",
        "def find_hyperparameter():\n",
        "    hidden_sizes = ????()\n",
        "    learning_rates = ????()\n",
        "    result_history = []\n",
        "    for hidden_size in hidden_sizes:\n",
        "        for learning_rate in learning_rates:\n",
        "            eval_result = ????(hidden_size, learning_rate)\n",
        "            result_history.append((hidden_size, learning_rate, eval_result))\n",
        "            print(hidden_size, learning_rate, eval_result)\n",
        "    return max(result_history, key = lambda x:x[2])\n",
        "\n",
        "find_hyperparameter()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "6YxkejqAw8--",
        "outputId": "4c083cde-6d59-4240-ffbc-b2e49961d8ef"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# write your code here\n",
        "# 함수 생성\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = [1,2,3,4]\n",
        "y = [2,4,6,8]\n",
        "plt.scatter(X, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAmP1Nzr01JZ"
      },
      "source": [
        "딥러닝으로\n",
        "\n",
        "\n",
        "1.   선형 회귀 y = 4 + 3x를 해볼 것\n",
        "  - 맨 처음에는 torch.tensor이용해서 아예 바닥부터 구현\n",
        "  - 그 이후에 nn.Linear 써서 구현\n",
        "  - 그 이후에 dataloader 써서 구현\n",
        "  - batching 구현해보기\n",
        "  - optimizer 써보기\n",
        "\n",
        "2. 다 하고 나서 $ y = w_1 x_1  + w_2 x_2  + w_3 x_3 + b$  해볼 것\n",
        "\n",
        "$ y = (w_1, w_2, w_3) * (x_1, x_2, x_3)^T +b $\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24rvHPi-0ZuG",
        "outputId": "62bd20f7-acca-400d-c70c-4f79096ad111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 1]) torch.Size([1000, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "0rl_S88Y2DYV",
        "outputId": "12875346-7a44-45d5-8bcf-3e2653385879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 1]) torch.Size([1000, 1])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Module.train() got an unexpected keyword argument 'epochs'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-34c346fc4e45>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtest_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Module.train() got an unexpected keyword argument 'epochs'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        self.b = torch.randn(1, requires_grad = True)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return ????\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = ????(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = self.????(y_pred, y)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.????()\n",
        "\n",
        "            with torch.????():\n",
        "                self.w -= learning_rate * self.w.grad\n",
        "                self.b -= learning_rate * self.b.grad\n",
        "\n",
        "            self.????.grad.zero_()\n",
        "            self.????.grad.zero_()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate_model(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return self.loss(y_pred, y).item()\n",
        "\n",
        "X, y = ????(noise_amplitude = 1)\n",
        "test_X, test_y = ????(noise_amplitude = 1)\n",
        "\n",
        "model = ????()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.w.item(), model.b.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-oOrtaXQE2"
      },
      "source": [
        "Optimizer를 써서 gradient descent를 해보기 (SGD)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dEMMb0CVvro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.????):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.w = torch.randn(????, ????, requires_grad = True)\n",
        "        self.b = torch.randn(????, requires_grad = True)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X @ self.w + self.b\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = ????(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = ????.loss(y_pred, y)\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.????()\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            #     # self.w -= learning_rate * self.w.grad\n",
        "            #     # self.b -= learning_rate * self.b.grad\n",
        "            optimizer.????()\n",
        "\n",
        "            # self.w.grad.zero_()\n",
        "            # self.b.grad.zero_()\n",
        "            optimizer.????()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return self.loss(y_pred, y).item()\n",
        "\n",
        "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz-aUPJ9XUvH"
      },
      "source": [
        "torch.tensor 대신 nn.Linear 써서 해보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gucWqihWXL3y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "    return X, y\n",
        "\n",
        "X, y = generate_synthetic_data()\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        # self.b = torch.randn(1, requires_grad = True)\n",
        "        self.layer = nn.????(1, 1)\n",
        "        self.loss = nn.????()\n",
        "        self.optimizer = optim.????\n",
        "\n",
        "    def forward(self, X):\n",
        "        # return X @ self.w + self.b\n",
        "        return self.layer(X)\n",
        "\n",
        "    def train_model(self, x, y, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        optimizer = self.????(????, lr = learning_rate)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            y_pred = self(x)\n",
        "            # print(y_pred, y)\n",
        "            loss = ????\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "            # compute the gradient of the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            #     # self.w -= learning_rate * self.w.grad\n",
        "            #     # self.b -= learning_rate * self.b.grad\n",
        "            optimizer.step()\n",
        "\n",
        "            # self.w.grad.zero_()\n",
        "            # self.b.grad.zero_()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x)\n",
        "            return ????\n",
        "\n",
        "X, y = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_X, test_y = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(X, y, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_X, test_y))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR3C2dVVYsNL"
      },
      "source": [
        "torch DataLoader 써서 데이터 다루기 (자동으로 알아서 batching 해줌!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7fQvJddYrtg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def plot_fitted_line(x, y, y_pred):\n",
        "    plt.scatter(x, y, alpha = 0.3)\n",
        "    sorted_line = [(x, y_p) for x, y_p in zip(x, y_pred)]\n",
        "    sorted_line = sorted(sorted_line, key = lambda x:x[0])\n",
        "    sorted_x = [e[0] for e in sorted_line]\n",
        "    sorted_y_pred = [e[1] for e in sorted_line]\n",
        "    plt.plot(sorted_x, sorted_y_pred, color = 'red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def generate_synthetic_data(n_samples = 1000, x_amplitude = 10, noise_amplitude = 1, batch_size = 32):\n",
        "    X = torch.randn(n_samples, 1) * x_amplitude\n",
        "    noise = torch.randn(n_samples, 1) * noise_amplitude\n",
        "\n",
        "    y = 4 + 3 * X + noise\n",
        "\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        # self.w = torch.randn(1, 1, requires_grad = True)\n",
        "        # self.b = torch.randn(1, requires_grad = True)\n",
        "        self.layer = ????\n",
        "        self.loss = ????\n",
        "        self.optimizer = ????\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X.shape: (batch_size, n_feature)\n",
        "        # self.layer.shape: (n_feature, hidden_layer)\n",
        "        # X.shape @ self.layer: (batch_size, hidden_layer)\n",
        "        # m, n @ n, p -> m, p\n",
        "        # return X @ self.w + self.b\n",
        "        return self.????\n",
        "\n",
        "    def train_model(self, data, learning_rate = 0.001, epochs = 20, print_log = True, plot = True):\n",
        "        # loss_function = nn.MSELoss()\n",
        "        loss_history = []\n",
        "        optimizer = ????\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in data:\n",
        "                y_pred = self(x)\n",
        "\n",
        "                loss = ????\n",
        "                loss_history.append(loss.item())\n",
        "\n",
        "                # compute the gradient of the loss\n",
        "                loss.????\n",
        "                optimizer.????\n",
        "                optimizer.????\n",
        "\n",
        "            if print_log:\n",
        "                print(f'[Linear Regression with torch.tensor] Epoch {epoch+1}/{epochs} Loss: {loss.item()}')\n",
        "        if plot:\n",
        "            plot_loss_history(loss_history)\n",
        "        return loss_history\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        with torch.no_grad():\n",
        "            loss_list = []\n",
        "\n",
        "            for x, y in data:\n",
        "                y_pred = ????\n",
        "                loss = ????\n",
        "                loss_list.append(loss)\n",
        "\n",
        "            return sum(loss_list) / len(loss_list)\n",
        "\n",
        "train_data = generate_synthetic_data(noise_amplitude = 1)\n",
        "test_data = generate_synthetic_data(noise_amplitude = 1)\n",
        "\n",
        "model = LinearRegressionModel()\n",
        "test_loss_history = []\n",
        "train_loss_history = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = model.train(train_data, epochs = 1, print_log = False, plot = False)\n",
        "    train_loss_history.extend(train_loss)\n",
        "    test_loss_history.append(model.evaluate(test_data))\n",
        "\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(test_loss_history)\n",
        "\n",
        "print(model.layer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBqAOBE5hbG5"
      },
      "source": [
        "1. $x+y<1$인지 판별하는\n",
        "  - 데이터셋을 만들고\n",
        "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
        "  - 학습/평가하고\n",
        "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
        "2. $x^2+y^2<1$인지 판별하는\n",
        "  - 데이터셋을 만들고\n",
        "  - 뉴럴넷을 자유롭게 여러 형태로 만든 후\n",
        "  - 학습/평가하고\n",
        "  - 최종적으로 가장 좋은 뉴럴넷의 성능을 확인해보세요.\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqpXFgRbe8QI",
        "outputId": "9f3f84d7-dd24-4d7a-d700-8e2cfc00042b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(tensor(0.5000), 0.17588959634304047)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_data(x, y):\n",
        "    plt.scatter(x, y, alpha = 0.5)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss_history(loss_history):\n",
        "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
        "    plt.show()\n",
        "\n",
        "def generate_artificial_data(n_samples, batch_size = 32):\n",
        "    x = torch.randn(n_samples, 2)\n",
        "    y = torch.sum(x**2, dim = 1).unsqueeze(dim = 1)\n",
        "\n",
        "    y = (y > 1).float()\n",
        "\n",
        "    dataset = TensorDataset(x, y)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.layer1 = ????\n",
        "        self.activation = ????\n",
        "        self.layer2 = ????\n",
        "        self.sigmoid = ????\n",
        "        self.loss = ????\n",
        "        self.optimizer = ????\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.???? # output.shape: (batch_size, hidden_size)\n",
        "        output = self.????\n",
        "        output = self.???? # output.shape: (batch_size, 1)\n",
        "        output = self.???? # output.shape: (batch_size, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_data, valid_data, epochs = 20, learning_rate = 0.002):\n",
        "        optimizer = ????\n",
        "        train_loss_history = []\n",
        "        valid_loss_history = []\n",
        "        valid_acc_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y in train_data:\n",
        "                y_pred = ????\n",
        "                loss = ????\n",
        "                train_loss_history.append(loss.item())\n",
        "\n",
        "                loss.????\n",
        "                optimizer.????\n",
        "                optimizer.????\n",
        "\n",
        "                acc, valid_loss = self.evaluate_model(valid_data)\n",
        "                self.train()\n",
        "                valid_acc_history.append(acc)\n",
        "                valid_loss_history.append(valid_loss)\n",
        "\n",
        "            # print(f'{epoch+1}/{epochs} Epoch, train loss = {loss}, valid loss = {valid_loss}, acc = {acc}')\n",
        "\n",
        "        return train_loss_history, valid_loss_history, valid_acc_history\n",
        "\n",
        "    def evaluate_model(self, data):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            loss_history = []\n",
        "            for x, y in data:\n",
        "                y_pred = ????\n",
        "                loss_history.append(self.loss(y, y_pred).item())\n",
        "                correct += torch.sum(((y_pred > 0.5).float() == y).float())\n",
        "                total += 32\n",
        "\n",
        "            return correct / total, sum(loss_history) / len(loss_history)\n",
        "\n",
        "model = BinaryClassifier(32) #\n",
        "train_data = generate_artificial_data(1000)\n",
        "valid_data = generate_artificial_data(100)\n",
        "test_data = generate_artificial_data(100)\n",
        "\n",
        "train_loss_history, valid_loss_history, valid_acc_history = model.train_model(train_data, valid_data, epochs = 100)\n",
        "plot_loss_history(train_loss_history)\n",
        "plot_loss_history(valid_loss_history)\n",
        "plot_loss_history(valid_acc_history)\n",
        "\n",
        "model.evaluate_model(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqBOCaUGGKiR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "xN5jjYflr1oE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
