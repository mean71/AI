{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "import sys\n",
    "print(*sys.path,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net CookBook\n",
    "---\n",
    "Seungwoo Schin - DataDiving\n",
    "\n",
    "## Contents\n",
    "- 1 Overview of the Whole Architecture 전체 아키텍쳐 개요 . . . . 1\n",
    "- 2 PyTorch Tensors and Basic Operations . . . . . . . . . . . . . . . . . . . . 5\n",
    "- 3 Building Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
    "    - 3.1 Feedforward Network . . . . . . . . . . . . . . . . . . . . . . . . . . . .14\n",
    "    - 3.2 Recurrent Neural Network . . . . . . . . . . . . . . . . . . . . . . . .15\n",
    "    - 3.3 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
    "    - 3.4 Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . .19\n",
    "    - 3.5 Convolutional Neural Networks (CNNs) . . . . . . . . . . . . 20\n",
    "- 4 Training and Evaluating the Model. . . . . . . . . . . . . . . . . . . . . . . 21\n",
    "    - 4.1 Standard Training Loop. . . . . . . . . . . . . . . . . . . . . . . . . . .21\n",
    "    - 4.2 Model Evaluation and Metrics . . . . . . . . . . . . . . . . . . . . 23\n",
    "    - 4.3 Various Training Techniques . . . . . . . . . . . . . . . . . . . . . . 25\n",
    "- 5 Loss Functions in PyTorch. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n",
    "    - 5.1 Cheat Sheet(code) for each Loss Function . . . . . . . . . .30\n",
    "    - 5.2 Explanation of Key Concepts . . . . . . . . . . . . . . . . . . . . . 34\n",
    "    - 5.3 Detailed Explanation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview of the Whole Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section, we will walk through the typical flow of deep learning code using PyTorch.\n",
    "- The process involves several key steps:\n",
    "    1. Collecting and preparing data.\n",
    "    2. Building the model.\n",
    "    3. Training the model.\n",
    "    4. Evaluating the model.\n",
    "    5. Tuning hyperparameters.\n",
    "- 이 섹션에서는 PyTorch를 사용한 딥러닝 코드의 일반적인 흐름을 살펴보겠습니다.\n",
    "- 프로세스에는 몇 가지 주요 단계가 포함됩니다.\n",
    "    1. 데이터 수집 및 준비.\n",
    "    2. 모델 구축.\n",
    "    3. 모델 훈련.\n",
    "    4. 모델 평가.\n",
    "    5. 하이퍼파라미터 튜닝."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each step is crucial for developing effective neural networks.\n",
    "- Below, we provide detailed explana-tions and carefully crafted code examples for each part\n",
    "- 각 단계는 효과적인 신경망을 개발하는 데 중요합니다.\n",
    "- 아래에서는 각 부분에 대한 자세한 설명과 신중하게 작성된 코드 예제를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collecting and Preparing Data\n",
    "    - The first step is to collect or generate data and prepare it for training.\n",
    "    - This involves converting data into tensors and creating datasets and data loaders.\n",
    "- Converting Data into Tensors\n",
    "    - Assuming you have data in the form of NumPy arrays or lists, you can convert them into PyTorch tensors using torch.tensor.\n",
    "- 데이터 수집 및 준비\n",
    "    - 첫 번째 단계는 데이터를 수집하거나 생성하고 훈련을 위해 준비하는 것입니다.\n",
    "    - 여기에는 데이터를 텐서로 변환하고 데이터 세트 및 데이터 로더를 생성하는 작업이 포함됩니다.\n",
    "- 데이터를 텐서로 변환\n",
    "    - NumPy 배열 또는 목록 형태의 데이터가 있다고 가정하면 torch.tensor를 사용하여 이를 PyTorch 텐서로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Example data\n",
    "features_np = np.random.rand(1000, 20) # 1000 samples, 20 features # 0~1사이 랜덤한 실수데이터 1000행20열 생성\n",
    "labels_np = np.random.randint(0, 2, size=(1000,)) # Binary lables # numpy.random.randint(low=0, high=None, size=None, dtype=int) row ~ high 사이의 정수를 size=개수 OR size=(행,열)로 생성\n",
    "# Convert to tensors\n",
    "features = torch.tensor(features_np, dtype=torch.float32)\n",
    "labels = torch.tensor(labels_np, dtype=torch.long)\n",
    "'''\n",
    "# Creating a TensorDataset and DataLoader\n",
    "# Once the data is in tensor form, we can create a TensorDataset and wrap it with a DataLoader for easy batching and shuffling.\n",
    "# TensorDataset 및 DataLoader 만들기\n",
    "# 데이터가 텐서 형식이면 TensorDataset를 생성하고 이를 DataLoader로 래핑하여 쉽게 일괄 처리하고 섞을 수 있습니다.\n",
    "'''\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(features, labels) # torch.utils.data.TensorDataset(*tensors)\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "'''\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1번 인수: dataset\n",
    "# - PyTorch에서 데이터를 관리하는 핵심 클래스인 Dataset의 인스턴스를 전달해야 함.\n",
    "# - Dataset 클래스는 데이터를 어떻게 가져올지, 접근할지 정의함.\n",
    "# - 예시: torchvision.datasets.MNIST 또는 커스텀 Dataset 클래스.\n",
    "\n",
    "# 2번 인수: batch_size\n",
    "# - 데이터를 몇 개씩 한 번에 처리할지 결정함.\n",
    "# - 예를 들어, batch_size=32이면, 32개의 데이터를 한 묶음으로 처리함.\n",
    "# - 기본값은 1\n",
    "\n",
    "# 3번 인수: shuffle\n",
    "# - 매 epoch마다 데이터를 랜덤으로 섞을지 설정하는 옵션.\n",
    "# - True로 설정하면 매번 데이터를 섞어서 학습 시 데이터 순서에 대한 의존을 줄임.\n",
    "# - False로 설정하면 원래 순서대로 데이터를 처리함.\n",
    "# - 기본값은 False.\n",
    "- 활성화 하는경우\n",
    "- 시계열 데이터 / 순서가 중요한 데이터 / 디버깅 / 재현성확보 / 분포가 균일하지 않거나 / 비율이 특이한 data를 처리가 목적일때는 False로 설정해야 한다.\n",
    "\n",
    "# 4번 인수: sampler\n",
    "# - 데이터를 어떻게 샘플링할지를 정의하는 객체.\n",
    "# - 기본적으로 shuffle=True를 설정하면 내부적으로 RandomSampler가 사용됨.\n",
    "# - 커스텀 샘플링 전략이 필요할 경우 사용할 수 있음.\n",
    "\n",
    "# 5번 인수: batch_sampler\n",
    "# - batch_size와 sampler를 결합하여 배치 단위로 샘플링하는 방법을 정의하는 옵션.\n",
    "# - batch_sampler는 배치를 어떻게 구성할지 직접 정의할 수 있음.\n",
    "# - 기본적으로는 사용하지 않음. batch_size와 sampler로 충분히 설정 가능.\n",
    "\n",
    "# 6번 인수: num_workers\n",
    "# - 데이터를 로드할 때 사용할 worker 프로세스의 수를 지정함.\n",
    "# - 기본적으로 num_workers=0이면 데이터 로드는 메인 프로세스에서 이루어짐.\n",
    "# - num_workers를 늘리면 데이터를 병렬로 로드할 수 있어 속도 향상 가능.\n",
    "\n",
    "# 7번 인수: collate_fn\n",
    "# - 각 batch에서 데이터를 어떻게 결합할지 정의하는 함수.\n",
    "# - 기본적으로는 전달받은 데이터를 텐서로 묶어서 반환하지만, 커스텀 방식으로 결합할 때 사용할 수 있음.\n",
    "# - 예를 들어, NLP에서 서로 길이가 다른 문장들을 패딩 처리할 때 사용 가능.\n",
    "\n",
    "# 8번 인수: pin_memory\n",
    "# - True로 설정하면, GPU 사용 시 데이터를 더 빠르게 전송할 수 있음.\n",
    "# - CPU에서 GPU로 데이터를 복사할 때 성능을 향상시킬 수 있음.\n",
    "# - GPU 사용 시 성능 최적화를 위해 True로 설정할 수 있음.\n",
    "# - 기본값은 False.\n",
    "\n",
    "# 9번 인수: drop_last\n",
    "# - 데이터셋 크기가 배치 크기로 나누어 떨어지지 않을 때, 마지막 남은 배치를 버릴지 여부.\n",
    "# - True로 설정하면 마지막 배치가 남은 경우 이를 버리고, False면 그대로 사용함.\n",
    "# - 예를 들어, batch_size가 32이고, 데이터가 100개인 경우 마지막 4개 데이터를 버릴지 결정함.\n",
    "# - 기본값은 False.\n",
    "- 활성화 하는 경우\n",
    "- 배치 정규화(Batch Normalization)를 사용하는 경우: 배치내에서 계산된 평균과 분산을 이용해 data분포를 정규화하는 방법을 사용중일때\n",
    "- GPU메모리 최적화 및 병렬처리를 위한경우/ 모델이 배치 크기에 민감한 경우 모델이 [다른크기의 배치]에 대해 학습하지 않고 일관된 학습패턴을 원할때\n",
    "\n",
    "# - DataLoader 예시\n",
    "# dataset: PyTorch Dataset 객체\n",
    "# batch_size: 한 번에 처리할 데이터의 크기\n",
    "# shuffle: 데이터를 랜덤으로 섞을지 여부\n",
    "# drop_last: 나머지 데이터가 남을 때 마지막 배치를 버릴지 여부\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True, num_workers=4)\n",
    "\n",
    "# DataLoader 사용 예시\n",
    "for data, labels in dataloader:\n",
    "    # data: 배치로 묶인 입력 데이터\n",
    "    # labels: 해당 데이터에 맞는 레이블\n",
    "    # 여기서 모델 학습 또는 예측 진행 가능\n",
    "    pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - TensorDataset takes tensors as input and creates a dataset where each sample is a tuple of the tensors at a given index.\n",
    "  - • DataLoader wraps the dataset and provides an iterable over the dataset with support for automatic batching, sampling, shuffling, and multiprocess data loading.\n",
    "- Building the Model\n",
    "  - Next, we define the neural network architecture by creating a subclass of torch.nn.Module.\n",
    "- 설명:\n",
    "  - TensorDataset는 텐서를 입력으로 사용하고 각 샘플이 주어진 인덱스에 있는 텐서의 튜플인 데이터세트를 생성합니다.\n",
    "  - • DataLoader는 데이터 세트를 래핑하고 자동 일괄 처리, 샘플링, 셔플링 및 다중 프로세스 데이터 로드를 지원하여 데이터 세트에 대한 반복 가능 항목을 제공합니다.\n",
    "- 모델 구축\n",
    "  - 다음으로, torch.nn.Module의 하위 클래스를 생성하여 신경망 아키텍처를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)   # First fully connected layer (입력 크기 -> 은닉층 크기)\n",
    "        self.relu = nn.ReLU()                           # Activation function # 활성함수\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Second fully connected layer (은닉층 크기 -> 출력 클래스 크기)\n",
    "    \n",
    "    def forward(self, x): # 순전파 함수 정의\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out) # 1번째 완전 연결 레이어 통과후 활성함수 ReLu적용\n",
    "        out = self.fc2(out)\n",
    "        return out # 2번째 레이어 통과후 결과 반환\n",
    "# 하이퍼파라미터 은닉층크기가 클수록 과적합 위험/ 모델의 복잡도 증가 -> 학습시간과 자원소모량 증가 \n",
    "# 반대로 작을수록 복잡한 패턴을 충분히 학습하지 못해 과소적합발생 위험/ 등의 위험이 증가한다 # 데이터의 양/목적을 고려해 적정값이 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - The __init__ method defines the layers and activation functions.\n",
    "  - The forward method defines the forward pass computation\n",
    "- Training the Model\n",
    "    - To train the model, we need to define a loss function and an optimizer, and\\\n",
    "    then iterate over the data in epochs.\\\n",
    "- 설명:\n",
    "  - __init__ 메소드는 레이어와 활성화 함수를 정의합니다.\n",
    "  - 순방향 방법은 순방향 전달 계산을 정의합니다.\n",
    "- 모델 훈련\n",
    "    - 모델을 훈련하려면 손실 함수와 최적화 프로그램을 정의해야 하며,\\\n",
    "    그런 다음 에포크 단위로 데이터를 반복합니다.\n",
    "\n",
    "- Defining Loss Function and Optimizer\n",
    "- 손실 함수 및 최적화 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 20 # Number of input features\n",
    "hidden_size = 64 # Number of neurons in the hidden layer\n",
    "num_classes = 2 # Number of output classes\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleNeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Loss function for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss in one single class.\n",
    "  - torch.optim.Adam is an optimization algorithm that can be used instead of the classical\n",
    "    stochastic gradient descent procedure.\n",
    "- Training Loop\n",
    "- 설명:\n",
    "  - nn.CrossEntropyLoss는 nn.LogSoftmax와 nn.NLLLoss를 하나의 단일 클래스로 결합합니다.\n",
    "  - torch.optim.Adam은 기존 알고리즘 대신 사용할 수 있는 최적화 알고리즘입니다.\n",
    "    확률적 경사하강법 절차.\n",
    "- 훈련 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_features, batch_labels in data_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - The training loop iterates over epochs and batches.\n",
    "  - For each batch, we perform a forward pass, compute the loss, perform a backward pass, and\n",
    "    update the model parameters.\n",
    "- 설명:\n",
    "  - 훈련 루프는 에포크(epoch)와 배치(batch)에 걸쳐 반복됩니다.\n",
    "  - 각 배치에 대해 정방향 패스를 수행하고, 손실을 계산하고, 역방향 패스를 수행하고,\n",
    "    모델 매개변수를 업데이트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluating the Model\n",
    "  - After training, we evaluate the model’s performance on a validation or test set.\n",
    "- Evaluation Function\n",
    "- 모델 평가\n",
    "  - 훈련 후에는 검증 또는 테스트 세트에서 모델의 성능을 평가합니다.\n",
    "- 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_features, batch_labels in data_loader:\n",
    "            outputs = model(batch_features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - model.eval() sets the model to evaluation mode, affecting layers like dropout and batch normalization.\n",
    "  - torch.no_grad() disables gradient calculation, reducing memory consumption and speed-ing up computations.\n",
    "  - torch.max(outputs.data, 1) returns the indices of the maximum value in each row, corre-sponding to the predicted class.\n",
    "- Running Evaluation\n",
    "  - Assuming we have a test_loader similar to data_loader:\n",
    "- 설명:\n",
    "  - model.eval()은 모델을 평가 모드로 설정하여 드롭아웃 및 배치 정규화와 같은 레이어에 영향을 줍니다.\n",
    "  - torch.no_grad()는 기울기 계산을 비활성화하여 메모리 소비를 줄이고 계산 속도를 높입니다.\n",
    "  - torch.max(outputs.data, 1)는 예측된 클래스에 해당하는 각 행의 최대값 인덱스를 반환합니다.\n",
    "- 평가 진행\n",
    "  - data_loader와 유사한 test_loader가 있다고 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, data_loader) # Here, using the same data_loader for simplicity\n",
    "# 단순화를 위해 동일data_loader사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tuning Hyperparameters\n",
    "  - Hyperparameter tuning involves experimenting with different settings to improve model performance.\n",
    "- Example: Manual Hyperparameter Tuning\n",
    "- 하이퍼파라미터 튜닝\n",
    "  - 초매개변수 조정에는 모델 성능을 개선하기 위해 다양한 설정을 실험하는 작업이 포함됩니다.\n",
    "- 예: 수동 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of hyperparameters to try\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "hidden_sizes = [32, 64, 128]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for hs in hidden_sizes:\n",
    "        # Initialize the model with current hyperparameters\n",
    "        model = SimpleNeuralNet(input_size, hs, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_features, batch_labels in data_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # Evaluate the model\n",
    "        accuracy = evaluate_model(model, data_loader)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = {'learning_rate': lr, 'hidden_size': hs}\n",
    "\n",
    "print(f'Best Hyperparameters: {best_params}, Accuracy: {best_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation:\n",
    "  - This is a simple grid search over specified hyperparameters.\n",
    "  - For each combination, the model is trained and evaluated.\n",
    "  - The best hyperparameters are selected based on the highest accuracy.\n",
    "- Summary\n",
    "  - This workflow provides a foundational template for developing deep learning models:\n",
    "  1. Data is prepared and loaded efficiently.\n",
    "  2. Models are built with clear architectures.\n",
    "  3. Training loops are structured for clarity and extensibility.\n",
    "  4. Evaluation functions provide insight into model performance.\n",
    "  5. Hyperparameter tuning allows for systematic improvement.\n",
    "- 설명:\n",
    "  - 지정된 하이퍼파라미터에 대한 간단한 그리드 검색입니다.\n",
    "  - 각 조합에 대해 모델을 훈련하고 평가합니다.\n",
    "  - 가장 높은 정확도를 기준으로 최고의 하이퍼파라미터가 선택됩니다.\n",
    "- 요약\n",
    "  - 이 워크플로는 딥 러닝 모델 개발을 위한 기본 템플릿을 제공합니다.\n",
    "  1. 데이터가 효율적으로 준비되고 로드됩니다.\n",
    "  2. 모델은 명확한 아키텍처로 구축됩니다.\n",
    "  3. 훈련 루프는 명확성과 확장성을 위해 구성되었습니다.\n",
    "  4. 평가 기능은 모델 성능에 대한 통찰력을 제공합니다.\n",
    "  5. 하이퍼파라미터 튜닝을 통해 체계적인 개선이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch Tensors and Basic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, the fundamental data structure is the torch.Tensor.\\\n",
    "  Tensors are multi-dimensional arrays similar to NumPy’s ndarray, but they can be operated on a GPU to accelerate computing.\n",
    "- PyTorch에서 기본적인 데이터 구조는 torch.Tensor입니다.\\\n",
    "  Tensor는 NumPy의 ndarray와 유사한 다차원 배열이지만 컴퓨팅 속도를 높이기 위해 GPU에서 작동할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating Tensors\n",
    "  - PyTorch provides several functions to create tensors in different ways:\n",
    "    - torch.tensor(data): Creates a tensor from data.\n",
    "    - torch.zeros(size): Creates a tensor filled with zeros.\n",
    "    - torch.ones(size): Creates a tensor filled with ones.\n",
    "    - torch.arange(start, end, step): Creates a tensor with values from start to end with a step size.\n",
    "    - torch.linspace(start, end, steps): Creates a tensor with steps equally spaced points between start and end.\n",
    "    - torch.rand(size): Creates a tensor with random values uniformly distributed between 0 and 1.\n",
    "    - torch.randn(size): Creates a tensor with random values from a standard normal distribu-tion\n",
    "- 텐서 생성\n",
    "  - PyTorch는 다양한 방식으로 텐서를 생성할 수 있는 여러 기능을 제공합니다.\n",
    "    - torch.tensor(data): 데이터로부터 텐서를 생성합니다.\n",
    "    - torch.zeros(size): 0으로 채워진 텐서를 생성합니다.\n",
    "    - torch.ones(size): 1로 채워진 텐서를 생성합니다.\n",
    "    - torch.arange(start, end, step): 시작부터 끝까지 단계 크기의 값을 가진 텐서를 생성합니다.\n",
    "    - torch.linspace(start, end, steps): 시작과 끝 사이에 동일한 간격의 지점이 있는 텐서를 생성합니다.\n",
    "    - torch.rand(size): 0과 1 사이에 균일하게 분포된 임의의 값을 갖는 텐서를 생성합니다.\n",
    "    - torch.randn(size): 표준 정규 분포에서 임의의 값을 가진 텐서를 생성합니다.\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "tensor_from_data = torch.tensor(data)\n",
    "\n",
    "# Create tensors with specific functions\n",
    "zeros_tensor = torch.zeros((2, 2))\n",
    "ones_tensor = torch.ones((2, 2))\n",
    "arange_tensor = torch.arange(0, 10, 2)\n",
    "linspace_tensor = torch.linspace(0, 1, steps=5)\n",
    "rand_tensor = torch.rand((2, 2))\n",
    "randn_tensor = torch.randn((2, 2))\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "tensor_from_data:\n",
    "tensor([[1, 2],\n",
    "        [3, 4]])\n",
    "\n",
    "zeros_tensor:\n",
    "tensor([[0., 0.],\n",
    "        [0., 0.]])\n",
    "\n",
    "ones_tensor:\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "arange_tensor:\n",
    "tensor([0, 2, 4, 6, 8])\n",
    "\n",
    "linspace_tensor:\n",
    "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
    "\n",
    "rand_tensor:\n",
    "tensor([[0.1234, 0.5678],\n",
    "        [0.9012, 0.3456]])\n",
    "\n",
    "randn_tensor:\n",
    "tensor([[ 0.1234, -1.2345],\n",
    "        [ 0.5678, 0.9012]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indexing and Slicing\n",
    "  - Similar to NumPy arrays, tensors can be indexed and sliced.\n",
    "- Example Code:\n",
    "- 인덱싱 및 슬라이싱\n",
    "  - NumPy 배열과 유사하게 텐서는 인덱싱 및 슬라이스가 가능합니다.\n",
    "- 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Indexing\n",
    "element = tensor[0, 1] # Access element at first row, second column\n",
    "\n",
    "# Slicing\n",
    "row = tensor[1, :] # Access second row\n",
    "column = tensor[:, 2] # Access third column\n",
    "sub_tensor = tensor[0:2, 1:3] # Access a sub-tensor\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "element:\n",
    "tensor(2)\n",
    "\n",
    "row:\n",
    "tensor([4, 5, 6])\n",
    "\n",
    "column:\n",
    "tensor([3, 6])\n",
    "\n",
    "sub_tensor:\n",
    "tensor([[2, 3],\n",
    "        [5, 6]])\n",
    "\n",
    "- Input Shape: (2, 3)\n",
    "- Output Shapes:\n",
    "    - element: ()\n",
    "    - row: (3,)\n",
    "    - column: (2,)\n",
    "    - sub_tensor: (2, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reshaping Operations\n",
    "  - These operations change the shape of the tensor without changing its data.\n",
    "    - tensor.view(shape): Returns a new tensor with the same data but different shape.\n",
    "    - tensor.reshape(shape): Similar to view, but can handle non-contiguous tensors.\n",
    "    - tensor.unsqueeze(dim): Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "    - tensor.squeeze(dim): Returns a tensor with all the dimensions of size 1 removed.\n",
    "    - tensor.permute(dims): Returns a view of the original tensor with its dimensions permuted.\n",
    "    - tensor.transpose(dim0, dim1): Swaps two dimensions of the tensor.\n",
    "- 작업 재구성\n",
    "  - 이러한 작업은 데이터를 변경하지 않고 텐서의 모양을 변경합니다.\n",
    "    - tensor.view(shape): 데이터는 동일하지만 모양이 다른 새 텐서를 반환합니다.\n",
    "    - tensor.reshape(shape): 뷰와 유사하지만 연속되지 않은 텐서를 처리할 수 있습니다.\n",
    "    - tensor.unsqueeze(dim): 지정된 위치에 삽입된 크기 1의 새 텐서를 반환합니다.\n",
    "    - tensor.squeeze(dim): 크기 1의 모든 차원이 제거된 텐서를 반환합니다.\n",
    "    - tensor.permute(dims): 차원이 순열된 원본 텐서의 뷰를 반환합니다.\n",
    "    - tensor.transpose(dim0,dim1): 텐서의 두 차원을 교환합니다.\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.arange(0, 8)\n",
    "reshaped = tensor.view(2, 4)\n",
    "unsqueezed = tensor.unsqueeze(0)\n",
    "squeezed = unsqueezed.squeeze()\n",
    "permuted = reshaped.permute(1, 0)\n",
    "transposed = reshaped.transpose(0, 1)\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "tensor:\n",
    "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "reshaped:\n",
    "tensor([[0, 1, 2, 3],\n",
    "        [4, 5, 6, 7]])\n",
    "\n",
    "unsqueezed shape:\n",
    "torch.Size([1, 8])\n",
    "\n",
    "squeezed shape:\n",
    "torch.Size([8])\n",
    "\n",
    "permuted:\n",
    "tensor([[0, 4],\n",
    "        [1, 5],\n",
    "        [2, 6],\n",
    "        [3, 7]])\n",
    "\n",
    "transposed:\n",
    "tensor([[0, 4],\n",
    "        [1, 5],\n",
    "        [2, 6],\n",
    "        [3, 7]])\n",
    "\n",
    "- Input Shape: (8,)\n",
    "- Output Shapes:\n",
    "    - reshaped: (2, 4)\n",
    "    - unsqueezed: (1, 8)\n",
    "    - squeezed: (8,)\n",
    "    - permuted: (4, 2)\n",
    "    - transposed: (4, 2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arithmetic Operations\n",
    "  - Element-wise arithmetic operations between tensors or between a tensor and a scalar.\n",
    "    - Addition: torch.add(a, b) or a + b\n",
    "    - Subtraction: torch.sub(a, b) or a - b\n",
    "    - Multiplication: torch.mul(a, b) or a * b\n",
    "    - Division: torch.div(a, b) or a / b\n",
    "    - Exponentiation: torch.pow(a, b) or a b\n",
    "- 산술 연산\n",
    "  - 텐서 간 또는 텐서와 스칼라 간의 요소별 산술 연산.\n",
    "    - 추가: torch.add(a, b) 또는 a b\n",
    "    - 빼기: torch.sub(a, b) 또는 a - b\n",
    "    - 곱셈: torch.mul(a, b) 또는 a * b\n",
    "    - 구분: torch.div(a, b) 또는 a / b\n",
    "    - 지수화: torch.pow(a, b) 또는 a b\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Addition\n",
    "add_result = a + b\n",
    "\n",
    "# Multiplication\n",
    "mul_result = a * b\n",
    "\n",
    "# Scalar multiplication\n",
    "scalar_mul = a * 2\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "add_result:\n",
    "tensor([5, 7, 9])\n",
    "\n",
    "mul_result:\n",
    "tensor([ 4, 10, 18])\n",
    "\n",
    "scalar_mul:\n",
    "tensor([2, 4, 6])\n",
    "\n",
    "- Input Shapes: (3,), (3,)\n",
    "- Output Shape: (3,)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduction Operations\n",
    "  - Operations that reduce the dimensions of tensors by applying an opera-tion along a specified axis.\n",
    "    - tensor.sum(dim): Sum of elements along dimension dim.\n",
    "    - tensor.mean(dim): Mean of elements along dimension dim.\n",
    "    - tensor.max(dim): Maximum value along dimension dim.\n",
    "    - tensor.min(dim): Minimum value along dimension dim.\n",
    "    - tensor.prod(dim): Product of elements along dimension dim.\n",
    "- 환원작업\n",
    "  - 지정된 축을 따라 연산을 적용하여 텐서의 크기를 줄이는 연산입니다.\n",
    "    - tensor.sum(dim): 차원 치수를 따른 요소의 합계입니다.\n",
    "    - tensor.mean(dim): 차원 희미한 요소의 평균입니다.\n",
    "    - tensor.max(dim): 치수 치수에 따른 최대값입니다.\n",
    "    - tensor.min(dim): 치수 치수에 따른 최소값입니다.\n",
    "    - tensor.prod(dim): 차원 치수에 따른 요소의 곱입니다.\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "# Sum over all elements\n",
    "total_sum = tensor.sum()\n",
    "\n",
    "# Sum over columns (dim=0)\n",
    "sum_dim0 = tensor.sum(dim=0)\n",
    "\n",
    "# Mean over rows (dim=1)\n",
    "mean_dim1 = tensor.mean(dim=1)\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "total_sum:\n",
    "tensor(10)\n",
    "\n",
    "sum_dim0:\n",
    "tensor([4, 6])\n",
    "\n",
    "mean_dim1:\n",
    "tensor([1.5, 3.5])\n",
    "\n",
    "- Input Shape: (2, 2)\n",
    "- Output Shapes:\n",
    "    - total_sum: ()\n",
    "    - sum_dim0: (2,)\n",
    "    - mean_dim1: (2,)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Element-wise Operations\n",
    "  - Operations applied individually to each element.\n",
    "    - torch.exp(tensor): Exponential of each element.\n",
    "    - torch.log(tensor): Natural logarithm of each element.\n",
    "    - torch.sqrt(tensor): Square root of each element.\n",
    "    - torch.sin(tensor): Sine of each element.\n",
    "    - torch.cos(tensor): Cosine of each element.\n",
    "    - torch.abs(tensor): Absolute value of each element.\n",
    "\n",
    "- 요소별 작업\n",
    "  - 각 요소에 개별적으로 적용되는 작업입니다.\n",
    "    - torch.exp(tensor): 각 요소의 지수입니다.\n",
    "    - torch.log(tensor): 각 요소의 자연 로그입니다.\n",
    "    - torch.sqrt(tensor): 각 요소의 제곱근.\n",
    "    - torch.sin(tensor): 각 요소의 사인.\n",
    "    - torch.cos(tensor): 각 요소의 코사인.\n",
    "    - torch.abs(tensor) : 각 요소의 절대값입니다.\n",
    "\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1.0, 4.0, 9.0])\n",
    "\n",
    "# Square root\n",
    "sqrt_result = torch.sqrt(tensor)\n",
    "\n",
    "# Natural logarithm\n",
    "log_result = torch.log(tensor)\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "sqrt_result:\n",
    "tensor([1.0000, 2.0000, 3.0000])\n",
    "\n",
    "log_result:\n",
    "tensor([0.0000, 1.3863, 2.1972])\n",
    "\n",
    "- Input Shape: (3,)\n",
    "- Output Shape: (3,)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Broadcasting\n",
    "  - Operations between tensors of different shapes, where PyTorch automatically expands the smaller tensor to match the shape of the larger tensor.\n",
    "- Broadcasting\n",
    "  - PyTorch가 더 큰 텐서의 모양과 일치하도록 작은 텐서를 자동으로 확장하는 다양한 모양의 텐서 간의 작업입니다.\n",
    "\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1], [2], [3]]) # Shape: (3, 1)\n",
    "b = torch.tensor([10, 20, 30]) # Shape: (3,)\n",
    "\n",
    "# Broadcasting addition\n",
    "result = a + b # Shape: (3, 3)\n",
    "\n",
    "# Example Execution and Results\n",
    "'''\n",
    "result:\n",
    "tensor([[11, 21, 31],\n",
    "        [12, 22, 32],\n",
    "        [13, 23, 33]])\n",
    "\n",
    "- Input Shapes: (3, 1), (3,)\n",
    "- Output Shape: (3, 3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Type Casting\n",
    "  - Changing the data type of a tensor.\n",
    "    - tensor.type(dtype): Returns a new tensor with the specified data type.\n",
    "    - tensor.float(): Converts tensor to torch.float32.\n",
    "    - tensor.int(): Converts tensor to torch.int32.\n",
    "    - tensor.long(): Converts tensor to torch.int64.\n",
    "- Example Code:\n",
    "- 타입 캐스팅\n",
    "  - 텐서의 데이터 유형을 변경합니다.\n",
    "    - tensor.type(dtype): 지정된 데이터 유형을 가진 새로운 텐서를 반환합니다.\n",
    "    - tensor.float(): 텐서를 torch.float32로 변환합니다.\n",
    "    - tensor.int(): 텐서를 torch.int32로 변환합니다.\n",
    "    - tensor.long(): 텐서를 torch.int64로 변환합니다.\n",
    "- 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1, 2, 3])\n",
    "float_tensor = tensor.float()\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "tensor dtype:\n",
    "torch.int64\n",
    "\n",
    "float_tensor dtype:\n",
    "torch.float32\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Device Management\n",
    "  - Moving tensors between CPU and GPU devices.\n",
    "- 장치 관리\n",
    "  - CPU와 GPU 장치 간에 텐서를 이동합니다.\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Move tensor to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    \n",
    "# Check device\n",
    "print(tensor.device)\n",
    "\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "cuda:0  # If CUDA is available\n",
    "cpu     # If CUDA is not available\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cloning and Detaching Tensors\n",
    "    - tensor.clone(): Creates a copy of the tensor.\n",
    "    - tensor.detach(): Returns a tensor that shares storage with tensor but without requiring gradients.\n",
    "- 텐서 복제 및 분리\n",
    "    - tensor.clone(): 텐서의 복사본을 생성합니다.\n",
    "    - tensor.detach(): 텐서와 저장 공간을 공유하지만 그라디언트를 요구하지 않는 텐서를 반환합니다.\n",
    "- Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Clone the tensor\n",
    "cloned_tensor = tensor.clone()\n",
    "\n",
    "# Detach the tensor\n",
    "detached_tensor = tensor.detach()\n",
    "# Example Execution and Results:\n",
    "'''\n",
    "cloned_tensor.requires_grad:\n",
    "True\n",
    "detached_tensor.requires_grad:\n",
    "False\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Summary of Operations by Category\n",
    "    - Creation: torch.tensor, torch.zeros, torch.ones, torch.rand, etc.\n",
    "    - Indexing/Slicing: Access elements or sub-tensors using indices and slices.\n",
    "    - Reshaping: view, reshape, unsqueeze, squeeze, permute, transpose.\n",
    "    - Arithmetic: Element-wise addition, subtraction, multiplication, division.\n",
    "    - Reduction: sum, mean, max, min, prod.\n",
    "    - Element-wise Functions: exp, log, sqrt, sin, cos, abs.\n",
    "    - Broadcasting: Automatic expansion of dimensions for arithmetic operations.\n",
    "    - Type Casting: Changing data types with type, float, int, long.\n",
    "    - Device Management: Moving tensors to CPU or GPU.\n",
    "    - Cloning/Detaching: Creating copies and detaching from computation graphs.\n",
    "- By understanding and utilizing these tensor operations, you can effectively manipulate data and build complex neural network models in PyTorch.\n",
    "- 부문별 사업개요\n",
    "    - 생성: torch.tensor, torch.zeros, torch.ones, torch.rand 등\n",
    "    - 인덱싱/슬라이싱: 인덱스와 슬라이스를 사용하여 요소 또는 하위 텐서에 액세스합니다.\n",
    "    - 모양 바꾸기: 보기, 모양 바꾸기, 압축 해제, 짜기, 순열, 전치.\n",
    "    - 산술: 요소별 덧셈, 뺄셈, 곱셈, 나눗셈.\n",
    "    - 감소: 합계, 평균, 최대, 최소, 생산.\n",
    "    - 요소별 함수: exp, log, sqrt, sin, cos, abs.\n",
    "    - 브로드캐스팅: 산술 연산을 위한 차원의 자동 확장.\n",
    "    - 유형 캐스팅: 유형, 부동 소수점, int, long으로 데이터 유형을 변경합니다.\n",
    "    - 장치 관리: 텐서를 CPU 또는 GPU로 이동합니다.\n",
    "    - 복제/분리: 복사본을 생성하고 계산 그래프에서 분리합니다.\n",
    "- 이러한 텐서 연산을 이해하고 활용함으로써 PyTorch에서 데이터를 효과적으로 조작하고 복잡한 신경망 모델을 구축할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deep learning models are built by stacking layers of computational units, each transforming input data into more abstract representations. Typical models in deep learning include:\n",
    "    - Feedforward Neural Networks (FNN)\n",
    "    - Convolutional Neural Networks (CNN)\n",
    "    - Recurrent Neural Networks (RNN)\n",
    "    - Long Short-Term Memory Networks (LSTM)\n",
    "    - Gated Recurrent Units (GRU)\n",
    "    - Transformer Models\n",
    "    - Autoencoders\n",
    "    - Generative Adversarial Networks (GANs)\n",
    "\n",
    "- These models utilize various techniques such as activation functions, pooling layers, normalization\n",
    "layers, and more to improve learning and performance.\n",
    "- In this section, we will implement some of these models step by step, starting from basic op-erations using torch.\n",
    "- tensor, and gradually incorporating higher-level PyTorch features. Each implementation builds upon the previous one, allowing for modular and reusable code.\n",
    "\n",
    "- 딥 러닝 모델은 입력 데이터를 보다 추상적인 표현으로 변환하는 계산 단위의 레이어를 쌓아 구축됩니다.\n",
    "  딥러닝의 일반적인 모델은 다음과 같습니다.\n",
    "    - 피드포워드 신경망(FNN)\n",
    "    - 컨볼루셔널 신경망(CNN)\n",
    "    - 순환 신경망(RNN)\n",
    "    - 장단기 기억 네트워크(LSTM)\n",
    "    - GRU(Gated Recurrent Unit)\n",
    "    - 변압기 모델\n",
    "    - 오토인코더\n",
    "    - 생성적 적대 신경망(GAN)\n",
    "\n",
    "- 이 모델은 활성화 함수, 풀링 레이어, 정규화 등 다양한 기술을 활용합니다.\n",
    "- 레이어 등을 통해 학습과 성과를 향상할 수 있습니다.\n",
    "- 이 섹션에서는 토치를 사용한 기본 작업부터 시작하여 이러한 모델 중 일부를 단계별로 구현해 보겠습니다.\n",
    "- 텐서, 그리고 점차적으로 더 높은 수준의 PyTorch 기능을 통합합니다.\n",
    "- 각 구현은 이전 구현을 기반으로 구축되므로 모듈식 및 재사용 가능한 코드가 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feedforward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing a Linear Layer with torch.tensor\n",
    "  - A linear layer performs a linear transformation on the input data:\n",
    "- y = xW⊤ + b\n",
    "- where x is the input tensor, W is the weight matrix, and b is the bias vector.\n",
    "- We can implement this using only torch.tensor operations:\n",
    "\n",
    "- torch.tensor를 사용하여 선형 레이어 구현\n",
    "  - 선형 레이어는 입력 데이터에 대해 선형 변환을 수행합니다.\n",
    "- y = x*W**⊤ b\n",
    "- 여기서 x는 입력 텐서, W는 가중치 행렬, ​​b는 편향 벡터입니다.\n",
    "- 오직 torch.tensor 연산만을 사용하여 이를 구현할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyLinearLayer: # A simple implementation of a linear layer using torch.tensor operations.\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = torch.randn(output_size, input_size, requires_grad=True)\n",
    "        self.b = torch.randn(output_size, requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_size)\n",
    "        y = x @ self.W.t() + self.b # Linear transformation\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, the weight matrix W and bias vector b are initialized as tensors with gradients enabled. The forward method computes the linear transformation.\n",
    "- 여기서 가중치 행렬 W와 바이어스 벡터 b는 기울기가 활성화된 텐서로 초기화됩니다. 순방향 방법은 선형 변환을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Replacing with nn.Linear\n",
    "  - PyTorch provides the nn.Linear module, which encapsulates the linear transformation and parameter management.\n",
    "  - The previous implementation can be simplified by replacing the manual tensor operations with nn.Linear:\n",
    "- nn.Linear로 대체\n",
    "  - PyTorch는 선형 변환 및 매개변수 관리를 캡슐화하는 nn.Linear 모듈을 제공합니다.\n",
    "  - 수동 텐서 연산을 nn.Linear로 대체하여 이전 구현을 단순화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MyLinearLayer(nn.Module): # A linear layer implemented using nn.Linear.\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyLinearLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "# The lines:\n",
    "'''\n",
    "self.W = torch.randn(output_size, input_size, requires_grad=True)\n",
    "self.b = torch.randn(output_size, requires_grad=True)\n",
    "y = x @ self.W.t() + self.b\n",
    "'''\n",
    "# are replaced by:\n",
    "'''\n",
    "self.linear = nn.Linear(input_size, output_size)\n",
    "y = self.linear(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing a Feedforward Neural Network (FNN)\n",
    "  - By leveraging nn.Module and built-in layers, we can further simplify the network:\n",
    "- 피드포워드 신경망(FNN) 구현\n",
    "  - nn.Module 및 내장 레이어를 활용하여 네트워크를 더욱 단순화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFeedforwardNN(nn.Module): # A simple feedforward neural network using nn.Module and built-in layers.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyFeedforwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.layer1(x))\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network inherits from nn.Module for better integration with PyTorch’s ecosystem.\n",
    "네트워크는 PyTorch 생태계와의 더 나은 통합을 위해 nn.Module을 상속합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing an RNN from Scratch Using nn.Linear\n",
    "  - An RNN processes sequences by main-taining a hidden state ht that evolves over time:\n",
    "    - h_t = tanh(x_t*W⊤_ih + h_(t−1)*W⊤_hh + b_h)\n",
    "- We can implement an RNN cell using nn.Linear:\n",
    "\n",
    "- nn.Linear를 사용하여 처음부터 RNN 구현\n",
    "  - RNN은 시간이 지남에 따라 진화하는 숨겨진 상태를 유지하여 시퀀스를 처리합니다.\n",
    "    - h_t = tanh(x_t*W⊤_ih h_(t−1)*W⊤_hh b_h)\n",
    "- nn.Linear를 사용하여 RNN 셀을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell(nn.Module): # An RNN cell implemented using nn.Linear layers.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNNCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        h_t = torch.tanh(self.i2h(x) + self.h2h(h_prev))\n",
    "        o_t = self.h2o(h_t)\n",
    "        return h_t\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we replaced the explicit weight matrices and biases with nn.Linear layers.\n",
    "- The previous-tensor operations:\n",
    "- 여기서는 명시적인 가중치 행렬과 편향을 nn.Linear 레이어로 대체했습니다.\n",
    "- 이전 텐서 연산:\n",
    "\n",
    "W_ih = torch.randn(input_size, hidden_size)\n",
    "\n",
    "W_hh = torch.randn(hidden_size, hidden_size)\n",
    "\n",
    "b_ih = torch.randn(hidden_size)\n",
    "\n",
    "b_hh = torch.randn(hidden_size)\n",
    "\n",
    "h_t = torch.tanh(x @ W_ih + b_ih + h_prev @ W_hh + b_hh)\n",
    "\n",
    "- are replaced by:\n",
    "\n",
    "self.i2h = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "h_t = torch.tanh(self.i2h(x) + self.h2h(h_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using nn.RNNCell\n",
    "  - PyTorch provides the nn.RNNCell module, which encapsulates the computa-tions of an RNN cell:\n",
    "- nn.RNNCell 사용\n",
    "  - PyTorch는 RNN 셀의 계산을 캡슐화하는 nn.RNNCell 모듈을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell(nn.Module): # An RNN cell implemented using nn.RNNCell.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNNCell, self).__init__()\n",
    "        self.cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, input_size)\n",
    "        h = self.init_hidden(x)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            h = self.rnn_cell(x[:, t, :], h)\n",
    "            y = self.output_layer(h)\n",
    "            outputs.append(y.unsqueeze(1))\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            return outputs\n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(x.size(0), hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The previous implementation using nn.Linear layers is replaced by nn.RNNCell, simplifying the code and leveraging optimized routines.\n",
    "- Using nn.RNN\n",
    "  - PyTorch’s nn.RNN module processes entire sequences efficiently:\n",
    "- nn.Linear 레이어를 사용한 이전 구현은 nn.RNNCell로 대체되어 코드를 단순화하고 최적화된 루틴을 활용합니다.\n",
    "- nn.RNN 사용\n",
    "  - PyTorch의 nn.RNN 모듈은 전체 시퀀스를 효율적으로 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module): # An RNN network implemented using nn.RNN.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, input_size)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The manual loop over time steps and the RNN cell are replaced by nn.RNN, which handles the sequence processing internally.\n",
    "- 시간 단계에 대한 수동 루프와 RNN 셀은 내부적으로 시퀀스 처리를 처리하는 nn.RNN으로 대체됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing an LSTM from Scratch (Simplified)\n",
    "  - An LSTM cell introduces gates to control the flow of information:\n",
    "    - it = σ(xtW⊤ ii + ht−1W⊤ hi + bi)\n",
    "    - ft = σ(xtW⊤ i f + ht−1W⊤ h f + bf )\n",
    "    - ot = σ(xtW⊤ io + ht−1W⊤ ho + bo)\n",
    "    - gt = tnh(xtW⊤ ig + ht−1W⊤ hg + bg)\n",
    "    - ct = ft ⊙ ct−1 + it ⊙ gt\n",
    "    - ht = ot ⊙ tanh(ct)\n",
    "\n",
    "- Implementing this from scratch using nn.Linear\n",
    "\n",
    "- 처음부터 LSTM 구현(간소화)\n",
    "  - LSTM 셀은 정보 흐름을 제어하기 위해 게이트를 도입합니다.\n",
    "    - it = σ(xtW⊤ ii + ht−1W⊤ hi + bi)\n",
    "    - ft = σ(xtW⊤ i f + ht−1W⊤ h f + bf )\n",
    "    - ot = σ(xtW⊤ io + ht−1W⊤ ho + bo)\n",
    "    - gt = tnh(xtW⊤ ig + ht−1W⊤ hg + bg)\n",
    "    - ct = ft ⊙ ct−1 + it ⊙ gt\n",
    "    - ht = ot ⊙ tanh(ct)\n",
    "\n",
    "- nn.Linear를 사용하여 처음부터 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module): # An LSTM cell implemented using nn.Linear layers.\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W_i = nn.Linear(input_size, hidden_size)\n",
    "        self.U_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_f = nn.Linear(input_size, hidden_size)\n",
    "        self.U_f = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size, hidden_size)\n",
    "        self.U_o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_g = nn.Linear(input_size, hidden_size)\n",
    "        self.U_g = nn.Linear(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        i_t = torch.sigmoid(self.W_i(x) + self.U_i(h_prev))\n",
    "        f_t = torch.sigmoid(self.W_f(x) + self.U_f(h_prev))\n",
    "        o_t = torch.sigmoid(self.W_o(x) + self.U_o(h_prev))\n",
    "        g_t = torch.tanh(self.W_g(x) + self.U_g(h_prev))\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using nn.LSTMCell\n",
    "  - We can simplify the LSTM cell implementation using nn.LSTMCell:\n",
    "- nn.LSTMCell 사용\n",
    "  - nn.LSTMCell을 사용하여 LSTM 셀 구현을 단순화할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module): # An LSTM cell implemented using nn.LSTMCell.\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        h_t, c_t = self.lstm_cell(x, (h_prev, c_prev))\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The manual computations of gates and cell states are replaced by nn.LSTMCell.\n",
    "- Implementing an LSTM Network\n",
    "  - To process sequences with the LSTM cell:\n",
    "- 게이트 및 셀 상태의 수동 계산이 nn.LSTMCell로 대체되었습니다.\n",
    "- LSTM 네트워크 구현\n",
    "  - LSTM 셀을 사용하여 시퀀스를 처리하려면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module): # An LSTM network using MyLSTMCell to process sequences.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = MyLSTMCell(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, input_size)\n",
    "        h, c = self.init_hidden(x)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            h, c = self.lstm_cell(x[:, t, :], h, c) # x[:, t, :] = x_t\n",
    "            y = self.output_layer(h)\n",
    "            outputs.append(y.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        return torch.zeros(x.size(0), self.hidden_size), torch.zeros(x.size(0), self.hidden_size),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using nn.LSTM\n",
    "  - PyTorch’s nn.LSTM simplifies the sequence processing:\n",
    "- nn.LSTM 사용\n",
    "  - PyTorch의 nn.LSTM은 시퀀스 처리를 단순화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module): # An LSTM network implemented using nn.LSTM.\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The manual loop and LSTM cell are replaced with nn.LSTM.\n",
    "- 수동 루프와 LSTM 셀은 nn.LSTM으로 대체됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing a Transformer Encoder Layer\n",
    "- Transformers utilize self-attention mechanisms.\n",
    "- A basic Transformer encoder layer includes multi-head attention and feedforward networks:\n",
    "- 변환기 인코더 계층 구현\n",
    "- Transformers는 self-attention 메커니즘을 활용합니다.\n",
    "- 기본 Transformer 인코더 계층에는 다중-헤드 주의 및 피드포워드 네트워크가 포함됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MyTransformerEncoderLayer(nn.Module):\n",
    "    # A Transformer encoder layer implemented using torch modules.\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MyTransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.linear1 = nn.Linear(d_model, d_model * 4)\n",
    "        self.linear2 = nn.Linear(d_model * 4, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src2, _ = self.self_attn(src, src, src)\n",
    "        src = self.norm1(src + src2)\n",
    "        src2 = self.linear2(F.relu(self.linear1(src)))\n",
    "        src = self.norm2(src + src2)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using nn.TransformerEncoderLayer\n",
    "- PyTorch provides nn.TransformerEncoderLayer for Trans-former architectures:\n",
    "\n",
    "- nn.TransformerEncoderLayer 사용\n",
    "- PyTorch는 Trans-former 아키텍처를 위한 nn.TransformerEncoderLayer를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformerEncoderLayer(nn.Module):\n",
    "    # A Transformer encoder layer using nn.TransformerEncoderLayer.\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(MyTransformerEncoderLayer, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        out = self.encoder_layer(src)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing a Transformer Model\n",
    "- A complete Transformer model can be built by stacking\n",
    "- encoder layers:\n",
    "- Transformer 모델 구현\n",
    "- 스태킹을 통해 완전한 Transformer 모델 구축 가능\n",
    "- 인코더 레이어:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    # A Transformer model implemented using nn.TransformerEncoder.\n",
    "    def __init__(self, d_model, nhead, num_layers, output_size):\n",
    "        super(MyTransformer, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "    def forward(self, src):\n",
    "        # src: (seq_length, batch_size, d_model)\n",
    "        out = self.transformer_encoder(src)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional Neural Networks (CNNs)\n",
    "- CNNs are effective for processing grid-like data such as images.\n",
    "- They utilize convolutional layers and pooling layers:\n",
    "- 컨볼루셔널 신경망(CNN)\n",
    "- CNN은 이미지와 같은 그리드 형태의 데이터를 처리하는 데 효과적입니다.\n",
    "- 컨볼루셔널 레이어와 풀링 레이어를 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    # A simple CNN model for image classification.\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120) # Adjust input features based on input image size\n",
    "        self.fc2 = nn.Linear(120, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x))) # Convolution + ReLU + Pooling\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5) # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Techniques to be covered\n",
    "  - Activation Functions: Introduce non-linearity (e.g., ReLU, Sigmoid, Tanh).\n",
    "  - Pooling Layers: Reduce spatial dimensions (e.g., Max Pooling, Average Pooling).\n",
    "  - Normalization Layers: Improve training stability (e.g., Batch Normalization).\n",
    "  - Dropout Layers: Prevent overfitting by randomly dropping units during training.\n",
    "  - Residual Connections: Enable training of deeper networks by adding shortcuts (used in ResNets).\n",
    "  - Attention Mechanisms: Allow models to focus on specific parts of the input (used in Transformers).\n",
    "- By implementing models incrementally and leveraging PyTorch’s modules, we can build complex architectures efficiently.\n",
    "-  Each abstraction allows us to focus on higher-level design without reinventing lower-level operations.\n",
    "- 다루어야 할 기술\n",
    "  - 활성화 함수: 비선형성을 도입합니다(예: ReLU, Sigmoid, Tanh).\n",
    "  - 풀링 레이어: 공간 차원을 줄입니다(예: 최대 풀링, 평균 풀링).\n",
    "  - 정규화 계층: 훈련 안정성을 향상시킵니다(예: 배치 정규화).\n",
    "  - 드롭아웃 레이어: 학습 중에 단위를 무작위로 삭제하여 과적합을 방지합니다.\n",
    "  - 잔여 연결: 바로가기(ResNets에서 사용됨)를 추가하여 더 깊은 네트워크 훈련을 가능하게 합니다.\n",
    "  - 주의 메커니즘: 모델이 입력의 특정 부분에 집중할 수 있도록 합니다(Transformers에서 사용됨).\n",
    "- 모델을 점진적으로 구현하고 PyTorch의 모듈을 활용함으로써 복잡한 아키텍처를 효율적으로 구축할 수 있습니다.\n",
    "- 각 추상화를 통해 하위 수준 작업을 재창조하지 않고도 상위 수준 설계에 집중할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Standard Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training the Model\n",
    "- Training a deep learning model involves iteratively updating the model parameters to minimize the loss function.\n",
    "- The training loop is a critical component that handles data loading, forward and backward passes, loss computation, optimization steps, and logging.\n",
    "- 모델 훈련\n",
    "- 딥러닝 모델을 훈련하려면 모델 매개변수를 반복적으로 업데이트하여 손실 함수를 최소화해야 합니다.\n",
    "- 훈련 루프는 데이터 로딩, 정방향 및 역방향 전달, 손실 계산, 최적화 단계 및 로깅을 처리하는 중요한 구성 요소입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic Training Loop\n",
    "- The basic training loop consists of the following steps:\n",
    "    1. Set the model to training mode using model.train().\n",
    "    2. Iterate over the training dataset using a DataLoader.\n",
    "    3. Forward pass: Compute the model’s output on the input data.\n",
    "    4. Compute loss: Calculate the loss between the predicted output and the true labels.\n",
    "    5. Backward pass: Perform backpropagation to compute gradients.\n",
    "    6. Optimizer step: Update the model parameters.\n",
    "    7. Logging: Record training loss and other metrics.\n",
    "- 기본 훈련 루프\n",
    "- 기본 훈련 루프는 다음 단계로 구성됩니다.\n",
    "    1. model.train()을 사용하여 모델을 훈련 모드로 설정합니다.\n",
    "    2. DataLoader를 사용하여 교육 데이터 세트를 반복합니다.\n",
    "    3. 순방향 전달: 입력 데이터에 대한 모델의 출력을 계산합니다.\n",
    "    4. 손실 계산: 예측된 출력과 실제 레이블 간의 손실을 계산합니다.\n",
    "    5. 역방향 전달: 역전파를 수행하여 기울기를 계산합니다.\n",
    "    6. 최적화 단계: 모델 매개변수를 업데이트합니다.\n",
    "    7. 로깅: 훈련 손실 및 기타 측정항목을 기록합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of a basic training loop that includes logging of training loss, evaluation on training and validation sets, and saving model checkpoints.\n",
    "다음은 훈련 손실 기록, 훈련 및 검증 세트 평가, 모델 체크포인트 저장을 포함하는 기본 훈련 루프의 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Train the model and evaluate on validation set.\n",
    "    Args:\n",
    "    model (nn.Module): The neural network model.\n",
    "    train_loader (DataLoader): DataLoader for the training data.\n",
    "    valid_loader (DataLoader): DataLoader for the validation data.\n",
    "    criterion (nn.Module): The loss function.\n",
    "    optimizer (Optimizer): The optimizer.\n",
    "    num_epochs (int): Number of epochs to train.\n",
    "    device (torch.device): Device to perform computations.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in valid_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "\n",
    "            total_valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader.dataset)\n",
    "    valid_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "        f'Train Loss: {avg_train_loss:.4f}, '\n",
    "        f'Valid Loss: {avg_valid_loss:.4f}, '\n",
    "        f'Valid Acc: {valid_accuracy:.4f}')\n",
    "    \n",
    "    # Checkpoint\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation\n",
    "    - Training Phase: The model is set to training mode using model.train(), and gradients are computed and used to update the model parameters.\n",
    "    - Validation Phase: The model is evaluated on the validation set using model.eval(), without computing gradients.\n",
    "    - Logging: Training loss, validation loss, and validation accuracy are printed for each epoch.\n",
    "    - Checkpointing: The model is saved whenever the validation loss improves.\n",
    "- 설명\n",
    "    - 훈련 단계: 모델은 model.train()을 사용하여 훈련 모드로 설정되고, 기울기가 계산되어 모델 매개변수를 업데이트하는 데 사용됩니다.\n",
    "    - 검증 단계: 기울기를 계산하지 않고 model.eval()을 사용하여 검증 세트에서 모델을 평가합니다.\n",
    "    - 로깅: 훈련 손실, 검증 손실, 검증 정확도가 각 에포크마다 인쇄됩니다.\n",
    "    - 체크포인트: 검증 손실이 개선될 때마다 모델이 저장됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Evaluation\n",
    "- Evaluating the model involves computing metrics on a test dataset that the model hasn’t seen during training.\n",
    "- This helps in assessing the generalization performance of the model.\n",
    "- 모델 평가\n",
    "- 모델 평가에는 훈련 중에 모델이 확인하지 못한 테스트 데이터세트에 대한 측정항목을 계산하는 작업이 포함됩니다.\n",
    "- 이는 모델의 일반화 성능을 평가하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "    model (nn.Module): The trained neural network model.\n",
    "    test_loader (DataLoader): DataLoader for the test data.\n",
    "    criterion (nn.Module): The loss function.\n",
    "    device (torch.device): Device to perform computations.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            total_test_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently Used Evaluation Metrics\n",
    "자주 사용되는 평가 지표\n",
    "- Accuracy\n",
    "  - Evaluation: The ratio of correctly predicted samples to the total number of samples.\n",
    "    - $ Accuracy = (Number of Correct Predictions / Total Number of Samples) $\n",
    "  - Usage: Commonly used in classification tasks where classes are balanced.\n",
    "  - Limitations: Not informative for imbalanced datasets, as it may give a misleading sense of performance.\n",
    "- 정확성\n",
    "  - 평가: 전체 샘플 수에 대해 정확하게 예측된 샘플의 비율입니다.\n",
    "    - $ 정확도 = (올바른 예측 수 / 총 샘플 수) $\n",
    "  - 용도: 클래스가 균형을 이루는 분류 작업에 일반적으로 사용됩니다.\n",
    "  - 제한 사항: 불균형한 데이터 세트에는 오해의 소지가 있는 성능을 제공할 수 있으므로 유익하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision, Recall, and F1 Score\n",
    "  - Precision:\n",
    "    - Evaluation: Measures the proportion of positive identifications that were actually correct.\n",
    "      - $ Precision = (True Positives / True Positives + False Positives) $\n",
    "    - Usage: Important when the cost of false positives is high.\n",
    "  - Recall:\n",
    "    - Evaluation: Measures the proportion of actual positives that were correctly identified.\n",
    "      - $ Recall = (True Positives / True Positives + False Negatives) $\n",
    "    - Usage: Important when the cost of false negatives is high.\n",
    "  - F1 Score:\n",
    "    - Evaluation: Harmonic mean of precision and recall.\n",
    "      - $ F1 Score = 2 ×  (Precision × Recall)/(Precision + Recall) $\n",
    "    - Usage: Provides a balance between precision and recall.\n",
    "  - Limitations: Do not account for true negatives; may not fully represent performance in all scenarios.\n",
    "\n",
    "- 정밀도, 재현율, F1 점수\n",
    "  - 정밀도:\n",
    "    - 평가: 실제로 정확했던 긍정적인 식별의 비율을 측정합니다.\n",
    "      - $ 정밀도 = (참 긍정 / 참 긍정 거짓 긍정) $\n",
    "    - 사용법: 오탐지 비용이 높을 때 중요합니다.\n",
    "  - 상기하다:\n",
    "    - 평가: 정확하게 식별된 실제 양성의 비율을 측정합니다.\n",
    "      - $ Recall = (참 긍정 / 참 긍정 거짓 부정) $\n",
    "    - 사용법: 위음성 비용이 높을 때 중요합니다.\n",
    "  - F1 점수:\n",
    "    - 평가: 정밀도와 재현율의 조화평균.\n",
    "      - $ F1 점수 = 2 × (정밀도 × 재현율)/(정밀도 재현율) $\n",
    "    - 사용법: 정밀도와 재현율 간의 균형을 제공합니다.\n",
    "  - 제한 사항: 참 부정을 설명하지 마십시오. 모든 시나리오에서 성능을 완전히 나타내지는 못할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "    - Evaluation: Plots the true positive rate against the false positive rate at various threshold settings and computes the area under the curve.\n",
    "    - Usage: Binary classification tasks, especially when classes are imbalanced.\n",
    "    - Limitations: May not be informative when classes are extremely imbalanced; focuses on ranking rather than absolute values\n",
    "- ROC-AUC(수신기 동작 특성 - 곡선 아래 면적)\n",
    "    - 평가: 다양한 임계값 설정에서 참양성률과 거짓양성률을 비교하고 곡선 아래 면적을 계산합니다.\n",
    "    - 용도: 특히 클래스가 불균형한 경우 이진 분류 작업입니다.\n",
    "    - 제한 사항: 클래스의 불균형이 심한 경우에는 유익하지 않을 수 있습니다. 절대값보다는 순위에 중점을 둡니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLEU Score (Bilingual Evaluation Understudy)\n",
    "    - Evaluation: Calculates the similarity between a candidate translation and a set of reference translations using n-gram overlaps.\n",
    "    - Usage: Machine translation, text summarization, and other sequence generation tasks.\n",
    "    - Limitations: Sensitive to exact matches; may not capture semantic meaning; does not consider sentence structure.\n",
    "- BLEU 점수(이중언어 평가 Understudy)\n",
    "    - 평가: n-gram 중복을 사용하여 후보 번역과 참조 번역 집합 간의 유사성을 계산합니다.\n",
    "    - 용도: 기계 번역, 텍스트 요약, 기타 시퀀스 생성 작업.\n",
    "    - 제한사항: 정확한 일치에 민감합니다. 의미론적 의미를 포착하지 못할 수도 있습니다. 문장 구조를 고려하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Limitations and Variants\n",
    "    - Accuracy: Misleading for imbalanced datasets; alternatives include balanced accuracy or Cohen’s Kappa.\n",
    "    - Precision and Recall: Focusing solely on one may not provide a complete performance picture; consider using the F1 Score.\n",
    "    - BLEU Variants: ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is another metric used, particularly in text summarization.\n",
    "- 제한 사항 및 변형\n",
    "    - 정확성: 불균형 데이터 세트에 대한 오해의 소지가 있습니다. 대안으로는 균형 잡힌 정확도 또는 Cohen's Kappa가 있습니다.\n",
    "    - 정밀도 및 재현율: 한 가지에만 초점을 맞추면 완전한 성능 그림을 제공할 수 없습니다. F1 점수 사용을 고려해보세요.\n",
    "    - BLEU 변형: ROUGE(Recall-Oriented Understudy for Gisting Evaluation)는 특히 텍스트 요약에 사용되는 또 다른 측정항목입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Various Training Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve model performance and training efficiency, several techniques are commonly employed.\n",
    "모델 성능과 훈련 효율성을 향상시키기 위해 여러 가지 기술이 일반적으로 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning Rate Scheduling\n",
    "    - Concept: Adjust the learning rate during training according to a predefined schedule.\n",
    "    - Usage: Helps in converging faster and escaping local minima by reducing the learning rate as training progresses.\n",
    "    - Example Code:\n",
    "- 학습률 스케줄링\n",
    "    - 개념: 미리 정의된 일정에 따라 학습 중 학습률을 조정합니다.\n",
    "    - 사용법: 훈련이 진행됨에 따라 학습률을 줄여 더 빠르게 수렴하고 로컬 최소값을 탈출하는 데 도움이 됩니다.\n",
    "    - 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Modify the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(...) # Your training function\n",
    "    scheduler.step() # Update the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Early Stopping\n",
    "    - Concept: Stop training when a monitored metric stops improving to prevent overfitting.\n",
    "    - Usage: Used when the validation loss starts to increase, indicating that the model is beginning to overfit the training data.\n",
    "    - Example Code:\n",
    "- 조기 정지\n",
    "    - 개념: 과적합을 방지하기 위해 모니터링된 지표가 개선되지 않으면 훈련을 중지합니다.\n",
    "    - 사용법: 검증 손실이 증가하기 시작할 때 사용됩니다. 이는 모델이 훈련 데이터에 과적합되기 시작했음을 나타냅니다.\n",
    "    - 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, train_loader, valid_loader, criterion,\n",
    "                                    optimizer, num_epochs, patience, device):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        valid_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (nn.Module): The loss function.\n",
    "        optimizer (Optimizer): The optimizer.\n",
    "        num_epochs (int): Maximum number of epochs to train.\n",
    "        patience (int): Number of epochs to wait for improvement before stopping.\n",
    "        device (torch.device): Device to perform computations.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase...\n",
    "        # Validation phase...\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping!')\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Clipping\n",
    "    - Concept: Limit the magnitude of gradients during backpropagation to prevent exploding gradients.\n",
    "    - Usage: Commonly used in training recurrent neural networks (RNNs) where gradients can grow exponentially.\n",
    "    - Example Code:\n",
    "- 그라데이션 클리핑\n",
    "    - 개념: 역전파 중 기울기의 크기를 제한하여 기울기가 폭발하는 것을 방지합니다.\n",
    "    - 사용법: 경사도가 기하급수적으로 증가할 수 있는 순환 신경망(RNN)을 훈련하는 데 일반적으로 사용됩니다.\n",
    "    - 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, target in train_loader:\n",
    "    # Forward pass...\n",
    "    # Compute loss...\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Augmentation\n",
    "    - Concept: Increase the diversity of training data by applying random transformations, thus improving model generalization.\n",
    "    - Usage: Used in computer vision tasks; transformations can include flipping, rotation, scaling, etc.\n",
    "    - Example Code:\n",
    "- 데이터 증강\n",
    "    - 개념: 무작위 변환을 적용하여 훈련 데이터의 다양성을 높여 모델 일반화를 향상시킵니다.\n",
    "    - 용도: 컴퓨터 비전 작업에 사용됩니다. 변환에는 뒤집기, 회전, 크기 조정 등이 포함될 수 있습니다.\n",
    "    - 예제 코드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Normalization and Dropout\n",
    "    - Batch Normalization:\n",
    "        - Concept: Normalizes the input of each layer to improve training speed and stability.\n",
    "        - Usage: Used in deep networks to mitigate internal covariate shift.\n",
    "    - Dropout:\n",
    "        - Concept: Randomly sets a fraction of input units to zero during training to prevent overfitting.\n",
    "        - Usage: Applied in fully connected layers or between layers.\n",
    "- Example Code with Batch Normalization and Dropout\n",
    "- 배치 정규화 및 드롭아웃\n",
    "    - 일괄 정규화:\n",
    "        - 개념: 각 레이어의 입력을 정규화하여 훈련 속도와 안정성을 향상시킵니다.\n",
    "        - 사용법: 내부 공변량 이동을 완화하기 위해 심층 네트워크에서 사용됩니다.\n",
    "    - 탈락:\n",
    "        - 개념: 과적합을 방지하기 위해 훈련 중에 입력 단위의 비율을 무작위로 0으로 설정합니다.\n",
    "        - 사용법: 완전 연결 레이어 또는 레이어 사이에 적용됩니다.\n",
    "- 일괄 정규화 및 드롭아웃이 포함된 예제 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedNN(nn.Module):\n",
    "    def __init__(self, input_size=20, hidden_size=64, num_classes=2):\n",
    "        super(EnhancedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Loss Functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss functions are a crucial component in training neural networks, as they quantify the difference between the predicted outputs and the true targets.\n",
    "- PyTorch provides a variety of loss functions suitable for different tasks.\n",
    "- In this section, we will explore some commonly used loss functions, their mathematical formulations, and usage examples with PyTorch code.\n",
    "- 손실 함수는 예측된 출력과 실제 목표 간의 차이를 정량화하므로 신경망 훈련에서 중요한 구성 요소입니다.\n",
    "- PyTorch는 다양한 작업에 적합한 다양한 손실 기능을 제공합니다.\n",
    "- 이 섹션에서는 일반적으로 사용되는 손실 함수, 수학적 공식 및 PyTorch 코드 사용 예를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Loss Function**          | **Use Case**                        | **PyTorch Function**         | **Input/Output Shapes**                                      |\n",
    "|----------------------------|-------------------------------------|------------------------------|---------------------------------------------------------------|\n",
    "| **Mean Squared Error (MSE) Loss** | Regression                          | `nn.MSELoss()`               | Outputs: (batch_size, n_outputs) <br> Targets: Same as outputs |\n",
    "| **Cross-Entropy Loss**      | Classification (Multi-class)        | `nn.CrossEntropyLoss()`       | Outputs: (batch_size, n_classes) <br> Targets: (batch_size)    |\n",
    "| **Binary Cross-Entropy (BCE) Loss** | Classification (Binary)            | `nn.BCELoss()`               | Outputs: (batch_size, n_outputs), values in [0, 1] <br> Targets: Same as outputs |\n",
    "| **BCE with Logits Loss**    | Classification (Binary)            | `nn.BCEWithLogitsLoss()`      | Outputs: (batch_size, n_outputs) (raw logits) <br> Targets: Same as outputs |\n",
    "| **Negative Log-Likelihood (NLL) Loss** | Classification (Multi-class)      | `nn.NLLLoss()`               | Log_probs: (batch_size, n_classes) <br> Targets: (batch_size)  |\n",
    "| **KL Divergence Loss**      | Probability Distributions           | `nn.KLDivLoss()`             | Log_probs: (batch_size, n_classes) <br> Targets: Same as log_probs |\n",
    "| **Margin Ranking Loss**     | Learning to Rank                    | `nn.MarginRankingLoss()`      | x1, x2: (batch_size) <br> Targets: (batch_size), values −1 or 1 |\n",
    "| **Huber Loss (Smooth L1 Loss)** | Regression (Robust)                | `nn.SmoothL1Loss()`          | Outputs: (batch_size, n_outputs) <br> Targets: Same as outputs |\n",
    "| **Multi-Label Soft Margin Loss** | Classification (Multi-label)        | `nn.MultiLabelSoftMarginLoss()` | Outputs: (batch_size, n_classes) (raw logits) <br> Targets: Same as outputs |\n",
    "| **Cosine Embedding Loss**   | Similarity Learning                 | `nn.CosineEmbeddingLoss()`    | x1, x2: (batch_size, embedding_dim) <br> Targets: (batch_size), values −1 or 1 |\n",
    "| **Triplet Margin Loss**     | Learning Embeddings                 | `nn.TripletMarginLoss()`      | Anchor, Positive, Negative: (batch_size, embedding_dim)        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cheat Sheet(code) for each Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean Squared Error (MSE) Loss\n",
    "- 평균 제곱 오차(MSE) 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inpurts\n",
    "batch_size = 4\n",
    "n_outputs = 1\n",
    "outputs = torch.randn(batch_size, n_outputs, requires_grad=True)\n",
    "targets = torch.randn(batch_size, n_outputs)\n",
    "\n",
    "# Intiialize loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('MSE Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_classes = 3\n",
    "outputs = torch.randn(batch_size, n_classes, requires_grad=True)\n",
    "targets = torch.randint(0, n_classes, (batch_size,))\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('Cross-Entropy Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary Cross-Entropy (BCE) Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_outputs = 1\n",
    "outputs = torch.sigmoid(torch.randn(batch_size, n_outputs, requires_grad=True))\n",
    "targets = torch.randint(0, 2, (batch_size, n_outputs)).float()\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('BCE Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary Cross-Entropy with Logits Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_outputs = 1\n",
    "outputs = torch.randn(batch_size, n_outputs, requires_grad=True)\n",
    "targets = torch.randint(0, 2, (batch_size, n_outputs)).float()\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('BCE with Logits Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative Log-Likelihood (NLL) Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_classes = 3\n",
    "outputs = torch.randn(batch_size, n_classes, requires_grad=True)\n",
    "log_probs = torch.log_softmax(outputs, dim=1)\n",
    "targets = torch.randint(0, n_classes, (batch_size,))\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(log_probs, targets)\n",
    "print('NLL Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL Divergence Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_classes = 3\n",
    "outputs = torch.randn(batch_size, n_classes, requires_grad=True)\n",
    "log_probs = torch.log_softmax(outputs, dim=1)\n",
    "targets = torch.softmax(torch.randn(batch_size, n_classes), dim=1)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(log_probs, targets)\n",
    "print('KL Divergence Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Margin Ranking Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "x1 = torch.randn(batch_size, requires_grad=True)\n",
    "x2 = torch.randn(batch_size, requires_grad=True)\n",
    "targets = torch.randint(0, 2, (batch_size,)) * 2 - 1 # Converts to -1 or 1\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(x1, x2, targets)\n",
    "print('Margin Ranking Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huber Loss (Smooth L1 Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_outputs = 1\n",
    "outputs = torch.randn(batch_size, n_outputs, requires_grad=True)\n",
    "targets = torch.randn(batch_size, n_outputs)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('Smooth L1 Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multi-Label Soft Margin Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "n_classes = 3\n",
    "outputs = torch.randn(batch_size, n_classes, requires_grad=True)\n",
    "targets = torch.randint(0, 2, (batch_size, n_classes)).float()\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(outputs, targets)\n",
    "print('Multi-Label Soft Margin Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine Embedding Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "embedding_dim = 5\n",
    "x1 = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "x2 = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "targets = torch.randint(0, 2, (batch_size,)) * 2 - 1 # Converts to -1 or 1\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.CosineEmbeddingLoss(margin=0.5)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(x1, x2, targets)\n",
    "print('Cosine Embedding Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Triplet Margin Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define inputs\n",
    "batch_size = 4\n",
    "embedding_dim = 5\n",
    "anchor = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "positive = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "negative = torch.randn(batch_size, embedding_dim, requires_grad=True)\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(anchor, positive, negative)\n",
    "print('Triplet Margin Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Explanation of Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When to Use Each Loss Function\n",
    "    - MSE Loss: Use for regression tasks where the goal is to predict continuous values.\n",
    "    - Cross-Entropy Loss: Use for multi-class classification tasks with mutually exclusive classes.\n",
    "    - BCE Loss: Use for binary classification tasks where outputs are probabilities.\n",
    "    - BCE with Logits Loss: Use for binary classification with raw logits; combines sigmoid activation and BCE loss for numerical stability.\n",
    "    - NLL Loss: Use for classification tasks when you have log-probabilities as inputs.\n",
    "    - KL Divergence Loss: Use when comparing two probability distributions.\n",
    "    - Margin Ranking Loss: Use for ranking tasks where you want to learn the relative ordering.\n",
    "    - Huber Loss: Use for regression tasks that are robust to outliers.\n",
    "    - Multi-Label Soft Margin Loss: Use for multi-label classification where each sample can belong to multiple classes.\n",
    "    - Cosine Embedding Loss: Use for measuring similarity or dissimilarity between two samples.\n",
    "    - Triplet Margin Loss: Use for embedding learning to ensure that similar samples are closer than dissimilar ones.\n",
    "- 각 손실 함수를 사용하는 경우\n",
    "    - MSE 손실: 연속 값을 예측하는 것이 목표인 회귀 작업에 사용합니다.\n",
    "    - 교차 엔트로피 손실: 상호 배타적인 클래스가 있는 다중 클래스 분류 작업에 사용합니다.\n",
    "    - BCE 손실: 출력이 확률인 이진 분류 작업에 사용합니다.\n",
    "    - 로지트 손실이 있는 BCE: 원시 로지트를 사용한 이진 분류에 사용합니다. 수치적 안정성을 위해 시그모이드 활성화와 BCE 손실을 결합합니다.\n",
    "    - NLL 손실: 입력으로 로그 확률이 ​​있는 경우 분류 작업에 사용합니다.\n",
    "    - KL Divergence Loss: 두 확률 분포를 비교할 때 사용합니다.\n",
    "    - 마진 순위 손실: 상대 순서를 배우고 싶은 순위 작업에 사용합니다.\n",
    "    - Huber Loss: 이상치에 강한 회귀 작업에 사용합니다.\n",
    "    - 다중 레이블 소프트 마진 손실: 각 샘플이 여러 클래스에 속할 수 있는 다중 레이블 분류에 사용합니다.\n",
    "    - 코사인 임베딩 손실: 두 샘플 간의 유사성 또는 차이점을 측정하는 데 사용됩니다.\n",
    "    - Triplet Margin Loss: 유사한 샘플이 다른 샘플보다 더 가까운지 확인하기 위해 임베딩 학습에 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input and Output Shapes\n",
    "- Understanding the expected input and output shapes is crucial for correctly implementing loss functions:\n",
    "    - Outputs: Typically the model’s predictions; shapes vary depending on the task.\n",
    "    - Targets: Ground truth labels; must match the shape and type expected by the loss function.\n",
    "- 입력 및 출력 형태\n",
    "- 손실 함수를 올바르게 구현하려면 예상되는 입력 및 출력 형태를 이해하는 것이 중요합니다.\n",
    "    - 출력: 일반적으로 모델의 예측입니다. 작업에 따라 모양이 달라집니다.\n",
    "    - 대상: Ground Truth 라벨; 손실 함수에서 예상되는 모양 및 유형과 일치해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining Functions for Numerical Stability\n",
    "  - Some loss functions, like nn.BCEWithLogitsLoss, combine activation functions and loss computations internally. This approach improves numerical stability by integrating operations that are commonly used together, reducing the risk of issues like vanishing gradients.\n",
    "- Example: BCEWithLogitsLoss vs. Sigmoid + BCELoss\n",
    "  - nn.BCEWithLogitsLoss is preferred over applying a sigmoid activation followed by nn.BCELoss because it combines these steps in a numerically stable way.\n",
    "- 수치적 안정성을 위한 기능 결합\n",
    "  - nn.BCEWithLogitsLoss와 같은 일부 손실 함수는 활성화 함수와 손실 계산을 내부적으로 결합합니다. 이 접근 방식은 일반적으로 함께 사용되는 연산을 통합하여 수치적 안정성을 향상시켜 기울기 소멸과 같은 문제의 위험을 줄입니다.\n",
    "- 예: BCEWithLogitsLoss 대 Sigmoid BCELoss\n",
    "  - nn.BCEWithLogitsLoss는 이러한 단계를 수치적으로 안정적인 방식으로 결합하기 때문에 시그모이드 활성화를 적용한 후 nn.BCELoss를 적용하는 것보다 선호됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BCEWithLogitsLoss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "# Equivalent but less stable approach\n",
    "sigmoid = nn.Sigmoid()\n",
    "outputs_sigmoid = sigmoid(outputs)\n",
    "criterion_bce = nn.BCELoss()\n",
    "loss_bce = criterion_bce(outputs_sigmoid, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By handling the sigmoid operation within the loss function, nn.BCEWithLogitsLoss ensures better stability during training.\n",
    "- 손실 함수 내에서 시그모이드 연산을 처리함으로써 nn.BCEWithLogitsLoss는 훈련 중에 더 나은 안정성을 보장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Detailed Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss functions measure the discrepancy between the model’s predictions and the target values.\n",
    "- They are essential for training neural networks, guiding the optimization process by providing a scalar value to minimize.\n",
    "- We will explore several commonly used loss functions in PyTorch, including their mathematical formulations, expected inputs, and example usage.\n",
    "- 손실 함수는 모델의 예측과 목표 값 간의 불일치를 측정합니다.\n",
    "- 최소화할 스칼라 값을 제공하여 최적화 프로세스를 안내하는 신경망 훈련에 필수적입니다.\n",
    "- 수학적 공식, 예상 입력 및 예제 사용법을 포함하여 PyTorch에서 일반적으로 사용되는 몇 가지 손실 함수를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mean Squared Error Loss (nn.MSELoss)\n",
    "  - Description: Also known as L2 loss, this loss function computes the mean squared error between the input (predictions) and the target.\n",
    "- Mathematical Formulation:\n",
    "    - $ LMSE = 1/N * N ∑N i=1 (xi − yi)^2 $\n",
    "- where:\n",
    "    - xi is the predicted value,\n",
    "    - yi is the target value,\n",
    "    - N is the number of samples.\n",
    "- Expected Input:\n",
    "    - input: Tensor of arbitrary shape.\n",
    "    - target: Tensor of the same shape as input.\n",
    "- 평균 제곱 오류 손실(nn.MSELoss)\n",
    "  - 설명: L2 손실이라고도 하는 이 손실 함수는 입력(예측)과 목표 사이의 평균 제곱 오차를 계산합니다.\n",
    "- 수학적 공식:\n",
    "    - $ LMSE = 1/N * N ∑N i=1 (xi − yi)^2 $\n",
    "- 어디:\n",
    "    - xi는 예측값이고,\n",
    "    - yi는 목표값이고,\n",
    "    - N은 샘플 수입니다.\n",
    "- 예상 입력:\n",
    "    - 입력: 임의 형태의 텐서.\n",
    "    - target : 입력과 동일한 형태의 Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create example inputs\n",
    "batch_size = 4\n",
    "n_features = 3\n",
    "predictions = torch.randn(batch_size, n_features, requires_grad=True)\n",
    "targets = torch.randn(batch_size, n_features)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation: The nn.CrossEntropyLoss expects the raw logits as input and applies F.log_softmax internally. It then computes the negative log-likelihood loss.\n",
    "- Note: Do not apply softmax to the predictions before passing them to nn.CrossEntropyLoss.\n",
    "- Negative Log-Likelihood Loss (nn.NLLLoss)\n",
    "  - Description: The negative log-likelihood loss is used in classification problems, usually in combination with nn.LogSoftmax as the final layer.\n",
    "- Mathematical Formulation:\n",
    "  - $ LNLL = −(1/N)∑N i=1 log(pi,yi) $\n",
    "- where:\n",
    "    - pi,yi is the predicted probability for the true class yi for sample i,\n",
    "    - N is the number of samples.\n",
    "- Expected Input:\n",
    "    - input: Tensor of shape (N, C) containing log-probabilities (log-softmax outputs).\n",
    "    - target: Tensor of shape (N,) with class indices in the range [0, C − 1].\n",
    "\n",
    "- 설명: nn.CrossEntropyLoss는 원시 로짓을 입력으로 예상하고 내부적으로 F.log_softmax를 적용합니다. 그런 다음 음의 로그 우도 손실을 계산합니다.\n",
    "- 참고: 예측을 nn.CrossEntropyLoss에 전달하기 전에 예측에 소프트맥스를 적용하지 마세요.\n",
    "- 음의 로그 우도 손실(nn.NLLLoss)\n",
    "  - 설명: 음의 로그 우도 손실은 분류 문제에 일반적으로 최종 계층으로 nn.LogSoftmax와 함께 사용됩니다.\n",
    "- 수학적 공식:\n",
    "  - $ LNLL = −(1/N)∑N i=1 log(pi,yi) $\n",
    "- 어디:\n",
    "    - pi,yi는 샘플 i에 대한 실제 클래스 yi에 대한 예측 확률입니다.\n",
    "    - N은 샘플 수입니다.\n",
    "- 예상 입력:\n",
    "    - 입력: 로그 확률(log-softmax 출력)을 포함하는 모양(N, C)의 텐서.\n",
    "    - 대상: [0, C − 1] 범위의 클래스 인덱스를 갖는 모양(N,)의 텐서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example inputs\n",
    "batch_size = 5\n",
    "n_classes = 4\n",
    "log_probs = F.log_softmax(torch.randn(batch_size, n_classes, requires_grad=True), dim=1)\n",
    "targets = torch.tensor([1, 2, 0, 3, 1])\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(log_probs, targets)\n",
    "\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation: The nn.NLLLoss expects the input to be log-probabilities (i.e., after applying log_softmax). If you have raw logits, you need to apply F.log_softmax before passing them to nn.NLLLoss.\n",
    "- Binary Cross-Entropy Loss (nn.BCELoss)\n",
    "  - Description: Computes the binary cross-entropy loss between the target and the output probabilities.\n",
    "- Mathematical Formulation:\n",
    "  - $ LBCE = −(1/N) ∑N i=1[yi log(xi) + (1 − yi) log(1 − xi)] $\n",
    "- where:\n",
    "    - xi is the predicted probability for sample i,\n",
    "    - yi is the target value (0 or 1),\n",
    "    - N is the number of samples.\n",
    "- Expected Input:\n",
    "    - input: Tensor of arbitrary shape with probabilities between 0 and 1.\n",
    "    - target: Tensor of the same shape as input with values 0 or 1.\n",
    "- 설명: nn.NLLLoss는 입력이 로그 확률일 것으로 예상합니다(즉, log_softmax를 적용한 후). 원시 로짓이 있는 경우 nn.NLLLoss에 전달하기 전에 F.log_softmax를 적용해야 합니다.\n",
    "- 바이너리 교차 엔트로피 손실(nn.BCELoss)\n",
    "  - 설명: 목표 확률과 출력 확률 간의 이진 교차 엔트로피 손실을 계산합니다.\n",
    "- 수학적 공식:\n",
    "  - $ LBCE = −(1/N) ∑N i=1[yi log(xi) (1 − yi) log(1 − xi)] $\n",
    "- 어디:\n",
    "    - xi는 표본 i에 대한 예측 확률입니다.\n",
    "    - yi는 목표값(0 또는 1)이고,\n",
    "    - N은 샘플 수입니다.\n",
    "- 예상 입력:\n",
    "    - 입력: 0과 1 사이의 확률을 갖는 임의 형태의 텐서.\n",
    "    - 대상: 값이 0 또는 1인 입력과 동일한 모양의 텐서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example inputs\n",
    "batch_size = 6\n",
    "predictions = torch.sigmoid(torch.randn(batch_size, requires_grad=True))\n",
    "targets = torch.randint(0, 2, (batch_size,)).float()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(predictions, targets)\n",
    "\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep_learning_&_pytorch_introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to RNN - Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오늘의 복습용 문제\n",
    "\n",
    "1. l = list of list of .... 를 받아서, 하는 함수 get_shape를 작성하세요.\n",
    "  - 여기서 torch.tensor(l).shape 과 같은 값을 가지는 리스트를 반환\n",
    "  - 만약 torch.tensor가 불가능하다면 False를 리턴\n",
    "2. l = list of list of ..., r = list of list of ... 두 input을 받아서, 아래와 같이 동작하는 함수 broadcasting을 작성하세요\n",
    "  - 브로드캐스팅이 될 때 각 l, r이 바뀌어야 하는 형태를 리턴\n",
    "  - 브로드캐스팅이 되지 않으면 False를 리턴\n",
    "\n",
    "[[1,2,3], [1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(lst):\n",
    "    res = []\n",
    "    cur = lst\n",
    "\n",
    "    while isinstance(cur, list):\n",
    "        res.append(len(cur))\n",
    "        cur = cur[0]\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_shape(lst):\n",
    "    if not isinstance(lst, list):\n",
    "        return []\n",
    "    else:\n",
    "        shapes = []\n",
    "        for elem in lst:\n",
    "            shape = get_shape(elem)\n",
    "            if shape not in shapes:\n",
    "                shapes.append(shape)\n",
    "        if len(shapes) == 1:\n",
    "            return [len(lst)] + get_shape(lst[0])\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "def fill(l, r):\n",
    "    \"\"\"If len(l) > len(r), fill 1 to r's front, so that len(l) == len(r),\n",
    "    If len(r) < len(l), do the opposite.\n",
    "    \"\"\"\n",
    "    if len(l) > len(r):\n",
    "        diff = len(l) - len(r)\n",
    "        r = [1 for _ in range(diff)] + r\n",
    "        return l, r\n",
    "    elif len(l) < len(r):\n",
    "        diff = len(r) - len(l)\n",
    "        l = [1 for _ in range(diff)] + l\n",
    "        return l, r\n",
    "    return l, r\n",
    "\n",
    "def expand_dimension(l, dim_idx, r_s):\n",
    "    \"\"\"\n",
    "    l = [[1,2,3,]] (shape 1, 3)\n",
    "    dim_idx = 0\n",
    "    r_s = 4\n",
    "    expand_dimension(l, 0, 4)\n",
    "    >> [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
    "\n",
    "    l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]] (shape 3, 1, 3 -> 3, 2, 3)\n",
    "    l[0] = [[1,2,3]], l[1], l[2] (shape 2, 3)\n",
    "    dim_idx = 1\n",
    "    r_s = 2\n",
    "    expand_dimension(l, 0, 2)\n",
    "    >> [[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]] (shape 3, 2, 3)\n",
    "\n",
    "    l / shape 4, 3, 2, 1, 2 -> 4, 3, 2, 5, 2\n",
    "    expand_dimension(l, 3, 5)\n",
    "    l[0], l[1], l[2], l[3] / shape 3, 2, 1, 2 -> 3, 2, 5, 2\n",
    "    expand_dimension(l[0], 2, 5)\n",
    "    expand_dimension(l[1], 2, 5)\n",
    "    expand_dimension(l[2], 2, 5)\n",
    "    expand_dimension(l[3], 2, 5)\n",
    "    \"\"\"\n",
    "    assert get_shape(l)[dim_idx] == 1, (get_shape(l), dim_idx)\n",
    "\n",
    "    if dim_idx == 0:\n",
    "        return [l[0] for _ in range(r_s)]\n",
    "    else:\n",
    "        return [expand_dimension(e, dim_idx - 1, r_s) for e in l]\n",
    "\n",
    "\n",
    "def broadcasting(l, r):\n",
    "    shape_l = get_shape(l)\n",
    "    shape_r = get_shape(r)\n",
    "\n",
    "    assert shape_l and shape_r\n",
    "\n",
    "    # 차원의 맞추기: 두 텐서의 차원(Dimension) 수가 다를 때,\n",
    "    # 차원이 작은 텐서의 앞쪽에 1을 추가하여 차원을 맞춥니다.\n",
    "\n",
    "    # (2, 3) / (4, 5, 2, 3) -> (1, 1, 2, 3) / (4, 5, 2, 3)\n",
    "\n",
    "    l_is_bigger = False\n",
    "    r_is_bigger = False\n",
    "    diff = abs(len(shape_l) - len(shape_r))\n",
    "\n",
    "    if len(shape_l) > len(shape_r):\n",
    "        l_is_bigger = True\n",
    "    elif len(shape_l) < len(shape_r):\n",
    "        r_is_bigger = True\n",
    "\n",
    "    shape_l, shape_r = fill(shape_l, shape_r)\n",
    "\n",
    "    for _ in range(diff):\n",
    "        if l_is_bigger:\n",
    "            r = [r]  # r.shape: a1, a2, ... , an / [r].shape : 1, a1, a2, ... , an\n",
    "        elif r_is_bigger:\n",
    "            l = [l]\n",
    "\n",
    "    assert shape_l == get_shape(l)\n",
    "    assert shape_r == get_shape(r)\n",
    "\n",
    "    # 크기 맞추기: 각 차원에서 크기가 1인 텐서는\n",
    "    # 해당 차원의 크기를 큰 텐서의 크기에 맞춰 늘릴 수 있습니다.\n",
    "\n",
    "    dim_idx = 0\n",
    "\n",
    "    for l_s, r_s in zip(shape_l, shape_r):\n",
    "        if l_s != r_s:\n",
    "            if min(l_s, r_s) == 1:\n",
    "                if l_s == 1: #\n",
    "                    l = expand_dimension(l, dim_idx, r_s)\n",
    "                else: # r_s == 1\n",
    "                    r = expand_dimension(r, dim_idx, l_s)\n",
    "            else:\n",
    "                return False\n",
    "        dim_idx += 1\n",
    "\n",
    "    return l, r\n",
    "\n",
    "l = [[[1,2,3]], [[1,2,3]], [[1,2,3]]]\n",
    "print(\"get_shape(I) :\", get_shape(l))\n",
    "\n",
    "r = [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]\n",
    "print(get_shape(r))\n",
    "\n",
    "# 3 1 3 / 4 3 -> 3 1 3 / 1 4 3 -> 3 1 3 / 3 4 3 -> 3 4 3 / 3 4 3\n",
    "# r = [[[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
    "r_ans = [[\n",
    "            [1,2,3,],\n",
    "            [1,2,3,],\n",
    "            [1,2,3,],\n",
    "            [1,2,3,]\n",
    "          ],\n",
    "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]],\n",
    "         [[1,2,3,], [1,2,3,], [1,2,3,], [1,2,3,]]]\n",
    "l_ans = [[[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
    "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]],\n",
    "         [[1,2,3], [1,2,3], [1,2,3], [1,2,3]]]\n",
    "l, r = broadcasting(l, r)\n",
    "print(get_shape(l), get_shape(r))\n",
    "print(l_ans == l, r_ans == r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Pytorch 설치 및 확인\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 파일을 다운로드합니다.\n",
    "\n",
    "https://download.pytorch.org/tutorial/data.zip\n",
    "\n",
    "어떤 형식으로 데이터가 저장되어 있는지 한번 열어서 읽어보세요.\n",
    "\n",
    "왼쪽 바 폴더모양 (files) 클릭 후 코랩에서 쓸 수 있도록 언어별 이름 데이터를 업로드해주세요.\n",
    "\n",
    "`names/` 폴더를 만든 후 그 안에 집어넣습시다.\n",
    "\n",
    "텍스트 파일을 클릭해서 정상적으로 업로드 되었는지 내용을 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `glob`으로 데이터 읽기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glob`은 패턴 매칭을 통해 디스크에 어떤 파일이 있는지 파일명을 읽어옵니다.\n",
    "\n",
    "우리가 사용할 데이터 목록인 18개 국어 각각의 파일명을 읽어옵시다.\n",
    "\n",
    "이후 파이썬 내장 파일 입출력 기능으로 파일을 읽어옵시다.\n",
    "\n",
    "읽어온 데이터는 다음 변수에 저장해둡시다.\n",
    "\n",
    "모든 이름은 알파벳 소문자만 사용해서 저장해둡시다.\n",
    "\n",
    "`category_names: dict[str, list[str]]`\n",
    "\n",
    "`all_categories: list[str]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('names/*.txt')\n",
    "assert len(files) == 18\n",
    "print(files)\n",
    "\n",
    "category_names = {}\n",
    "all_categories = []\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "n_letters = len(all_letters)\n",
    "assert n_letters == 26 # 이게 아니면 assertionError를 발생시킴\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        names = f.read().strip().split('\\n')\n",
    "        #strip() : 파이썬 문자열에서 양 끝에 있는 공백이나 특정 문자를 제거하는데 사용하는 함수\n",
    "\n",
    "    lang = file.split('/')[-1].split('.')[0]\n",
    "    # : split으로 분리한 다음에 -1번째 0번째\n",
    "    all_categories.append(lang)\n",
    "\n",
    "    names = [n.lower() for n in names] # Make everything lowercases\n",
    "    names = [''.join([c for c in n if c in all_letters]) for n in names] # Ignore non-alphabet letters\n",
    "    category_names[lang] = names\n",
    "\n",
    "    print(f'{lang}: {len(names)} |', names[0], names[1], names[2])\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "files = glob.glob('names/*.txt')\n",
    "\n",
    "assert len(files) == 18\n",
    "\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "character_dict = defaultdict(int)\n",
    "\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        names = f.read().strip().split('\\n')\n",
    "    lang = file.split('/')[1].split('.')[0]\n",
    "\n",
    "    for name in names:\n",
    "        for char in name:\n",
    "            character_dict[char.lower()] += 1\n",
    "\n",
    "lst = []\n",
    "for k, v in character_dict.items():\n",
    "    lst.append((k, v))\n",
    "\n",
    "lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
    "total_count = sum([e[1] for e in lst])\n",
    "\n",
    "s = 0\n",
    "alphabets = []\n",
    "\n",
    "for k, v in lst:\n",
    "    s += v\n",
    "    if s > 0.999*total_count:\n",
    "        print(f'{k}: {v} - considered OOV')  # OOV : out of voca\n",
    "        alphabets.append(k)\n",
    "    else:\n",
    "        print(f'{k}: {v}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 이름을 텐서로 변환하는 방법입니다. 알단은 변환 함수만 정의해둡니다.\n",
    "\n",
    "하나의 알파벳 문자를 \"one-hot vector\" 방식으로 표상합시다. 보다 구체적으로는 다음과 같습니다.\n",
    "\n",
    "```\n",
    "a -> <1 0 0 0 ... 0>\n",
    "b -> <0 1 0 0 ... 0>\n",
    "...\n",
    "z -> <0 0 0 0 ... 1>\n",
    "```\n",
    "\n",
    "문자 하나가 아닌 단어 하나는 (line_length, n_letters)이라는 shape를 갖는 텐서로 변환됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot 인코딩\n",
    "def letter2tensor(letter):\n",
    "  res = []\n",
    "\n",
    "  for char in all_letters:\n",
    "    if char == letter:\n",
    "      res.append(1)\n",
    "    else:\n",
    "      res.append(0)\n",
    "  return torch.tensor([res])\n",
    "\n",
    "a_tensor = letter2tensor('a')\n",
    "print(a_tensor.shape) # (26, 1)\n",
    "# a_tensor = torch.tensor([1, 0, ..., 0])\n",
    "\n",
    "def word2tensor(word): # < batch_size x word_length x n_letters>, # 단어를 원핫인코딩으로\n",
    "    res = torch.zeros(len(word), len(all_letters))\n",
    "\n",
    "    for idx, char in enumerate(word):\n",
    "        res[idx] = letter2tensor(char)\n",
    "    return res # res.squeeze(dim = 1)\n",
    "\n",
    "print(word2tensor('abc').shape)\n",
    "print(word2tensor('abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.zeros(3,1,2)\n",
    "\n",
    "a.squeeze(dim = 1)\n",
    "print(a.shape)\n",
    "print(a.unsqueeze(dim = 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter.lower())\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('c'))\n",
    "\n",
    "print(lineToTensor('cat').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "aaa = torch.randn(18)\n",
    "print(aaa)\n",
    "print(torch.softmax(aaa, -1))\n",
    "print(torch.sum(torch.softmax(aaa, -1)))\n",
    "torch.tensor([0, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 레이어를 통과하면, 어떠한 값이든 확률 값으로 변하게 됩니다.\n",
    "\n",
    "위 예시 코드에서, 랜덤 값인 `aaa`가 softmax 레이어를 통과했더니 확률값으로 변화했다는 점을 관찰하세요. 입력값인 원래의 `aaa`에 저장되어 있던 값이 더 클수록 더 큰 확률값으로 변하게 됩니다. 이 값이 확률값이라는 점은 더했을 때 1이 된다는 점에서 알 수 있습니다.\n",
    "\n",
    "신경망의 마지막 레이어의 값을 해석하기 위하여, 보통 마지막 레이어에 softmax를 넣어두는 경우가 많습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `dim` parameter (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 섹션은 어렵지만 중요한 내용을 다룹니다. `torch.sum`, `torch.max` 등 수많은 함수에서 똑같이 사용되는 내용입니다. Softmax를 예시로 들어 설명하겠습니다.\n",
    "\n",
    "`torch.softmax(aaa, -1)`에서 적용할 `-1`은 텐서 차원의 인덱스입니다. 이 값을 이해하기 위해 예시를 들겠습니다.\n",
    "\n",
    "예를 들어, shape가 (2, 3)인 텐서에 softmax를 적용하고 싶다면 어떻게 해야 할까요?\n",
    "\n",
    "```\n",
    "mat =\n",
    "[[231, 252, 419]\n",
    " [434, 593, 321]]\n",
    "(예시를 위한 랜덤값입니다)\n",
    "```\n",
    "\n",
    "각 행마다 softmax를 적용해서 각 행의 합을 1로 만들 수도 있고 (이렇게 하면 행렬 원소의 총 합은 2가 됩니다) 각 열마다 softmax를 적용해서 각 열의 합을 1으로 만들 수도 있습니다 (이렇게 하면 행렬 원소의 총 합은 3이 됩니다).\n",
    "\n",
    "각 행마다 softmax를 적용하고 싶으면 `torch.softmax(mat, 1)`라고 코딩하면 되고, 각 열마다 softmax를 적용하고 싶으면 `torch.softmax(mat, 0)` 이라고 코딩하면 됩니다. 마지막 인덱스를 `-1`로 쓸 수 있으므로, `1`대신 `-1`이라고 해도 됩니다.\n",
    "\n",
    "**Challenge**\n",
    "- 3차원 이상의 텐서의 경우 어떤 식으로 해야할지, 잘 생각해보시고, 구현하여 본인 생각이 맞는지 확인해보세요.\n",
    "- https://medium.com/analytics-vidhya/an-intuitive-understanding-on-tensor-sum-dimension-with-pytorch-d9b0b6ebbae\n",
    "- https://jamesmccaffrey.wordpress.com/2020/07/09/understanding-the-dim-parameter-in-pytorch-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 네트워크 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 `nn.Module`을 상속하여 PyTorch의 신경망을 클래스로 정의할 수 있습니다. 네트워크 구조가 어떻게 코드로 구현되는지 잘 살펴보세요.\n",
    "\n",
    "**Quiz**\n",
    "- `nn.Linear`는 몇 개의 parameter를 가지고 있을까요?\n",
    "- 입력 텐서의 shape가 (x, x, x) 일 때, `nn.Linear(x, y)`를 거치면 어떻게 될까요? (직접 해보세요)\n",
    "- `nn.LogSoftmax`는 몇 개의 parameter를 가지고 있을까요?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # 고치다 귀찮아진 문제발생\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def letter2tensor(letter):\n",
    "    res = []\n",
    "\n",
    "    for char in all_letters:\n",
    "        if char == letter:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return idx2tensor(all_letters.find(letter), len(all_letters))\n",
    "    return torch.tensor([res])\n",
    "\n",
    "def idx2tensor(idx, N):\n",
    "    res = []\n",
    "\n",
    "    for i in range(N):\n",
    "        if i == idx:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "\n",
    "    return torch.tensor(res)\n",
    "\n",
    "def word2tensor(word):\n",
    "    res = torch.zeros(len(word), 1, len(all_letters))\n",
    "\n",
    "    for idx, char in enumerate(word):\n",
    "        res[idx] = letter2tensor(char)\n",
    "\n",
    "    return res.squeeze(dim = 1)\n",
    "\n",
    "def prepare_data(batch_size = 32):\n",
    "    files = glob.glob('names/*.txt')\n",
    "    assert len(files) == 18\n",
    "    category_names = {}\n",
    "    all_categories = []\n",
    "\n",
    "    n_letters = len(all_letters)\n",
    "    assert n_letters == 26\n",
    "\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            names = f.read().strip().split('\\n')\n",
    "\n",
    "        lang = file.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(lang)\n",
    "\n",
    "        names = [n.lower() for n in names] # Make everything lowercases\n",
    "        names = [''.join([c for c in n if c in all_letters]) for n in names] # Ignore non-alphabet letters\n",
    "        category_names[lang] = names # {언어 : 이름}\n",
    "        # print(f'{lang}: {len(names)} |', names[0], names[1], names[2])\n",
    "\n",
    "    n_categories = len(all_categories)\n",
    "    # print(\"n_categories : \", n_categories) # 18\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for lang, names in category_names.items():\n",
    "        for name in names:\n",
    "            x.append(word2tensor(name)) # word_length (= len(name)) X number_of_characters (= len(all_letters))\n",
    "            y.append(idx2tensor(all_categories.index(lang), len(all_categories))) # number_of_languages (= len(all_categories))\n",
    "\n",
    "    # dataset = TensorDataset(torch.stack(x), torch.stack(y))\n",
    "    # dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return zip(x, y)\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 0)\n",
    "        self.optimizer = optim.Adam\n",
    "        self.loss = torch.nn.NLLLoss()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "        output = self.h2o(hidden)\n",
    "        # print('after h2o', output.shape)\n",
    "        output = self.softmax(output) # (output_size, character_no )\n",
    "        print('after softmax', output.shape)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.hidden_size)\n",
    "\n",
    "    def train_model(self, train_data, learning_rate = 0.001, epochs = 20):\n",
    "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_data:\n",
    "                hidden = self.initHidden()\n",
    "\n",
    "                for char in x:\n",
    "                    output, hidden = self(char, hidden)\n",
    "                    # print(output.shape)\n",
    "\n",
    "                # print(output.shape, y.shape, x.shape)\n",
    "\n",
    "                loss = self.loss(output, y)\n",
    "                loss_history.append(torch.log(torch.mean(loss)).item())\n",
    "                # print(torch.mean(loss).item())\n",
    "\n",
    "                if len(loss_history) % 1000 == 0:\n",
    "                    print(torch.mean(loss).item())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "def predict_nationality(model, word):\n",
    "    hidden = model.initHidden()\n",
    "\n",
    "    for char in word:\n",
    "        letter_tensor = letterToTensor(char)\n",
    "        output, hidden = rnn(letter_tensor, hidden)\n",
    "    print(output.shape)\n",
    "    print(torch.argmax(output))\n",
    "\n",
    "n_hidden = 32\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "predict_nationality(rnn, 'ang')\n",
    "dataset = prepare_data()\n",
    "rnn.train_model(dataset)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def letter2tensor(letter): # 원 핫 인코딩 -> tensor 변환 #letter = 이름 1개\n",
    "  res = [0]\n",
    "\n",
    "  for char in all_letters:\n",
    "    if char == letter:\n",
    "      res.append(1)\n",
    "    else:\n",
    "      res.append(0)\n",
    "  assert len(res) == len(all_letters) + 1\n",
    "  assert res[0] == 0\n",
    "  return torch.tensor(res)\n",
    "\n",
    "# y.append(idx2tensor(all_categories.index(lang),n_categories))\n",
    "# all_categories : 언어\n",
    "# n_categories : len(all_categories)\n",
    "def idx2tensor(idx, N): # 인덱스 -> tensor 변환 # (언어의 인덱스, 언어의 총 갯수)\n",
    "  res = []\n",
    "\n",
    "  for i in range(N):\n",
    "    if i == idx:\n",
    "      res.append(1)\n",
    "    else:\n",
    "      res.append(0)\n",
    "  return torch.tensor(res)\n",
    "\n",
    "def word2tensor(word, max_length = 10): # word -> tensor 변환\n",
    "  res = torch.zeros(max_length, len(all_letters) + 1, dtype = torch.float32)\n",
    "\n",
    "  for idx, char in enumerate(word): # word = name\n",
    "    res[idx] = letter2tensor(char)\n",
    "  # res: len(word), len(all_letters)\n",
    "  for idx in range(max_length - len(word)):\n",
    "    res[len(word) + idx] = torch.tensor([1] + [0 for _ in range(len(all_letters))])\n",
    "  return res\n",
    "\n",
    "# step 1. 데이터 준비\n",
    "def prepare_data(batch_size = 32):\n",
    "  files = glob.glob('/content/drive/MyDrive/Colab Notebooks/datascience/새싹_RNN/names/*.txt')\n",
    "  # assert len(files) == 18\n",
    "\n",
    "  category_names = {}\n",
    "  all_categories = []\n",
    "\n",
    "  n_letters = len(all_letters)\n",
    "  assert n_letters == 26\n",
    "\n",
    "  for file in files:\n",
    "    with open(file) as f:\n",
    "      names = f.read().strip().split('\\n')\n",
    "\n",
    "    lang = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    if lang in ['Korean', 'Czech']:\n",
    "      all_categories.append(lang)\n",
    "\n",
    "      names = [n.lower() for n in names[:10]]\n",
    "      names = [''.join([c for c in n if c in all_letters]) for n in names]\n",
    "      category_names[lang] = names # {언어 : 이름}\n",
    "\n",
    "  print(\"all_categories :\", all_categories)\n",
    "  n_categories = len(all_categories)\n",
    "  print(\"n_categories : \", n_categories) # 18\n",
    "  for lang, names in category_names.items():\n",
    "      print(len(names))\n",
    "  x = []\n",
    "  y = []\n",
    "  max_length = 0\n",
    "\n",
    "  for lang, names in category_names.items():\n",
    "    for name in names:\n",
    "      if len(name) > max_length:\n",
    "        max_length = len(name)\n",
    "\n",
    "  for lang, names in category_names.items():\n",
    "    for name in names:\n",
    "      # x.append(list(name))\n",
    "      x.append(word2tensor(name, max_length = max_length)) # word2tensor(name): len(word), len(all_letters)\n",
    "      y.append(all_categories.index(lang))\n",
    "\n",
    "  # print('len(x) : ', len(x)) # == 20074\n",
    "  # print('len(y) : ', len(y)) # == 20074\n",
    "\n",
    "  x_array =  np.array(x)\n",
    "  y_array =  np.array(y)\n",
    "  # x_tensor: len(names), max_name_len, len(all_letters)\n",
    "  X_tensor = torch.tensor(x_array, dtype = torch.float32)\n",
    "  y_tensor = torch.tensor(y_array, dtype = torch.long)\n",
    "\n",
    "  print(\"X_tensor.shape() : \", X_tensor.size())\n",
    "  print(\"y_tensor.shape() : \", y_tensor.size())\n",
    "  dataset = TensorDataset(X_tensor,y_tensor)\n",
    "  dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "  return dataloader, all_categories\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "  plt.plot(range(1, len(loss_history)+1), loss_history)\n",
    "  plt.show()\n",
    "\n",
    "# rnn = RNN(n_letters, n_hidden, n_categories) # 26, 32, 18\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size): # 26, 32, 18\n",
    "    super(RNN, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    self.i2h = nn.Linear(input_size, hidden_size)\n",
    "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "    self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    self.optimizer = optim.Adam\n",
    "    self.loss = torch.nn.NLLLoss()\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    # input: (batch_size, input_size)\n",
    "    # hidden: (hidden_size, )\n",
    "\n",
    "    # self.i2h(input): (batch_size, hidden_size)\n",
    "    # self.h2h(hidden): (1, hidden, )\n",
    "    # hidden: F.tanh( (batch_size, hidden_size) + (1, hidden_size,)): (batch_size, hidden_size)\n",
    "    batch_size, input_size = input.size()\n",
    "    assert input_size == self.input_size\n",
    "    assert hidden.size(1) == self.hidden_size\n",
    "\n",
    "    hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "    output = self.h2o(hidden) # output: (batch_size, output_size)\n",
    "    output = self.softmax(output) # output: (batch_size, output_size)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "  def train_model(self, train_data, learning_rate = 0.001, epochs = 20):\n",
    "    optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for idx, (x, y) in enumerate(train_data):\n",
    "        hidden = self.initHidden()\n",
    "        # for char in x:\n",
    "        #   output, hidden = self(char, hidden)\n",
    "        #   #print(output)\n",
    "\n",
    "        for i in range(x.size(1)):\n",
    "          output, hidden = self(x[:,i], hidden)\n",
    "\n",
    "        # loss = self.loss(output.view(-1, n_categories), y)\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "          loss_history.append(loss.item())\n",
    "        #print(torch.mean(loss).item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n",
    "    plot_loss_history(loss_history)\n",
    "    return loss_history\n",
    "\n",
    "def predict_nationality(model, word, all_categories): # predict_nationality(rnn, 'ang')\n",
    "  hidden = model.initHidden()\n",
    "\n",
    "  word = word2tensor(word) # res\n",
    "  output, hidden = model(word, hidden)\n",
    "  print(\"output :\", output)\n",
    "  print('output.shape : ', output.shape)\n",
    "  print('output.argmax : ', torch.argmax(output[1]))\n",
    "  print(\"lang : \", all_categories[torch.argmax(output[1]).item()])\n",
    "\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "n_letters = len(all_letters) + 1\n",
    "n_hidden = 32\n",
    "n_categories = 18\n",
    "\n",
    "# 데이터 준비 및 모델 학습\n",
    "dataset, all_categories  = prepare_data()\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_categories) # 26, 32, 18\n",
    "rnn.train_model(dataset)\n",
    "# learning_rate_lst = [0.001, 0.0001, 0.0001] # 0.001이 loss가 가장 작음\n",
    "# for i in (learning_rate_lst):\n",
    "#   print(f\"learning_rate : {i}\\n\")\n",
    "#   rnn.train_model(dataset,learning_rate = i, epochs = 20)\n",
    "\n",
    "predict_nationality(rnn, 'ang', all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고고\n",
    "def test_accuracy(all_categories, category_names):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for lang, names in category_names.items():\n",
    "    # print(len(names))\n",
    "    for i in range(len(names)):\n",
    "      try:\n",
    "        pred = predict_nationality(rnn, names[i], all_categories, max_length)\n",
    "        if pred == lang:\n",
    "          correct += 1\n",
    "        total += 1\n",
    "      except IndexError:\n",
    "        continue\n",
    "\n",
    "  print('correct : ', correct)\n",
    "  print('total : ', total)\n",
    "  return correct / total\n",
    "\n",
    "test_accuracy(all_categories, category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def letter2tensor(letter): # 원 핫 인코딩 -> tensor 변환 #letter = 이름 1개\n",
    "  res = [0]\n",
    "\n",
    "  for char in all_letters:\n",
    "    if char == letter:\n",
    "      res.append(1)\n",
    "    else:\n",
    "      res.append(0)\n",
    "  assert len(res) == len(all_letters) + 1\n",
    "  assert res[0] == 0\n",
    "  return torch.tensor(res)\n",
    "\n",
    "# y.append(idx2tensor(all_categories.index(lang),n_categories))\n",
    "# all_categories : 언어\n",
    "# n_categories : len(all_categories)\n",
    "def idx2tensor(idx, N): # 인덱스 -> tensor 변환 # (언어의 인덱스, 언어의 총 갯수)\n",
    "  res = []\n",
    "\n",
    "  for i in range(N):\n",
    "    if i == idx:\n",
    "      res.append(1)\n",
    "    else:\n",
    "      res.append(0)\n",
    "  return torch.tensor(res)\n",
    "\n",
    "def word2tensor(word, max_length = 10): # word -> tensor 변환\n",
    "  res = torch.zeros(max_length, len(all_letters) + 1, dtype = torch.float32)\n",
    "\n",
    "  for idx, char in enumerate(word): # word = name\n",
    "      res[idx] = letter2tensor(char)\n",
    "  # res: len(word), len(all_letters)\n",
    "  for idx in range(max_length - len(word)):\n",
    "    res[len(word) + idx] = torch.tensor([1] + [0 for _ in range(len(all_letters))])\n",
    "  return res\n",
    "\n",
    "# step 1. 데이터 준비\n",
    "def prepare_data(all_letters, batch_size = 32):\n",
    "  files = glob.glob('/content/drive/MyDrive/Colab Notebooks/datascience/새싹_RNN/names/*.txt')\n",
    "  assert len(files) == 18\n",
    "\n",
    "  category_names = {}\n",
    "  all_categories = []\n",
    "\n",
    "  n_letters = len(all_letters)\n",
    "  assert n_letters == 26\n",
    "\n",
    "  for file in files:\n",
    "    with open(file) as f:\n",
    "      names = f.read().strip().split('\\n')\n",
    "\n",
    "    lang = file.split('/')[-1].split('.')[0]\n",
    "\n",
    "    # if lang in ['Korean', 'Czech']:\n",
    "    all_categories.append(lang)\n",
    "\n",
    "    names = [n.lower() for n in names]\n",
    "    names = [''.join([c for c in n if c in all_letters]) for n in names]\n",
    "    category_names[lang] = names # {언어 : 이름}\n",
    "\n",
    "  print(\"all_categories :\", all_categories)\n",
    "  n_categories = len(all_categories)\n",
    "  print(\"n_categories : \", n_categories) # 18\n",
    "  # for lang, names in category_names.items():\n",
    "  #     # print(len(names))\n",
    "  x = []\n",
    "  y = []\n",
    "  max_length = 0\n",
    "\n",
    "  for lang, names in category_names.items():\n",
    "    for name in names:\n",
    "      if len(name) > max_length:\n",
    "        max_length = len(name)\n",
    "        print(max_length, name)\n",
    "  print(max_length)\n",
    "  for lang, names in category_names.items():\n",
    "    for name in names:\n",
    "      # x.append(list(name))\n",
    "      x.append(word2tensor(name, max_length = max_length)) # word2tensor(name): len(word), len(all_letters)\n",
    "      y.append(all_categories.index(lang))\n",
    "\n",
    "  # print('len(x) : ', len(x)) # == 20074\n",
    "  # print('len(y) : ', len(y)) # == 20074\n",
    "\n",
    "  x_array =  np.array(x)\n",
    "  y_array =  np.array(y)\n",
    "  # x_tensor: len(names), max_name_len, len(all_letters)\n",
    "  X_tensor = torch.tensor(x_array, dtype = torch.float32)\n",
    "  y_tensor = torch.tensor(y_array, dtype = torch.long)\n",
    "\n",
    "  print(\"X_tensor.shape() : \", X_tensor.size())\n",
    "  print(\"y_tensor.shape() : \", y_tensor.size())\n",
    "  dataset = TensorDataset(X_tensor,y_tensor)\n",
    "  dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "  return dataloader, all_categories, category_names, max_length\n",
    "\n",
    "def plot_loss_history(loss_history):\n",
    "  plt.plot(range(1, len(loss_history)+1), loss_history)\n",
    "  plt.show()\n",
    "\n",
    "# rnn = RNN(n_letters, n_hidden, n_categories) # 26, 32, 18\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size): # 26, 32, 18\n",
    "    super(RNN, self).__init__()\n",
    "\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "\n",
    "    self.i2h = nn.Linear(input_size, hidden_size)\n",
    "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "    self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    # self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    self.optimizer = optim.Adam\n",
    "    # self.loss = torch.nn.NLLLoss()\n",
    "    self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    # input: (batch_size, input_size)\n",
    "    # hidden: (hidden_size, )\n",
    "\n",
    "    # self.i2h(input): (batch_size, hidden_size)\n",
    "    # self.h2h(hidden): (1, hidden, )\n",
    "    # hidden: F.tanh( (batch_size, hidden_size) + (1, hidden_size,)): (batch_size, hidden_size)\n",
    "    batch_size, input_size = input.size()\n",
    "    assert input_size == self.input_size\n",
    "    assert hidden.size(1) == self.hidden_size\n",
    "\n",
    "    hidden = F.tanh(self.i2h(input) + self.h2h(hidden))\n",
    "    output = self.h2o(hidden) # output: (batch_size, output_size)\n",
    "    # output = self.softmax(output) # output: (batch_size, output_size)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "  def train_model(self, train_data, learning_rate = 0.001, epochs = 100):\n",
    "    optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "    loss_history = []\n",
    "\n",
    "    save_loss = 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for idx, (x, y) in enumerate(train_data):\n",
    "        hidden = self.initHidden()\n",
    "        # for char in x:\n",
    "        #   output, hidden = self(char, hidden)\n",
    "        #   #print(output)\n",
    "\n",
    "        for i in range(x.size(1)):\n",
    "          output, hidden = self(x[:,i], hidden)\n",
    "\n",
    "        # loss = self.loss(output.view(-1, n_categories), y)\n",
    "        loss = self.loss(output, y)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "          loss_history.append(loss.item())\n",
    "        #print(torch.mean(loss).item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "      print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "      if  save_loss > loss.item():\n",
    "        save_loss = loss.item()\n",
    "        torch.save(self.state_dict(), 'model.pth')\n",
    "\n",
    "    plot_loss_history(loss_history)\n",
    "    print(\"save_loss : \", save_loss)\n",
    "    return loss_history\n",
    "\n",
    "def predict_nationality(model, word, all_categories, max_length): # predict_nationality(rnn, 'ang')\n",
    "  hidden = model.initHidden()\n",
    "\n",
    "  word = word2tensor(word, max_length) # res\n",
    "  output, hidden = model(word, hidden)\n",
    "  # print(\"output :\", output)\n",
    "  # print('output.shape : ', output.shape)\n",
    "  # print('output.argmax : ', torch.argmax(output[1]))\n",
    "  # print(\"lang : \", all_categories[torch.argmax(output[1]).item()])\n",
    "  return all_categories[torch.argmax(output[1]).item()]\n",
    "\n",
    "all_letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "n_letters = len(all_letters) + 1\n",
    "n_hidden = 128\n",
    "n_categories = 18\n",
    "\n",
    "# 데이터 준비 및 모델 학습\n",
    "dataset, all_categories, category_names, max_length = prepare_data(all_letters)\n",
    "\n",
    "rnn = RNN(n_letters, n_hidden, n_categories) # 26, 32, 18\n",
    "rnn.train_model(dataset)\n",
    "# learning_rate_lst = [0.001, 0.0001, 0.0001] # 0.001이 loss가 가장 작음\n",
    "# for i in (learning_rate_lst):\n",
    "#   print(f\"learning_rate : {i}\\n\")\n",
    "#   rnn.train_model(dataset,learning_rate = i, epochs = 20)\n",
    "\n",
    "predict_nationality(rnn, 'ang', all_categories, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고\n",
    "def test_accuracy(all_categories, category_names ):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  model = RNN(n_letters, n_hidden, n_categories)  # 모델 인스턴스 생성\n",
    "  model.load_state_dict(torch.load('model.pth'))  # 가중치 로드\n",
    "  model.eval()  # 모델을 평가 모드로 전환\n",
    "\n",
    "  for lang, names in category_names.items():\n",
    "    # print(len(names))\n",
    "    for i in range(len(names)):\n",
    "      try:\n",
    "        pred = predict_nationality(model, names[i], all_categories, max_length)\n",
    "        if pred == lang:\n",
    "          correct += 1\n",
    "        total += 1\n",
    "      except IndexError:\n",
    "        continue\n",
    "\n",
    "  print('correct : ', correct)\n",
    "  print('total : ', total)\n",
    "  return correct / total\n",
    "\n",
    "accuracy = test_accuracy(all_categories, category_names )\n",
    "print('Accuarcy :', accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**\n",
    "- `nn.Linear`를 PyTorch를 사용하지 말고 구현해보세요.\n",
    "- `nn.LogSoftmax`를 PyTorch를 사용하지 말고 구현해보세요.\n",
    "- `nn.Softmax`를 PyTorch를 사용하지 말고 구현해보세요.\n",
    "- `torch.allclose`등을 사용해 정확히 구현되었는지 결과값을 비교하여 검증하세요. 필요하다면, 레이어 내부에 저장된 weight를 꺼내어 쓰세요.\n",
    "- 이 과정에서 PyTorch의 공식문서를 참조하시면 좋습니다.\n",
    "- (difficult) forward뿐만 아니라 backward도 구현해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN은 출력값을 입력값으로 재사용합니다.\n",
    "\n",
    "이 때, *재사용*이 어떻게 코드로 구현되는지 다음 코드를 읽고 이해하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Inference using RNN\n",
    "\n",
    "input = lineToTensor('jake') # (4, 26)\n",
    "hidden0 = torch.zeros(n_hidden) # (1, n_hidden)\n",
    "\n",
    "with torch.no_grad(): # No training, no gradient\n",
    "    out1, hidden1 = rnn(input[0], hidden0) # 1st character 'j'\n",
    "    out2, hidden2 = rnn(input[1], hidden1) # 2nd character 'a'\n",
    "    out3, hidden3 = rnn(input[2], hidden2) # 3rd character 'k'\n",
    "    out4, hidden4 = rnn(input[3], hidden3) # 4th character 'e'\n",
    "out = out4\n",
    "\n",
    "print(out) # (n_categories)\n",
    "\n",
    "# The output is meaningless because the network is not yet trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 셀의 출력 텐서 `out`을 어떻게 해석해야 할까요?\n",
    "\n",
    "일단 학습되지 않아서 의미없는 예시 값이지만, 분명 18개의 언어 카테고리 각각의 확률을 표시해주는 값이어야 핪니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log softmax는 그냥 softmax 이후에 log를 취한 값입니다.\n",
    "\n",
    "log softmax에서 softmax를 얻고 싶으면? log의 역연산인 exp를 취하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.exp(out))\n",
    "print(torch.sum(torch.exp(out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 학습이 되지 않아 그저 랜덤한 확률값이 출력됩니다. 학습의 목표는 적절한 확률값이 출력되도록 하는 것입니다.\n",
    "\n",
    "확률에 로그를 취하든 취하지 않든 가장 큰 값이 결국 네트워크의 예측값이 됩니다. 다음은 확률이 가장 높은 카테고리를 고르는 헬퍼 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "print(categoryFromOutput(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "면접관: 당신의 장점은?\n",
    "나: 저는 머신러닝 전문가입니다.\n",
    "면접관: 9+10은?\n",
    "나: 3 입니다.\n",
    "면접관: 틀렸네. 전혀 달라. 답은 19일세.\n",
    "나: 16 입니다.\n",
    "면접관: 틀렸네. 답은 19일세.\n",
    "나: 18 입니다.\n",
    "면접관: 틀렸네. 답은 19일세.\n",
    "나: 19 입니다.\n",
    "면접관: 자넨 합격일세.\n",
    "```\n",
    "\n",
    "위는 놀랍게도 실제 뉴럴네트워크의 학습 과정입니다. 다만 아래와 같이 좀 더 정확하게 고칠 수 있습니다.\n",
    "\n",
    "```\n",
    "훈련교관: 9+10은?\n",
    "신경망: 3입니다.\n",
    "훈련교관: 답은 19일세.\n",
    "신경망: 명심하겠습니다.\n",
    "\n",
    "훈련교관: 8+45는?\n",
    "신경망: 20입니다.\n",
    "훈련교관: 답은 53일세.\n",
    "신경망: 명심하겠습니다.\n",
    "\n",
    "훈련교관: 10+15는? (1. 학습 데이터 샘플링)\n",
    "신경망: 78입니다. (2. 추론)\n",
    "훈련교관: 답은 25일세. (3. 정답과 비교해서 손실함수 계산)\n",
    "신경망: 명심하겠습니다. (4. 손실함수에서 역전파된 그래디언트를 바탕으로 파라미터 조정)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training\n",
    "\n",
    "import random\n",
    "\n",
    "def randomTrainingExample():\n",
    "    category = random.sample(all_categories, 1)[0]\n",
    "    line = random.sample((category_names[category]), 1)[0]\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "# Show examples\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터 랜덤 샘플링 단계와 추론 단계만으로 구성된 불완전한 트레이닝 코드는 아래와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(rnn, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    return output\n",
    "\n",
    "for step in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = infer(rnn, line_tensor)\n",
    "    print(line, category, categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 손실 함수 계산 단계를 더하기 전에, 손실 함수 자체에 대해 좀 더 알아보겠습니다.\n",
    "\n",
    "본 실습에서는 손실함수로 `torch.nn.NLLLoss`를 사용합니다. 이 함수는 기본적으로 다음과 같이 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.NLLLoss()\n",
    "aaa = torch.tensor([[11, 22, 33, 44, 55]], dtype=torch.float32)\n",
    "print(loss_fn(aaa, torch.tensor([0])))\n",
    "print(loss_fn(aaa, torch.tensor([1])))\n",
    "print(loss_fn(aaa, torch.tensor([2])))\n",
    "print(loss_fn(aaa, torch.tensor([3])))\n",
    "print(loss_fn(aaa, torch.tensor([4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 셀에서 `aaa`는 임의의 텐서입니다. `torch.nn.NLLLoss`는 텐서에서 특정 인덱스에 해당하는 값을 뽑아 -1을 곱한 후 돌려줍니다. 복잡한 수식 계산을 하지는 않습니다.\n",
    "\n",
    "**Challenge**\n",
    "- `torch.nn.NLLLoss`를 직접 구현하세요. 만약 forward와 backward를 모두 구현한다면 학습에도 사용할 수 있을 것입니다.\n",
    "\n",
    "저희가 구현한 RNN의 출력값은 log softmax라는 의미를 지닙니다. 만약 log softmax가 아니라 그냥 softmax였다면, 정답 카테고리의 출력값은 1, 나머지는 0이 되는 것이 이상적입니다. 그러나 log를 적용하면, 정답 카테코리의 출력값은 0, 나머지는 -inf가 되는 것이 이상적입니다. 이번 학습에서는 정답 카테고리의 출력값만 고려하겠습니다.\n",
    "\n",
    "예시를 들어보겠습니다. 만약 RNN의 출력값이 다음과 같고, 정답 카테고리의 인덱스는 0이라고 합시다.\n",
    "\n",
    "```\n",
    "tensor([[-2.9579, -2.7449, -2.9624, -2.7420, -2.8171, -3.0771, -2.7278, -3.0051,\n",
    "         -3.0478, -2.9685, -2.5545, -2.8554, -3.2244, -3.1169, -2.5231, -2.8236,\n",
    "         -3.1662, -3.0583]])\n",
    "```\n",
    "\n",
    "그렇다면 저희가 원하는 것은 0번째 인덱스이 값인 `-2.9579`가 0에 가까워지는 것입니다. 다른 말로 하면, -1을 곱한 값인 `2.9579`가 낮아지는 것입니다.\n",
    "\n",
    "위와 같은 점을 고려하여 `torch.nn.NLLLoss`를 사용하여 손실함수 계산까지 구현하면 다음과 같습니다.\n",
    "\n",
    "1. 학습 데이터 샘플링 (구현함)\n",
    "2. 추론 (구현함)\n",
    "3. 정답과 비교하여 손실함수 계산 (구현함)\n",
    "4. 파라미터 조정 (아직 구현안함)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "for step in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = infer(rnn, line_tensor)\n",
    "    output = torch.unsqueeze(output, dim = 0)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 잘 된다면 이렇게 계산한 손실함수가 학습과정중에 점점 낮아져야 합니다.\n",
    "\n",
    "이제 모든 단계를 포함하여 학습 코드를 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "optim = torch.optim.Adam(rnn.parameters(), 1e-3)\n",
    "\n",
    "for step in range(10000):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = infer(rnn, line_tensor)\n",
    "    output = torch.unsqueeze(output, dim = 0)\n",
    "    loss = criterion(output, category_tensor)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(step, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**\n",
    "- 배치 크기를 키운 학습을 구현하세요.\n",
    "- 데이터를 학습 데이터와 테스트 데이터로 구분하고 epoch을 나눠 구현하세요.\n",
    "- learning rate, hidden layer size 등을 조절하며 더 최적화해 보세요.\n",
    "- vanishing/exploding gradient 현상이 일어나고 있는 것은 아닌지 조사하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "cnt_correct = 0\n",
    "\n",
    "for step in range(1000):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    with torch.no_grad():\n",
    "        output = infer(rnn, line_tensor)\n",
    "    cnt += 1\n",
    "    cnt_correct += 1 if category == categoryFromOutput(output)[0] else 0\n",
    "\n",
    "print(f'Accuracy {cnt_correct}/{cnt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 내용을 모두 합쳐서 하나의 코드로 작성한 후, 위 코드를 여러 방향으로 개선해 봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 중간실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순방향 신경망(Feedforward Neural Network) -> 다층 퍼셉트론(MLP: Multi-Layer Perceptron)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import string\n",
    "\n",
    "unknown_char,padding = '?','_'\n",
    "all_letters = string.ascii_lowercase + unknown_char + padding\n",
    "category_names,all_categories = {},[]\n",
    "max_word_length,batch_size = 10,32\n",
    "len_all_categories=0\n",
    "test_size,val_size = 0.2, 0.1\n",
    "\n",
    "def activate_config(unknown_char='?', padding='_', max_word_length=10, batch_size=32, test_size=0.2, val_size=0.1):\n",
    "    unknown_char,padding = unknown_char,padding\n",
    "    all_letters = string.ascii_lowercase + unknown_char + padding\n",
    "    max_word_length,batch_size = max_word_length,batch_size\n",
    "    test_size=test_size\n",
    "    val_size=val_size\n",
    "\n",
    "def category_to_tensor(idx, N):\n",
    "    return torch.tensor([(i==idx)/1 for i in range(N)])\n",
    "\n",
    "def input_padding_and_unknown_char_handling(word):\n",
    "    word = ''.join(char if char in string.ascii_lowercase else unknown_char for char in word.lower())\n",
    "    return word + '_'*(10 - len(word)) if len(word)!=10 else word\n",
    "\n",
    "def word_to_tensor(word):\n",
    "    word = input_padding_and_unknown_char_handling(word)\n",
    "    res = torch.zeros(len(word), len(all_letters))\n",
    "    for idx, letter in enumerate(word):\n",
    "        res[idx][all_letters.find(letter)] = 1\n",
    "    # print(word, res, res.shape)\n",
    "    return res #.squeeze(dim = 1) # shape(10,28)    \n",
    "\n",
    "def preprocessing_data(batch_size = 32, test_size=0.2, val_size=0.1):\n",
    "    files = glob.glob('./data/raw/names/*.txt')\n",
    "    # print(f'{len(files)}fiels',*files,sep='\\n')\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            words = f.read().strip().split('\\n')\n",
    "        category = file.split('\\\\')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "\n",
    "        words = [word for word in words if len(word) <= max_word_length]\n",
    "        category_names[category] = words\n",
    "        # print(f'{category}: {len(words)} |', *words[0:10])\n",
    "    # print(all_categories)\n",
    "    # print(category_names)\n",
    "\n",
    "    \n",
    "    for category, words in category_names.items():\n",
    "        x = [word_to_tensor(word) for word in words]\n",
    "        y = [torch.tensor(all_categories.index(category)) for word in words]\n",
    "    \n",
    "    x = torch.stack(x)\n",
    "    y = torch.stack(y)\n",
    "\n",
    "    x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=(test_size + val_size))\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=(test_size / (test_size + val_size)))\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def plot_loss_history(loss_history, val_loss_history=None): # 훈련손실, 검증손실\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history, label='Training Loss', color='blue', marker='o')\n",
    "    if val_loss_history is not None: plt.plot(range(1, len(val_loss_history) + 1), val_loss_history, label='Validation Loss', color='orange', marker='x')\n",
    "    plt.title('Loss History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xticks(range(1, len(loss_history) + 1)) # x축 눈금 설정\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.hidden_output = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim = 1) # 출력값을 확률분포로 변환\n",
    "        self.optimizer = optim.Adam # 다양한 이동 평균(학습률)을 적용하여 손실함수 최적화 # 모델의 파라미터(가중치)를 업데이트하여 손실을 최소화하는 역할\n",
    "        self.loss = torch.nn.NLLLoss() # 모델의 예측과 실제 값 간의 차이를 측정 : 다중 클래스 분류문제에서 모델성능평가에 사용되는 중요 손실함수\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = self.relu(self.input_hidden(input))\n",
    "        hidden = self.relu(self.hidden_hidden(hidden))\n",
    "        output = self.hidden_output(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "    def train_model(self, train_data, learning_rate = 0.001, epochs = 20):\n",
    "        optimizer = self.optimizer(self.parameters(), lr = learning_rate)\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "            \n",
    "            for input, label in train_data:\n",
    "                input = input.reshape(input.size(0), input.size(1)*input.size(2))\n",
    "                label = label.long()  # label을 LongTensor로 변환\n",
    "                # hidden = self.initHidden() # 은닉상태 초기화 # MLP는 시간적인 정보를 처리하지 않아 히든상태의 개념이 없다 무의미하다#RNN은 반복되어 필수\n",
    "                optimizer.zero_grad()\n",
    "                output = self(input)# 전체 입력 처리\n",
    "                # print(output)\n",
    "                \n",
    "                loss = self.loss(output, label)\n",
    "                loss.backward() # 기울기 누적\n",
    "                optimizer.step() # 파라미터 업데이트\n",
    "                \n",
    "                running_loss += loss.item() # 손실기록\n",
    "                \n",
    "                print(torch.mean(loss).item())\n",
    "                print(output.shape)\n",
    "                # predicted = output.argmax(1)  # 가장 높은 확률의 인덱스\n",
    "                predicted = torch.argmax(output, 1)  # 가장 높은 확률의 인덱스\n",
    "                \n",
    "                print(predicted.shape)\n",
    "                print(label.shape)\n",
    "                correct_predictions += (predicted == label).float().sum().item()  # 맞춘 예측 개수\n",
    "                total_predictions += label.size(0)  # 총 예측 개수\n",
    "                print(correct_predictions)\n",
    "                print(total_predictions)\n",
    "                \n",
    "                if len(loss_history)%1000 == 0:\n",
    "                    print(torch.mean(loss).item())\n",
    "            \n",
    "            \n",
    "            epoch_loss = running_loss / len(train_data)\n",
    "            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0 # 정확도 계산\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.5f}, Accuracy: {accuracy:.5f}') # 정확도 출력\n",
    "            loss_history.append(epoch_loss)  # 손실 기록\n",
    "        plot_loss_history(loss_history)\n",
    "        return loss_history\n",
    "\n",
    "def predict_nationality(model, word):\n",
    "    word_tensor = word_to_tensor(word).unsqueeze(0)\n",
    "    print(word_tensor.shape)\n",
    "    \n",
    "    output = model(word_tensor)\n",
    "    _, predicted_index = torch.max(output, 1)\n",
    "    return predicted_index\n",
    "\n",
    "activate_config(unknown_char='?', padding='_',\n",
    "                max_word_length=10, batch_size=32,\n",
    "                test_size=0.2, val_size=0.1)\n",
    "train_loader, val_loader, test_loader = preprocessing_data(batch_size,test_size,val_size)\n",
    "\n",
    "model = MLP(input_size=len(all_letters)*10, hidden_size=32, output_size=len(all_categories))\n",
    "# predict_nationality(model, 'ang')\n",
    "\n",
    "model.train_model(train_loader, learning_rate = 0.001, epochs = 200)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오늘은 지금까지 배운 전반부 딥러닝에서의 내용을 총체적으로 활용하여 문제를 직접 a부터 z까지 해결해보는 시간입니다.\n",
    "\n",
    "오늘 다룰 데이터는 names 안에 있는 각국의 이름을 받아, 이름의 국적을 추론하는 뉴럴 넷을 만드는 것입니다. 방법은 자유이나, 다음의 두 가지 방법을 꼭 포함시켜 구현하고, 성능 측정/튜닝의 과정을 거치세요.\n",
    "\n",
    "1. Feedforward Network(MLP)를 사용하여 해볼 것.\n",
    "  -  Feedforward Network를 이용하여 할 경우, input size가 이름에 따라 다른데 이를 어떤 식으로 다룰지 생각해 볼 것\n",
    "2. RNN을 이용하여 해볼 것.\n",
    "  - torch.DataLoader을 사용할 경우 batching을 하게 되는데, 이 경우 위 1에서 써 있는 문제와 비슷한 문제가 있으며 해결 또한 비슷하게 할 수 있음. torch.DataLoader을 쓸 때와 쓰지 않을 때를 비교해볼 것.\n",
    "  - 어제는 영어 알파벳만 사용하였는데, 다른 알파벳까지 사용할 경우 성능이 올라가는지 내려가는지 관찰해볼 것.\n",
    "  - 데이터에서 관찰하지 못한 알파벳들을 OOV로 처리하는 방식을 시도할 경우 성능이 올라가는지 내려가는지 관찰해볼 것\n",
    "\n",
    "성능 측정을 위해서 train-valid-test를 적절히 나누고, valid에서의 loss와 정확도를 측정하면서 학습을 적절할 때 멈추거나 할 것.\n",
    "\n",
    "튜닝을 위해서는 다음의 방법들을 사용할 수 있습니다.\n",
    "\n",
    "- hyperparameter을 다양하게 시도해볼 것\n",
    "- 뉴럴 넷 구조를 조금씩 바꿔볼 것\n",
    "- 데이터를 더 만들어 볼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(int)\n",
    "\n",
    "for i in range(10):\n",
    "    d[i] += 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def generate_dataset(batch_size = 32):\n",
    "    files = glob.glob('./data/raw/names/*.txt')\n",
    "    assert len(files) == 18\n",
    "\n",
    "    character_dict = defaultdict(int)\n",
    "    name_length_dict = defaultdict(int)\n",
    "    data, languages = [], []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            names = f.read().strip().split('\\n')\n",
    "        lang = file.split('/')[1].split('.')[0]\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages.append(lang)\n",
    "\n",
    "        for name in names:\n",
    "            for char in name:\n",
    "                character_dict[char.lower()] += 1\n",
    "            name_length_dict[len(name)] += 1\n",
    "            data.append([name, lang])\n",
    "\n",
    "    lst = []\n",
    "    for k, v in character_dict.items():\n",
    "        lst.append((k, v))\n",
    "\n",
    "    name_length_lst = []\n",
    "    for k, v in name_length_dict.items():\n",
    "        name_length_lst.append((k, v))\n",
    "\n",
    "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
    "    name_length_lst = sorted(name_length_lst, key = lambda x:x[1], reverse = True)\n",
    "\n",
    "    total_count = sum([e[1] for e in lst])\n",
    "    total_count_name = sum([e[1] for e in name_length_lst])\n",
    "\n",
    "    s = 0\n",
    "    alphabets = []\n",
    "\n",
    "    oov = '[OOV]'\n",
    "    pad = '[PAD]'\n",
    "\n",
    "    for k, v in lst:\n",
    "        s += v\n",
    "        if s < 0.999*total_count:\n",
    "            alphabets.append(k)\n",
    "    s = 0\n",
    "    max_length_candidate = []\n",
    "    for k, v in name_length_lst:\n",
    "        s += v\n",
    "        if s < 0.999*total_count_name:\n",
    "            max_length_candidate.append(k)\n",
    "\n",
    "    alphabets.append(oov)\n",
    "\n",
    "    for elem in data:\n",
    "        tmp = []\n",
    "        for char in data[0]:\n",
    "            if char in alphabets:\n",
    "                tmp.append(char)\n",
    "            else:\n",
    "                tmp.append(oov)\n",
    "        data[0] = word2tensor(tmp)\n",
    "\n",
    "    x = torch.stack([e[0] for e in data])\n",
    "    y = torch.stack([torch.tensor(e[1]) for e in data])\n",
    "\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# now the data is list of[list of character, lang_name]\n",
    "\n",
    "alphabets = 'abcdefghijklmnopqrstuvwxzy'\n",
    "max_length = 17\n",
    "\n",
    "\n",
    "\n",
    "dataset = generate_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "OOV = '[OOV]'\n",
    "PAD = '[PAD]'\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "def letter2tensor(letter, alphabets, oov = OOV):\n",
    "    res = [0 for _ in range(len(alphabets))]\n",
    "\n",
    "    if letter in alphabets:\n",
    "        idx = alphabets.index(letter)\n",
    "    else:\n",
    "        idx = alphabets.index(oov)\n",
    "\n",
    "    res[idx] = 1\n",
    "\n",
    "    return torch.tensor(res)\n",
    "\n",
    "def word2tensor(word, max_length, alphabets, pad = PAD, oov = OOV):\n",
    "    # return torch.tensor with size (max_length, len(alphabets))\n",
    "    res = torch.zeros(max_length, len(alphabets))\n",
    "\n",
    "    for idx, char in enumerate(word):\n",
    "        if idx < max_length:\n",
    "            res[idx] = letter2tensor(char, alphabets, oov = oov)\n",
    "\n",
    "    for i in range(max_length - len(word)):\n",
    "        res[len(word) + i] = letter2tensor(pad, alphabets, oov = oov)\n",
    "    return res\n",
    "\n",
    "def determine_alphabets(data, pad = PAD, oov = OOV, threshold = 0.999):\n",
    "    # data = list of [name, language_name]\n",
    "    lst = []\n",
    "    character_dict = defaultdict(int)\n",
    "\n",
    "    for name, lang in data:\n",
    "        for char in name:\n",
    "            character_dict[char.lower()] += 1\n",
    "\n",
    "    for k, v in character_dict.items():\n",
    "        lst.append((k, v))\n",
    "\n",
    "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
    "    total_count = sum([e[1] for e in lst])\n",
    "    s = 0\n",
    "\n",
    "    alphabets = []\n",
    "\n",
    "    for k, v in lst:\n",
    "        s += v\n",
    "        if s < threshold * total_count:\n",
    "            alphabets.append(k)\n",
    "\n",
    "    alphabets.append(pad)\n",
    "    alphabets.append(oov)\n",
    "\n",
    "    return alphabets\n",
    "\n",
    "def determine_max_length(data, threshold = 0.99):\n",
    "    lst = []\n",
    "    name_length_dict = defaultdict(int)\n",
    "\n",
    "    for name, lang in data:\n",
    "         name_length_dict[len(name)] += 1\n",
    "\n",
    "    for k, v in name_length_dict.items():\n",
    "        lst.append((k, v))\n",
    "\n",
    "    lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
    "    total_count = sum([e[1] for e in lst])\n",
    "    s = 0\n",
    "\n",
    "    for k, v in lst:\n",
    "        s += v\n",
    "        if s > threshold * total_count:\n",
    "            return k - 1\n",
    "    # if not, return the maximum value in lst\n",
    "    return max(lst, key = lambda x:x[0])[0]\n",
    "\n",
    "def load_file(): \n",
    "    files = glob.glob('./data/names/*.txt')\n",
    "    \n",
    "    assert len(files) == 18\n",
    "\n",
    "    data = []\n",
    "    languages = []\n",
    "\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            names = f.read().strip().split('\\n')\n",
    "        lang = file.split('/')[1].split('.')[0]\n",
    "\n",
    "        if lang not in languages:\n",
    "            languages.append(lang)\n",
    "\n",
    "        for name in names:\n",
    "            data.append([name, lang])\n",
    "\n",
    "    return data, languages\n",
    "\n",
    "def generate_dataset(batch_size = 32, pad = PAD, oov = OOV):\n",
    "    data, languages = load_file()\n",
    "\n",
    "    alphabets = determine_alphabets(data, pad = pad, oov = oov)\n",
    "    max_length = determine_max_length(data)\n",
    "    print(alphabets, max_length)\n",
    "\n",
    "    for idx, elem in enumerate(data):\n",
    "        tmp = []\n",
    "        for char in elem[0]:\n",
    "            if char.lower() in alphabets:\n",
    "                tmp.append(char.lower())\n",
    "            else:\n",
    "                tmp.append(oov)\n",
    "\n",
    "        data[idx][0] = word2tensor(tmp, max_length, alphabets, pad = pad, oov = oov)\n",
    "        data[idx][1] = languages.index(data[idx][1])\n",
    "\n",
    "    x = [e[0] for e in data]\n",
    "    y = [torch.tensor(e[1]) for e in data]\n",
    "\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = split_train_valid_test(x, y)\n",
    "\n",
    "    train_x = torch.stack(train_x)\n",
    "    train_y = torch.stack(train_y)\n",
    "    valid_x = torch.stack(valid_x)\n",
    "    valid_y = torch.stack(valid_y)\n",
    "    test_x = torch.stack(test_x)\n",
    "    test_y = torch.stack(test_y)\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    valid_dataset = TensorDataset(valid_x, valid_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader, alphabets, max_length, languages\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset, alphabets, max_length, languages  = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataset:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3,4,2)\n",
    "print(t)\n",
    "print(t.reshape(3, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2word(t, alphabets):\n",
    "    # t.shpae: max_length, len(alphabets)\n",
    "    res = []\n",
    "    for char_tensor in t:\n",
    "        char = alphabets[int(torch.argmax(char_tensor).item())]\n",
    "        res.append(char)\n",
    "\n",
    "    return res\n",
    "\n",
    "def idx2lang(idx, languages):\n",
    "    return languages[idx]\n",
    "\n",
    "for batch_x, batch_y in dataset:\n",
    "    for i in range(batch_x.size(0)):\n",
    "        print(tensor2word(batch_x[i], alphabets), idx2lang(batch_y[i], languages))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_train_valid_test(train, valid, test):\n",
    "    assert [train, valid, test] != [0, 0, 0]\n",
    "    options = [train, valid, test]\n",
    "\n",
    "    pick = random.choice([0, 1, 2])\n",
    "\n",
    "    while options[pick] == 0:\n",
    "        pick = random.choice([0, 1, 2])\n",
    "    assert options[pick] != 0\n",
    "    return pick\n",
    "\n",
    "print(pick_train_valid_test(0,3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([1.0, 2.0])\n",
    "print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "def modify_dataset_for_ffn(dataset):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for batch_x, batch_y in dataset:\n",
    "        for i in range(batch_x.size(0)):\n",
    "            x.append(batch_x[i].reshape((batch_x.size(1) * batch_x.size(2))))\n",
    "            y.append(batch_y[i])\n",
    "\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = split_train_valid_test(x, y)\n",
    "\n",
    "    train_x = torch.stack(train_x)\n",
    "    train_y = torch.stack(train_y)\n",
    "    valid_x = torch.stack(valid_x)\n",
    "    valid_y = torch.stack(valid_y)\n",
    "    test_x = torch.stack(test_x)\n",
    "    test_y = torch.stack(test_y)\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    valid_dataset = TensorDataset(valid_x, valid_y)\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "def pick_train_valid_test(train, valid, test):\n",
    "    assert [train, valid, test] != [0, 0, 0]\n",
    "    options = [train, valid, test]\n",
    "\n",
    "    pick = random.choice([0, 1, 2])\n",
    "\n",
    "    while options[pick] == 0:\n",
    "        pick = random.choice([0, 1, 2])\n",
    "    assert options[pick] != 0\n",
    "    return pick\n",
    "\n",
    "def split_train_valid_test(x, y, train_valid_test_ratio = (0.7, 0.15, 0.15)):\n",
    "    # TensorDataset -> TensorDataset, TensorDataset, TensorDataset\n",
    "    # x, y: list of data\n",
    "    train_ratio, valid_ratio, test_ratio = train_valid_test_ratio\n",
    "    y_label_dict = defaultdict(int)\n",
    "    for y_data in y:\n",
    "        y_label_dict[y_data.item()] += 1\n",
    "\n",
    "    no_per_labels = {} # y_label별로 각각 train, valid, test\n",
    "\n",
    "    for y_label, freq in y_label_dict.items():\n",
    "        train_size, valid_size, test_size = int(freq * train_ratio), int(freq * valid_ratio), freq - int(freq * train_ratio) - int(freq * valid_ratio)\n",
    "        no_per_labels[y_label] = [train_size, valid_size, test_size]\n",
    "\n",
    "    train_x, train_y = [], []\n",
    "    valid_x, valid_y = [], []\n",
    "    test_x, test_y = [], []\n",
    "\n",
    "    for x_data, y_data in zip(x, y):\n",
    "        idx = pick_train_valid_test(*no_per_labels[y_data.item()])\n",
    "        assert no_per_labels[y_data.item()][idx] > 0\n",
    "        no_per_labels[y_data.item()][idx] -= 1\n",
    "\n",
    "        if idx == 0:\n",
    "            train_x.append(x_data)\n",
    "            train_y.append(y_data)\n",
    "        elif idx == 1:\n",
    "            valid_x.append(x_data)\n",
    "            valid_y.append(y_data)\n",
    "        elif idx == 2:\n",
    "            test_x.append(x_data)\n",
    "            test_y.append(y_data)\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
    "\n",
    "\n",
    "def plot_loss_history(loss_history, other_loss_history = []):\n",
    "    plt.plot(range(1, len(loss_history) + 1), loss_history)\n",
    "    if other_loss_history != []:\n",
    "        plt.plot(range(1, len(other_loss_history) + 1), other_loss_history)\n",
    "    plt.show()\n",
    "\n",
    "# len(alphabets) * max_length * hidden_size + hidden_size * len(languages)\n",
    "# 32 * 12 * 64 + 64 * 18 = 25000\n",
    "#\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(len(alphabets) * max_length, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, len(languages))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, max_length, len(alphabets) : 32, 12, 57) -> (32, 12*57)\n",
    "        output = self.layer1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.layer2(output)\n",
    "        output = F.log_softmax(output, dim = -1)\n",
    "\n",
    "        return output # (batch_size, len(languages) : 32, 18)\n",
    "\n",
    "    def train_model(self, train_data, valid_data, epochs = 100, learning_rate = 0.001, print_every = 1000):\n",
    "        criterion = F.nll_loss\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "        step = 0\n",
    "        train_loss_history = []\n",
    "        valid_loss_history = []\n",
    "\n",
    "        train_log = {}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_data:\n",
    "                step += 1\n",
    "                y_pred = self(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                mean_loss = torch.mean(loss).item()\n",
    "\n",
    "                if step % print_every == 0 or step == 1:\n",
    "                    train_loss_history.append(mean_loss)\n",
    "                    valid_loss, valid_acc = self.evaluate(valid_data)\n",
    "                    valid_loss_history.append(valid_loss)\n",
    "                    print(f'[Epoch {epoch}, Step {step}] train loss: {mean_loss}, valid loss: {valid_loss}, valid_acc: {valid_acc}')\n",
    "                    torch.save(self, f'checkpoints/feedforward_{step}.chkpts')\n",
    "                    print(f'saved model to checkpoints/feedforward_{step}.chkpts')\n",
    "                    train_log[f'checkpoints/feedforward_{step}.chkpts'] = [valid_loss, valid_acc]\n",
    "\n",
    "        pickle.dump(train_log, open('checkpoints/train_log.dict', 'wb+'))\n",
    "\n",
    "        return train_loss_history, valid_loss_history\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        criterion = F.nll_loss\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in data:\n",
    "                y_pred = self(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "                loss_list.append(torch.mean(loss).item())\n",
    "                correct += torch.sum((torch.argmax(y_pred, dim = 1) == y).float())\n",
    "                total += y.size(0)\n",
    "            return sum(loss_list) / len(loss_list), correct / total\n",
    "\n",
    "train_data, valid_data, test_data = modify_dataset_for_ffn(dataset)\n",
    "\n",
    "model = FeedForwardNetwork(32)\n",
    "loss, acc = model.evaluate(train_data)\n",
    "\n",
    "train_loss_history, valid_loss_history = model.train_model(train_data, valid_data)\n",
    "\n",
    "plot_loss_history(train_loss_history, valid_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_best_model():\n",
    "    train_log = pickle.load(open('checkpoints/train_log.dict', 'rb'))\n",
    "    lst = []\n",
    "\n",
    "    for k, v in train_log.items():\n",
    "        lst.append((k, v))\n",
    "\n",
    "    path_to_model = max(lst, key = lambda x:x[1][1])[0]\n",
    "\n",
    "    return torch.load(path_to_model)\n",
    "\n",
    "model = find_best_model()\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_first = True):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.i2h = nn.Linear(len(alphabets), hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, len(languages))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x: (batch_size, max_length, len(alphabets))\n",
    "        hidden = F.tanh(self.i2h(x) + self.h2h(hidden)) # hidden: (batch_size, hidden_size)\n",
    "        if self.batch_first:\n",
    "            output = self.h2o(hidden)\n",
    "            output = F.log_softmax(output, dim = -1)\n",
    "        else:\n",
    "            output = F.log_softmax(self.h2o(hidden), dim = 0)\n",
    "        # output.shape: batch_size, output_size\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.hidden_size)\n",
    "\n",
    "    def train_model(self, train_data, valid_data, epochs = 100, learning_rate = 0.001, print_every = 1000):\n",
    "        criterion = F.nll_loss\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "        step = 0\n",
    "        train_loss_history = []\n",
    "        valid_loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_data:\n",
    "                step += 1\n",
    "                # x: (batch_size, max_length, len(alphabets))\n",
    "                if self.batch_first:\n",
    "                    x = x.transpose(0, 1)\n",
    "                # x: (max_length, batch_size, len(alphabets))\n",
    "                hidden = self.init_hidden()\n",
    "                for char in x:\n",
    "                    output, hidden = self(char, hidden)\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                mean_loss = torch.mean(loss).item()\n",
    "\n",
    "                if step % print_every == 0 or step == 1:\n",
    "                    train_loss_history.append(mean_loss)\n",
    "                    valid_loss, valid_acc = self.evaluate(valid_data)\n",
    "                    valid_loss_history.append(valid_loss)\n",
    "                    print(f'[Epoch {epoch}, Step {step}] train loss: {mean_loss}, valid loss: {valid_loss}, valid_acc: {valid_acc}')\n",
    "\n",
    "        return train_loss_history, valid_loss_history\n",
    "\n",
    "    def evaluate(self, data):\n",
    "        self.eval()\n",
    "        criterion = F.nll_loss\n",
    "\n",
    "        correct, total = 0, 0\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in data:\n",
    "                # x: (batch_size, max_length, len(alphabets))\n",
    "                if self.batch_first:\n",
    "                    x = x.transpose(0, 1)\n",
    "                # x: (max_length, batch_size, len(alphabets))\n",
    "                hidden = self.init_hidden()\n",
    "                for char in x:\n",
    "                    output, hidden = self(char, hidden)\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                loss_list.append(torch.mean(loss).item())\n",
    "                correct += torch.sum((torch.argmax(output, dim = 1) == y).float())\n",
    "                total += y.size(0)\n",
    "            return sum(loss_list) / len(loss_list), correct / total\n",
    "\n",
    "rnn = RecurrentNeuralNetwork(128)\n",
    "train_loss_history, valid_loss_history = rnn.train_model(train_dataset, valid_dataset)\n",
    "\n",
    "plot_loss_history(train_loss_history, valid_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/18HO8pbjFh2Fmu2pBbCt_Pgl1h5KiwfBV?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_dataset(seq_length, num_samples, vocab_size): # vocab_size:\n",
    "    inputs = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
    "    outputs = inputs.clone()\n",
    "    return TensorDataset(inputs, outputs)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_length = input_seq.size() # batch_size, seq_length\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        \n",
    "        for char_idx in range(seq_length):\n",
    "            x_t = nn.functional.one_hot(input_seq[:, char_idx], num_classes = self.linear.in_features).float()\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # self.i2h = nn.Linear(input_size, hidden_size) # input -> hidden\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, target_seq, hidden):\n",
    "        batch_size, seq_len = target_seq.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.output_size)\n",
    "        \n",
    "        for char_idx in range(seq_len):\n",
    "            if char_idx == 0:\n",
    "                previous_y = torch.zeros(batch_size, self.input_size)\n",
    "            else:\n",
    "                y_prev = target_seq[:, char_idx -1]\n",
    "                previous_y = nn.functional.one_hot(y_prev, self.input_size).float()\n",
    "            hidden = self.activation(self.linear1(previous_y) + hidden)\n",
    "            output = self.linear2(hidden)\n",
    "            outputs[:, char_idx, :] = output\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        encoder_hidden = self.encoder(input_seq)\n",
    "        decoder_output = self.decoder(target_seq, encoder_hidden)\n",
    "        return decoder_output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            # inputs.shape - batch_size, sequence_length\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs, targets)\n",
    "            outputs = outputs.view(-1, outputs.size(-1)) # batch_size * seq_size, output_size\n",
    "            targets = targets.view(-1) # batch_size * seq_len\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], loss: {avg_loss}')\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs, targets) # batch_size, seq_length, vocab_size\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim = 2)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq_length = 10\n",
    "    num_samples = 1000\n",
    "    vocab_size = 5  # Including a padding index if needed\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = generate_dataset(seq_length, num_samples, vocab_size)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    encoder = Encoder(input_size = vocab_size, hidden_size = hidden_size)\n",
    "    decoder = Decoder(input_size = vocab_size, hidden_size = hidden_size, output_size = vocab_size)\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "    acc = evaluate_model(model, dataloader, device)\n",
    "    print(f\"Training Accuracy: {acc * 100:.2f}%\\n\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_input, test_target = dataset[0]\n",
    "        test_input = test_input.unsqueeze(0).to(device)\n",
    "        test_target = test_target.unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(test_input, test_target)\n",
    "\n",
    "        predicted = torch.argmax(output, dim = 2)\n",
    "        print(\"Sample Input Sequence:   \", test_input.squeeze().tolist())\n",
    "        print(\"Sample Target Sequence:  \", test_target.squeeze().tolist())\n",
    "        print(\"Predicted Sequence       :  \", predicted.squeeze().tolist())\n",
    "    \n",
    "    for x, y in dataset:\n",
    "        print(x, y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN의 파라미터 수를 계산하는 방법에 대해 설명해 드리겠습니다. 입력층과 은닉층 사이의 파라미터를 중심으로 설명하겠습니다.\n",
    "- RNN 파라미터 구조\n",
    "- RNN의 기본 구조에서 입력층과 은닉층 사이의 파라미터는 다음과 같습니다:\n",
    "    - 입력 가중치 (Wx)\n",
    "    - 은닉 상태 가중치 (Wh)\n",
    "    - 편향 (b)\n",
    "- 파라미터 수 계산\n",
    "- 입력 가중치 (Wx)\n",
    "    - 크기: $ D_h \\times d $\n",
    "      - $ D_h $: 은닉 상태의 크기\n",
    "      - $ d $: 입력 벡터의 차원\n",
    "  - 파라미터 수: $D_h \\times d$\n",
    "- 은닉 상태 가중치 (Wh)\n",
    "    - 크기: $D_h \\times D_h$\n",
    "    - 파라미터 수: $D_h \\times D_h$\n",
    "- 편향 (b)\n",
    "  - 크기: $D_h \\times 1$\n",
    "  - 파라미터 수: $D_h$\n",
    "- 총 파라미터 수\n",
    "  - 총 파라미터 수는 위의 세 가지 파라미터를 합한 것입니다:\n",
    "    - $ (D_h × d)+(D_h × D_h)+Dh(D_h​ × d)+(D_h​ × D_h)+D_h $\n",
    "- 예시 계산\n",
    "  - 입력 벡터의 차원($d$)이 4이고, 은닉 상태의 크기($D_h$)가 5인 경우:\n",
    "    - $ W_x: 5 \\times 4 = 20$\n",
    "    - $ W_h: 5 \\times 5 = 25$\n",
    "    - $ 4b: 5$\n",
    "    - 총 파라미터 수: $20 + 25 + 5 = 50$\n",
    "- 이렇게 RNN의 입력층과 은닉층 사이의 파라미터 수를 계산할 수 있습니다. 이 구조는 시간에 따라 같은 가중치를 공유하므로, 시간 단계가 늘어나도 파라미터 수는 변하지 않습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_dataset(seq_length, num_samples, vocab_size):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset where each input sequence is identical to the output sequence.\n",
    "\n",
    "    Args:\n",
    "        seq_length (int): The length of each sequence.\n",
    "        num_samples (int): The number of samples in the dataset.\n",
    "        vocab_size (int): The size of the vocabulary (number of unique tokens).\n",
    "\n",
    "    Returns:\n",
    "        TensorDataset: A dataset containing input and output sequences.\n",
    "    \"\"\"\n",
    "    # Generate random integers between 1 and vocab_size-1 for input sequences\n",
    "    inputs = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
    "    outputs = inputs.clone()  # Output is the same as input\n",
    "    return TensorDataset(inputs, outputs)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Encoder implemented with nn.Linear layers.\n",
    "\n",
    "    Architecture:\n",
    "        Input -> Linear -> Tanh -> Hidden State\n",
    "        Repeats for each time step.\n",
    "\n",
    "        x_t ----> [Linear] ----> [Tanh] ----> h_t\n",
    "\n",
    "    Model Formula:\n",
    "        h_t = tanh(W_x * x_t + W_h * h_{t-1} + b)\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Size of input features (vocab size).\n",
    "        hidden_size (int): Size of hidden state.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass for the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Input sequence tensor of shape (batch, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Final hidden state tensor of shape (batch, hidden_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_seq.size()\n",
    "        # Initialize hidden state to zeros\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        for t in range(seq_len):\n",
    "            # One-hot encode input tokens\n",
    "            x_t = nn.functional.one_hot(input_seq[:, t], num_classes=self.linear.in_features).float()\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN Decoder implemented with nn.Linear layers.\n",
    "\n",
    "    Architecture:\n",
    "        Input -> Linear -> Tanh -> Hidden State -> Linear -> Output\n",
    "        Repeats for each time step.\n",
    "\n",
    "        y_{t-1} ----> [Linear] ----> [Tanh] ----> h_t ----> [Linear] ----> y_t\n",
    "\n",
    "    Model Formula:\n",
    "        h_t = tanh(W_x * y_{t-1} + W_h * h_{t-1} + b)\n",
    "        y_t = W_o * h_t + b_o\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Size of input features (vocab size).\n",
    "        hidden_size (int): Size of hidden state.\n",
    "        output_size (int): Size of output features (vocab size).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target_seq, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            target_seq (Tensor): Target sequence tensor of shape (batch, seq_len).\n",
    "            hidden (Tensor): Hidden state tensor of shape (batch, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch, seq_len, output_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = target_seq.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.linear2.out_features)\n",
    "        for t in range(seq_len):\n",
    "            # During training, use teacher forcing: input is the actual target token\n",
    "            if t == 0:\n",
    "                # At t=0, use a start-of-sequence token (assuming index 0)\n",
    "                y_t_minus_1 = torch.zeros(batch_size, self.linear1.in_features, device=target_seq.device)\n",
    "            else:\n",
    "                y_prev = target_seq[:, t-1]\n",
    "                y_t_minus_1 = nn.functional.one_hot(y_prev, num_classes=self.linear1.in_features).float()\n",
    "            hidden = self.activation(self.linear1(y_t_minus_1) + hidden)\n",
    "            output = self.linear2(hidden)\n",
    "            outputs[:, t, :] = output\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence-to-Sequence model integrating Encoder and Decoder.\n",
    "\n",
    "    Architecture:\n",
    "        Encoder -> Decoder\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): Encoder module.\n",
    "        decoder (nn.Module): Decoder module.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        \"\"\"\n",
    "        Forward pass for the Seq2Seq model.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Input sequence tensor of shape (batch, seq_len).\n",
    "            target_seq (Tensor): Target sequence tensor of shape (batch, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits of shape (batch, seq_len, output_size).\n",
    "        \"\"\"\n",
    "        encoder_hidden = self.encoder(input_seq)\n",
    "        decoder_output = self.decoder(target_seq, encoder_hidden)\n",
    "        return decoder_output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Trains the Seq2Seq model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Seq2Seq model to train.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimizer for updating model parameters.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to run the training on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(inputs, targets)\n",
    "            # Reshape outputs and targets for loss computation\n",
    "            outputs = outputs.view(-1, outputs.size(-1))  # (batch * seq_len, output_size)\n",
    "            targets = targets.view(-1)  # (batch * seq_len)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs, targets)\n",
    "            # Get predicted tokens\n",
    "            predicted = torch.argmax(outputs, dim=2)\n",
    "            # Compare with targets\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.numel()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Example usage and Training Code\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    seq_length = 10\n",
    "    num_samples = 1000\n",
    "    vocab_size = 50  # Including a padding index if needed\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Generate dataset\n",
    "    dataset = generate_dataset(seq_length, num_samples, vocab_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initialize Encoder and Decoder\n",
    "    encoder = Encoder(input_size=vocab_size, hidden_size=hidden_size)\n",
    "    decoder = Decoder(input_size=vocab_size, hidden_size=hidden_size, output_size=vocab_size)\n",
    "    model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "    print(\"Training completed.\\n\")\n",
    "\n",
    "    # Evaluate accuracy on the training set\n",
    "    accuracy = evaluate_accuracy(model, dataloader, device)\n",
    "    print(f\"Training Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "    # Sample Output after Training\n",
    "    # Let's test the model on a sample input\n",
    "    with torch.no_grad():\n",
    "        test_input, test_target = dataset[0]\n",
    "        test_input = test_input.unsqueeze(0).to(device)  # (1, seq_len)\n",
    "        test_target = test_target.unsqueeze(0).to(device)  # (1, seq_len)\n",
    "        output = model(test_input, test_target)\n",
    "        predicted = torch.argmax(output, dim=2)\n",
    "        print(\"Sample Input Sequence:   \", test_input.squeeze().tolist())\n",
    "        print(\"Sample Target Sequence:  \", test_target.squeeze().tolist())\n",
    "        print(\"Predicted Sequence       :  \", predicted.squeeze().tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq - live class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def generate_dataset(seq_length, num_sample, vocab_size):\n",
    "    inputs = torch.randint(1, vocab_size, (num_sample, seq_length))\n",
    "    outputs = inputs.clone()\n",
    "\n",
    "    return TensorDataset(inputs, outputs)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_length = input_seq.size() # batch_size, seq_elngth\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size).to(device)\n",
    "\n",
    "        for char_idx in range(seq_length):\n",
    "            x_t = nn.functional.one_hot(input_seq[:, char_idx], num_classes = self.linear.in_features).float()\n",
    "            hidden = self.activation(self.linear(x_t) + hidden)\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # self.i2h = nn.Linear(input_size, hidden_size) # input -> hidden\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, target_seq, hidden):\n",
    "        batch_size, seq_len = target_seq.size()\n",
    "        outputs = torch.zeros(batch_size, seq_len, self.output_size).to(device)\n",
    "\n",
    "        for char_idx in range(seq_len):\n",
    "            if char_idx == 0:\n",
    "                previous_y = torch.zeros(batch_size, self.input_size).to(device)\n",
    "            else:\n",
    "                y_prev = target_seq[:, char_idx - 1]\n",
    "                previous_y = nn.functional.one_hot(y_prev, self.input_size).to(device).float()\n",
    "            hidden = self.activation(self.linear1(previous_y) + hidden)\n",
    "            output = self.linear2(hidden)\n",
    "\n",
    "            outputs[:, char_idx, :] = output\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, target_seq):\n",
    "        encoder_hidden = self.encoder(input_seq)\n",
    "        decoder_output = self.decoder(target_seq, encoder_hidden)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            # inputs.shape - batch_size, sequence_length\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs, targets)\n",
    "            outputs = outputs.view(-1, outputs.size(-1)) # batch_size * seq_length, output_size\n",
    "            targets = targets.view(-1) # batch_size * seq_len\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'Epoch {epoch}, loss: {avg_loss}')\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs, targets) # batch_size, seq_length, vocab_size\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim = 2)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0) * targets.size(1)\n",
    "    acc = correct / total\n",
    "    return acc\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq_length = 10\n",
    "    num_samples = 1000\n",
    "    vocab_size = 5  # Including a padding index if needed\n",
    "    hidden_size = 64\n",
    "    batch_size = 32\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = generate_dataset(seq_length, num_samples, vocab_size)\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    encoder = Encoder(input_size = vocab_size, hidden_size = hidden_size)\n",
    "    decoder = Decoder(input_size = vocab_size, hidden_size = hidden_size, output_size = vocab_size)\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "    acc = evaluate_model(model, dataloader, device)\n",
    "    print(f\"Training Accuracy: {acc * 100:.2f}%\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_input, test_target = dataset[0]\n",
    "        test_input = test_input.unsqueeze(0).to(device)\n",
    "        test_target = test_target.unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(test_input, test_target)\n",
    "\n",
    "        predicted = torch.argmax(output, dim = 2)\n",
    "        print(\"Sample Input Sequence:   \", test_input.squeeze().tolist())\n",
    "        print(\"Sample Target Sequence:  \", test_target.squeeze().tolist())\n",
    "        print(\"Predicted Sequence       :  \", predicted.squeeze().tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.randn(3, 5)\n",
    "print(t)\n",
    "print(t>0)\n",
    "print(torch.sum(t > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = [torch.randint(1, 4, (5,)) for _ in range(4)]\n",
    "input_seq = torch.stack(input_sequence)\n",
    "\n",
    "# print(input_seq)\n",
    "# print(input_seq.shape)\n",
    "# print(input_seq[:, 3])\n",
    "# one_hot = nn.functional.one_hot(input_seq[:, 3], 3)\n",
    "# print(one_hot.shape)\n",
    "# print(one_hot)\n",
    "# 1 2 3\n",
    "# [1, 0, 0]\n",
    "# [0, 1, 0]\n",
    "# [0, 0, 1]\n",
    "# x_t = nn.functional.one_hot(input_seq[:, char_idx],\n",
    "              # num_classes = self.linear.in_features).float()\n",
    "\n",
    "t = torch.randn(10, 3, 5)\n",
    "print(t.view(-1, 7).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN_Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1UjkpaxyoQPcBEjMpcE1aOrqMs-ZKxxMr?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================= ===\n",
    "RNN의 장기 종속성 문제 시연\n",
    "================================================= ===\n",
    "이 스크립트는 순환 신경망(RNN)에 내재된 장기 종속성 문제를 탐구합니다.\n",
    "LSTM(Long Short-Term Memory) 및 GRU(Gated Recurrent Unit)와 같은 고급 아키텍처가 어떻게 사용되는지 보여줍니다.\n",
    "이러한 과제를 해결하십시오. 실험에서는 다음과 같이 설계된 합성 데이터 세트인 Copy Task를 활용합니다.\n",
    "확장된 시퀀스에 대한 정보를 유지하고 재현하는 모델의 능력을 테스트합니다.\n",
    "\n",
    "주요 개념:\n",
    "- 장기 의존성 문제: 기존 RNN은 긴 시퀀스에 걸쳐 정보를 유지하는 데 어려움을 겪습니다.\n",
    "  관련 정보 간의 거리가 멀어질수록 성능 저하가 발생합니다.\n",
    "- LSTM 및 GRU: 이러한 아키텍처는 유지 관리 및 관리를 허용하는 게이팅 메커니즘을 통합합니다.\n",
    "  정보 흐름을 규제하여 장기적인 의존성 문제를 효과적으로 완화합니다.\n",
    "- 복사 작업: 모델이 구분 기호 뒤의 입력 시퀀스를 재현해야 하는 간단하면서도 효과적인 작업입니다.\n",
    "  이 설정은 긴 시퀀스를 기억하고 재현하는 모델의 기능을 평가하는 데 이상적입니다.\n",
    "\n",
    "목적:\n",
    "1. 데이터 세트 생성: 임베딩 레이어를 사용하지 않고 복사 작업을 위한 합성 데이터 세트를 생성합니다.\n",
    "   각 모델 유형에 대해 두 가지 버전을 구현합니다.\n",
    "   - 수동 구현: `nn.Linear` 레이어를 활용하여 모델 방정식을 직접 구현합니다.\n",
    "   - 내장 구현: PyTorch의 내장 `nn.RNN`, `nn.LSTM` 및 `nn.GRU` 모듈 활용\n",
    "     간소화된 모델 정의를 위한 임베딩 레이어가 있습니다.\n",
    "2. 모델 정의: 수동 및 내장 구성 모두에서 RNN, LSTM 및 GRU 모델을 정의합니다.\n",
    "3. 훈련 및 평가: 합성 데이터 세트에서 각 모델 변형을 훈련하고 성능을 평가합니다.\n",
    "   LSTM과 GRU가 표준 RNN이 직면한 장기적인 종속성 문제를 어떻게 극복하는지 보여줍니다.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seeds for reproducibility 재현성을 위한 랜덤시드설정\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def generate_dataset(num_samples, seq_length, vocab_size=3):\n",
    "    \"\"\"\n",
    "    복사 작업을 위한 합성 데이터세트를 생성합니다.\n",
    "    복사 작업에서 각 입력 시퀀스는 일련의 기호와 구분 기호로 구성됩니다.\n",
    "    대상 시퀀스는 입력 시퀀스의 복사본(구분 기호가 없는 정확한 기호 시퀀스)입니다.\n",
    "    인수:\n",
    "        num_samples (int): 생성할 샘플 수를 지정\n",
    "        seq_length (int): 구분 기호 앞에 있는 입력 시퀀스의 길이를 지정\n",
    "        vocab_size (int, 선택사항): 고유 기호 수. 기본값은 3(0, 1, 구분 기호)입니다.\n",
    "    보고:\n",
    "        TensorDataset: 입력 및 대상 시퀀스를 포함하는 PyTorch TensorDataset입니다.\n",
    "    \"\"\"\n",
    "    delimiter = vocab_size - 1  # 마지막 인덱스를 구분 기호로 가정합니다.\n",
    "    inputs = []  # 입력 시퀀스를 저장할 리스트 초기화\n",
    "    targets = []  # 대상 시퀀스를 저장할 리스트 초기화\n",
    "    # num_samples 만큼 반복하여 입력 및 대상 시퀀스를 생성합니다.\n",
    "    for _ in range(num_samples):\n",
    "        # 입력 시퀀스 생성: 0부터 (vocab_size - 2)까지의 무작위 정수를 seq_length 만큼 생성\n",
    "        input_seq = [random.randint(0, vocab_size - 2) for _ in range(seq_length)]\n",
    "        input_seq.append(delimiter)  # 구분 기호 추가\n",
    "        inputs.append(input_seq)  # 생성된 입력 시퀀스를 입력 리스트에 추가\n",
    "        targets.append(input_seq)  # 생성된 입력 시퀀스를 대상 리스트에도 추가 (구분 기호 없이)\n",
    "    # 입력 및 대상 리스트를 텐서로 변환\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.long)  # 입력 시퀀스를 텐서로 변환\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)  # 대상 시퀀스를 텐서로 변환\n",
    "    # targets_tensor = indices_to_one_hot(targets_tensor, vocab_size)\n",
    "    # TensorDataset을 생성하여 입력 및 대상 시퀀스를 포함\n",
    "    dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "    return dataset  # 생성된 데이터셋 반환\n",
    "\n",
    "def generate_answer_mod_sum_prev_k(input_seq, mod, k):\n",
    "    target_seq = []\n",
    "    for i in range(len(input_seq)):\n",
    "        if i < k:\n",
    "            target_seq.append(sum(input_seq[:i+1]) % mod)\n",
    "        else:\n",
    "            target_seq.append(sum(input_seq[i-k+1:i+1]) % mod)\n",
    "    return target_seq\n",
    "\n",
    "def generate_dataset_mod_sum_prev_k(num_samples, seq_length, vocab_size=128, mod=10, k=4):\n",
    "    \"\"\"\n",
    "    이전 k 요소의 모듈러 합에 대한 합성 데이터 세트를 생성합니다.\n",
    "    Example:\n",
    "        when = 4, mod = 10\n",
    "        input = [0, 4, 5, 6, 7, 8]\n",
    "        target = [0, 4, 9, 5, 2, 6]\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(num_samples):\n",
    "        input_seq = [random.randint(0, vocab_size - 1) for _ in range(seq_length)]\n",
    "        target_seq = generate_answer_mod_sum_prev_k(input_seq, mod, k)\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    # targets_tensor = indices_to_one_hot(targets_tensor, mod)\n",
    "    dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "    return dataset\n",
    "\n",
    "def indices_to_one_hot(tensor, vocab_size):\n",
    "    # Get the shape of the input tensor except the last dimension\n",
    "    shape = tensor.shape[:-1]\n",
    "    # Get the last dimension (which contains the vocabulary indices)\n",
    "    last_dim = tensor.shape[-1]\n",
    "    # Flatten the tensor except the last dimension\n",
    "    tensor_flat = tensor.view(-1, last_dim)\n",
    "    # Create the one-hot encoded tensor with shape (total_elements, vocab_size)\n",
    "    one_hot_flat = torch.nn.functional.one_hot(tensor_flat, num_classes=vocab_size)\n",
    "    # Reshape the one-hot encoded tensor back to the original dimensions + vocab_size\n",
    "    one_hot_tensor = one_hot_flat.view(*shape, last_dim, vocab_size)\n",
    "    return one_hot_tensor.float()\n",
    "# ================================\n",
    "# Manual Implementations\n",
    "# ================================\n",
    "class RNNManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ===================================\n",
    "    Manual Recurrent Neural Network (RNN)\n",
    "    ===================================\n",
    "    Architecture:\n",
    "        Input -> Input-to-Hidden (Linear) -> Tanh Activation -> Hidden-to-Hidden (Linear) -> Output (Linear)\n",
    "    Formula:\n",
    "        h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b_h)\n",
    "        o_t = W_ho * h_t + b_o\n",
    "    설명:\n",
    "        이 클래스는 PyTorch의 내장 RNN 모듈을 사용하지 않고 간단한 RNN을 수동으로 구현합니다.\n",
    "        선형 레이어를 사용하여 숨겨진 상태와 출력을 계산합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(RNNManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_ih = nn.Linear(vocab_size, hidden_dim)  # Input to hidden\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim)  # Hidden to hidden\n",
    "        self.W_ho = nn.Linear(hidden_dim, output_dim)  # Hidden to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual RNN.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            h_t = torch.tanh(self.W_ih(x_t) + self.W_hh(h_t))  # [batch_size, hidden_dim]\n",
    "            o_t = self.W_ho(h_t)  # [batch_size, output_dim]\n",
    "            outputs.append(o_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "\n",
    "class LSTMManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ========================================\n",
    "    Manual Long Short-Term Memory (LSTM)\n",
    "    ========================================\n",
    "    Architecture:\n",
    "        Input -> Input Gates (Linear Layers) -> LSTM Cell -> Output (Linear)\n",
    "    Formula:\n",
    "        i_t = sigmoid(W_ii * x_t + W_hi * h_{t-1} + b_i)\n",
    "        f_t = sigmoid(W_if * x_t + W_hf * h_{t-1} + b_f)\n",
    "        g_t = tanh(W_ig * x_t + W_hg * h_{t-1} + b_g)\n",
    "        o_t = sigmoid(W_io * x_t + W_ho * h_{t-1} + b_o)\n",
    "        c_t = f_t * c_{t-1} + i_t * g_t\n",
    "        h_t = o_t * tanh(c_t)\n",
    "        y_t = W_ho * h_t + b_y\n",
    "    Description:\n",
    "        This class manually implements an LSTM cell without using PyTorch's built-in LSTM modules.\n",
    "        It includes input, forget, cell, and output gates to regulate information flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(LSTMManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Input gate components\n",
    "        self.W_ii = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hi = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Forget gate components\n",
    "        self.W_if = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hf = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Cell gate components\n",
    "        self.W_ig = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hg = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output gate components\n",
    "        self.W_io = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_ho = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output layer\n",
    "        self.W_yo = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual LSTM.\n",
    "        1. 데이터 세트 생성: 임베딩 레이어를 사용하지 않고 복사 작업을 위한 합성 데이터 세트를 생성합니다.\n",
    "        우리는 각 모델 유형에 대해 두 가지 버전을 구현합니다.\n",
    "        - 수동 구현: `nn.Linear` 레이어를 사용하여 모델 방정식을 직접 구현합니다.\n",
    "        - 내장 구현: PyTorch의 내장 `nn.RNN`, `nn.LSTM` 및 `nn.GRU` 모듈을 활용합니다.\n",
    "        단순화된 모델 정의를 위한 임베딩 레이어가 있습니다.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            i_t = torch.sigmoid(self.W_ii(x_t) + self.W_hi(h_t))  # Input gate\n",
    "            f_t = torch.sigmoid(self.W_if(x_t) + self.W_hf(h_t))  # Forget gate\n",
    "            g_t = torch.tanh(self.W_ig(x_t) + self.W_hg(h_t))     # Cell gate\n",
    "            o_t = torch.sigmoid(self.W_io(x_t) + self.W_ho(h_t))  # Output gate\n",
    "            c_t = f_t * c_t + i_t * g_t                            # Cell state\n",
    "            h_t = o_t * torch.tanh(c_t)                            # Hidden state\n",
    "            y_t = self.W_yo(h_t)                                    # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "\n",
    "class GRUManual(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    수동 게이트 순환 장치(GRU)\n",
    "    =======================================\n",
    "    Architecture:\n",
    "        Input -> Update Gate (Linear Layers) -> Reset Gate (Linear Layers) -> GRU Cell -> Output (Linear)\n",
    "    공식:\n",
    "        z_t = sigmoid(W_iz * x_t + W_hz * h_{t-1} + b_z)\n",
    "        r_t = sigmoid(W_ir * x_t + W_hr * h_{t-1} + b_r)\n",
    "        n_t = tanh(W_in * x_t + W_hn * (r_t * h_{t-1}) + b_n)\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
    "        y_t = W_ho * h_t + b_y\n",
    "    설명:\n",
    "        이 클래스는 PyTorch의 내장 GRU 모듈을 사용하지 않고 GRU 셀을 수동으로 구현합니다.\n",
    "        여기에는 정보 흐름을 제어하는 ​​업데이트 및 재설정 게이트가 포함됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(GRUManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Update gate\n",
    "        self.W_iz = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Reset gate\n",
    "        self.W_ir = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # New gate\n",
    "        self.W_in = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output layer\n",
    "        self.W_ho = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual GRU.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            z_t = torch.sigmoid(self.W_iz(x_t) + self.W_hz(h_t))  # Update gate\n",
    "            r_t = torch.sigmoid(self.W_ir(x_t) + self.W_hr(h_t))  # Reset gate\n",
    "            n_t = torch.tanh(self.W_in(x_t) + self.W_hn(r_t * h_t))  # New gate\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_t  # Hidden state\n",
    "            y_t = self.W_ho(h_t)  # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "# ================================\n",
    "# Built-In Implementations\n",
    "# ================================\n",
    "class RNNBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    Built-In Recurrent Neural Network (RNN)\n",
    "    =======================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.RNN] -> [Linear Output]\n",
    "    Formula:\n",
    "        h_t = RNN(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.RNN` 모듈을 활용합니다.\n",
    "        'nn.RNN' 내에 반복 작업을 캡슐화하여 모델 정의를 단순화합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in RNN.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "\n",
    "class LSTMBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ==========================================\n",
    "    Built-In Long Short-Term Memory (LSTM)\n",
    "    ==========================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.LSTM] -> [Linear Output]\n",
    "    Formula:\n",
    "        (h_t, c_t) = LSTM(x_t, (h_{t-1}, c_{t-1}))\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.LSTM` 모듈을 활용합니다.\n",
    "        LSTM 게이트의 복잡성을 추상화하여 반복 작업을 위한 간소화된 인터페이스를 제공합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in LSTM.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # output: [batch, seq, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "\n",
    "class GRUBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ======================================\n",
    "    Built-In Gated Recurrent Unit (GRU)\n",
    "    ======================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.GRU] -> [Linear Output]\n",
    "    Formula:\n",
    "        h_t = GRU(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.GRU` 모듈을 사용합니다.\n",
    "        이는 GRU의 게이팅 메커니즘을 캡슐화하여 반복 작업의 효율적인 구현을 제공합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in GRU.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, hidden = self.gru(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "# ================================\n",
    "# Training and Evaluation\n",
    "# ================================\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): 훈련할 신경망 모델\n",
    "        dataloader (DataLoader): DataLoader for the training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimization algorithm.\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion): # 평가\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset.\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader for the validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "    Returns:\n",
    "        float: Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "# ================================\n",
    "# Main Execution\n",
    "# ================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    VOCAB_SIZE = 16         # 단어집합 크기\n",
    "    EMBEDDING_DIM = 8       # 임베딩 벡터 차원 : (벡터의 길이 설정) : (입력데이터 차원을 낮춰 표현)\n",
    "    HIDDEN_DIM = 32         # 은닉층 벡터 차원\n",
    "    OUTPUT_DIM = 10         # 출력층 벡터 차원 : 예측해야할 클래스(카테고리)개수\n",
    "    SEQ_LENGTH = 50         # Length of the input sequence before the delimiter\n",
    "    NUM_SAMPLES = 1000     # 샘플\n",
    "    BATCH_SIZE = 64         # batch\n",
    "    EPOCHS = 100             # epochs\n",
    "    LEARNING_RATE = 0.001   # learning_rate\n",
    "    LOOKBACK = 3            # 참조기간 : 시퀀스 데이터에서 과거의 몇 시점 까지 데이터정보를 참조 할지를 설정하는 값\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Generate dataset\n",
    "    # dataset = generate_dataset(NUM_SAMPLES, SEQ_LENGTH, VOCAB_SIZE)\n",
    "    dataset = generate_dataset_mod_sum_prev_k(NUM_SAMPLES, SEQ_LENGTH, VOCAB_SIZE, OUTPUT_DIM, LOOKBACK)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "    # Initialize models\n",
    "    # Manual Implementations\n",
    "    rnn_manual = RNNManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    lstm_manual = LSTMManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    gru_manual = GRUManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "\n",
    "    # Built-In Implementations\n",
    "    rnn_builtin = RNNBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                             hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    lstm_builtin = LSTMBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                               hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    gru_builtin = GRUBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                             hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = lambda x, y: torch.nn.functional.cross_entropy(x.permute(0, 2, 1), y)\n",
    "\n",
    "    # Define optimizers\n",
    "    optimizers = {\n",
    "        'RNN_Manual': optim.Adam(rnn_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'LSTM_Manual': optim.Adam(lstm_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'GRU_Manual': optim.Adam(gru_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'RNN_BuiltIn': optim.Adam(rnn_builtin.parameters(), lr = LEARNING_RATE),\n",
    "        'LSTM_BuiltIn': optim.Adam(lstm_builtin.parameters(), lr = LEARNING_RATE),\n",
    "        'GRU_BuiltIn': optim.Adam(gru_builtin.parameters(), lr = LEARNING_RATE),\n",
    "    }\n",
    "\n",
    "    # Initialize loss tracking\n",
    "    loss_history = {\n",
    "        'RNN_Manual': {'train': [], 'val': []},\n",
    "        'LSTM_Manual': {'train': [], 'val': []},\n",
    "        'GRU_Manual': {'train': [], 'val': []},\n",
    "        'RNN_BuiltIn': {'train': [], 'val': []},\n",
    "        'LSTM_BuiltIn': {'train': [], 'val': []},\n",
    "        'GRU_BuiltIn': {'train': [], 'val': []},\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "        # Train RNN Manual\n",
    "        train_loss = train(rnn_manual, train_loader, criterion, optimizers['RNN_Manual'])\n",
    "        val_loss = evaluate(rnn_manual, val_loader, criterion)\n",
    "        loss_history['RNN_Manual']['train'].append(train_loss)\n",
    "        loss_history['RNN_Manual']['val'].append(val_loss)\n",
    "        print(f'RNN Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train LSTM Manual\n",
    "        train_loss = train(lstm_manual, train_loader, criterion, optimizers['LSTM_Manual'])\n",
    "        val_loss = evaluate(lstm_manual, val_loader, criterion)\n",
    "        loss_history['LSTM_Manual']['train'].append(train_loss)\n",
    "        loss_history['LSTM_Manual']['val'].append(val_loss)\n",
    "        print(f'LSTM Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train GRU Manual\n",
    "        train_loss = train(gru_manual, train_loader, criterion, optimizers['GRU_Manual'])\n",
    "        val_loss = evaluate(gru_manual, val_loader, criterion)\n",
    "        loss_history['GRU_Manual']['train'].append(train_loss)\n",
    "        loss_history['GRU_Manual']['val'].append(val_loss)\n",
    "        print(f'GRU Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train RNN Built-In\n",
    "        train_loss = train(rnn_builtin, train_loader, criterion, optimizers['RNN_BuiltIn'])\n",
    "        val_loss = evaluate(rnn_builtin, val_loader, criterion)\n",
    "        loss_history['RNN_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['RNN_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'RNN Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train LSTM Built-In\n",
    "        train_loss = train(lstm_builtin, train_loader, criterion, optimizers['LSTM_BuiltIn'])\n",
    "        val_loss = evaluate(lstm_builtin, val_loader, criterion)\n",
    "        loss_history['LSTM_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['LSTM_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'LSTM Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train GRU Built-In\n",
    "        train_loss = train(gru_builtin, train_loader, criterion, optimizers['GRU_BuiltIn'])\n",
    "        val_loss = evaluate(gru_builtin, val_loader, criterion)\n",
    "        loss_history['GRU_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['GRU_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'GRU Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "    # Plotting the loss curves\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    for model_name in loss_history:\n",
    "        plt.plot(loss_history[model_name]['train'], label=f'{model_name} Train')\n",
    "        plt.plot(loss_history[model_name]['val'], label=f'{model_name} Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses for RNN, LSTM, and GRU (Manual and Built-In)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Display final validation losses\n",
    "    print(\"\\nFinal Validation Losses:\")\n",
    "    for model_name in loss_history:\n",
    "        final_val_loss = loss_history[model_name]['val'][-1]\n",
    "        print(f'{model_name}: {final_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- isort를 사용하여 Python 파일에 대한 조직 지원 가져오기\n",
    "- isort를 사용하여 Visual Studio Code용 정렬 확장 가져오기\n",
    "- isort를 사용하여 가져오기 정렬을 제공하는 Visual Studio Code 확장입니다. 확장은 isort=5.11.5와 함께 제공됩니다. 이 확장은 LSP(Language Server Protocol)를 사용하여 서버와 같은 모드에서 isort를 실행합니다.\n",
    "- \n",
    "- 메모:\n",
    "- \n",
    "- 이 확장은 적극적으로 지원되는 모든 Python 언어 버전(예: Python >= 3.7)에서 지원됩니다.\n",
    "- 번들 isort는 선택한 Python 환경에 설치된 isort 버전이 없는 경우에만 사용됩니다.\n",
    "- isort의 최소 지원 버전은 5.10.1입니다.\n",
    "- 용법\n",
    "- Visual Studio Code에 설치되면 확장은 isort를 가져오기 구성자로 등록합니다. 키보드 단축키 Shift Alt O를 사용하여 가져오기 편집기 작업 구성을 트리거할 수 있습니다. 가져오기가 구성되지 않은 경우 사용 가능한 빠른 수정에서 이를 트리거할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "=====================================================\n",
    "Demonstrating Long-Term Dependency Problems in RNNs\n",
    "=====================================================\n",
    "\n",
    "This script explores the long-term dependency problem inherent in Recurrent Neural Networks (RNNs)\n",
    "and showcases how advanced architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs)\n",
    "address these challenges. The experiment utilizes the Copy Task, a synthetic dataset designed to\n",
    "test a model's ability to retain and reproduce information over extended sequences.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "- Long-Term Dependency Problem: Traditional RNNs struggle to retain information over long sequences,\n",
    "  leading to performance degradation as the distance between relevant information increases.\n",
    "\n",
    "- LSTM and GRU: These architectures incorporate gating mechanisms that allow them to maintain and\n",
    "  regulate information flow, effectively mitigating the long-term dependency problem.\n",
    "\n",
    "- Copy Task: A simple yet effective task where the model must reproduce an input sequence after a delimiter.\n",
    "  This setup is ideal for evaluating a model's capability to remember and reproduce long sequences.\n",
    "\n",
    "Objective:\n",
    "\n",
    "1. Dataset Generation: Create a synthetic dataset for the Copy Task without using embedding layers.\n",
    "   Implement two versions for each model type:\n",
    "   - Manual Implementation: Utilize `nn.Linear` layers to directly implement the model equations.\n",
    "   - Built-In Implementation: Leverage PyTorch's built-in `nn.RNN`, `nn.LSTM`, and `nn.GRU` modules\n",
    "     with embedding layers for streamlined model definitions.\n",
    "\n",
    "2. Model Definitions: Define RNN, LSTM, and GRU models in both manual and built-in configurations.\n",
    "\n",
    "3. Training and Evaluation: Train each model variant on the synthetic dataset and evaluate their performance\n",
    "   to demonstrate how LSTM and GRU overcome the long-term dependency issues faced by standard RNNs.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def generate_dataset(num_samples, seq_length, vocab_size=3):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for the Copy Task.\n",
    "\n",
    "    In the Copy Task, each input sequence consists of a series of symbols followed by a delimiter.\n",
    "    The target sequence is the exact sequence of symbols without the delimiter.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): Number of samples to generate.\n",
    "        seq_length (int): Length of the input sequence before the delimiter.\n",
    "        vocab_size (int, optional): Number of unique symbols. Defaults to 3 (0, 1, delimiter).\n",
    "\n",
    "    Returns:\n",
    "        TensorDataset: A PyTorch TensorDataset containing input and target sequences.\n",
    "    \"\"\"\n",
    "    delimiter = vocab_size - 1  # Assuming the last index is the delimiter\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(num_samples):\n",
    "        input_seq = [random.randint(0, vocab_size - 2) for _ in range(seq_length)]\n",
    "        input_seq.append(delimiter)\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(input_seq)\n",
    "\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    # targets_tensor = indices_to_one_hot(targets_tensor, vocab_size)\n",
    "    dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def generate_answer_mod_sum_prev_k(input_seq, mod, k):\n",
    "    target_seq = []\n",
    "    for i in range(len(input_seq)):\n",
    "        if i < k:\n",
    "            target_seq.append(sum(input_seq[:i+1]) % mod)\n",
    "        else:\n",
    "            target_seq.append(sum(input_seq[i-k+1:i+1]) % mod)\n",
    "    return target_seq\n",
    "\n",
    "\n",
    "def generate_dataset_mod_sum_prev_k(num_samples, seq_length, vocab_size=128, mod=10, k=4):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for modular sum of previoous k elements\n",
    "\n",
    "    Example:\n",
    "        when = 4, mod = 10\n",
    "        input = [0, 4, 5, 6, 7, 8]\n",
    "        target = [0, 4, 9, 5, 2, 6]\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for _ in range(num_samples):\n",
    "        input_seq = [random.randint(0, vocab_size - 1) for _ in range(seq_length)]\n",
    "        target_seq = generate_answer_mod_sum_prev_k(input_seq, mod, k)\n",
    "\n",
    "        inputs.append(input_seq)\n",
    "        targets.append(target_seq)\n",
    "\n",
    "\n",
    "    inputs_tensor = torch.tensor(inputs, dtype=torch.long)\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    # targets_tensor = indices_to_one_hot(targets_tensor, mod)\n",
    "    dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def indices_to_one_hot(tensor, vocab_size):\n",
    "    # Get the shape of the input tensor except the last dimension\n",
    "    shape = tensor.shape[:-1]\n",
    "\n",
    "    # Get the last dimension (which contains the vocabulary indices)\n",
    "    last_dim = tensor.shape[-1]\n",
    "\n",
    "    # Flatten the tensor except the last dimension\n",
    "    tensor_flat = tensor.view(-1, last_dim)\n",
    "\n",
    "    # Create the one-hot encoded tensor with shape (total_elements, vocab_size)\n",
    "    one_hot_flat = torch.nn.functional.one_hot(tensor_flat, num_classes=vocab_size)\n",
    "\n",
    "    # Reshape the one-hot encoded tensor back to the original dimensions + vocab_size\n",
    "    one_hot_tensor = one_hot_flat.view(*shape, last_dim, vocab_size)\n",
    "\n",
    "    return one_hot_tensor.float()\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Manual Implementations\n",
    "# ================================\n",
    "\n",
    "class RNNManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ===================================\n",
    "    Manual Recurrent Neural Network (RNN)\n",
    "    ===================================\n",
    "\n",
    "    Architecture:\n",
    "        Input -> Input-to-Hidden (Linear) -> Tanh Activation -> Hidden-to-Hidden (Linear) -> Output (Linear)\n",
    "\n",
    "    Formula:\n",
    "        h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b_h)\n",
    "        o_t = W_ho * h_t + b_o\n",
    "\n",
    "    Description:\n",
    "        This class manually implements a simple RNN without using PyTorch's built-in RNN modules.\n",
    "        It uses linear layers to compute the hidden states and outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(RNNManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W_ih = nn.Linear(vocab_size, hidden_dim)  # Input to hidden\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim)  # Hidden to hidden\n",
    "        self.W_ho = nn.Linear(hidden_dim, output_dim)  # Hidden to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual RNN.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            h_t = torch.tanh(self.W_ih(x_t) + self.W_hh(h_t))  # [batch_size, hidden_dim]\n",
    "            o_t = self.W_ho(h_t)  # [batch_size, output_dim]\n",
    "            outputs.append(o_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "\n",
    "class LSTMManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ========================================\n",
    "    Manual Long Short-Term Memory (LSTM)\n",
    "    ========================================\n",
    "\n",
    "    Architecture:\n",
    "        Input -> Input Gates (Linear Layers) -> LSTM Cell -> Output (Linear)\n",
    "\n",
    "    Formula:\n",
    "        i_t = sigmoid(W_ii * x_t + W_hi * h_{t-1} + b_i)\n",
    "        f_t = sigmoid(W_if * x_t + W_hf * h_{t-1} + b_f)\n",
    "        g_t = tanh(W_ig * x_t + W_hg * h_{t-1} + b_g)\n",
    "        o_t = sigmoid(W_io * x_t + W_ho * h_{t-1} + b_o)\n",
    "        c_t = f_t * c_{t-1} + i_t * g_t\n",
    "        h_t = o_t * tanh(c_t)\n",
    "        y_t = W_ho * h_t + b_y\n",
    "\n",
    "    Description:\n",
    "        This class manually implements an LSTM cell without using PyTorch's built-in LSTM modules.\n",
    "        It includes input, forget, cell, and output gates to regulate information flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(LSTMManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Input gate components\n",
    "        self.W_ii = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hi = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Forget gate components\n",
    "        self.W_if = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hf = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Cell gate components\n",
    "        self.W_ig = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hg = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output gate components\n",
    "        self.W_io = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_ho = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output layer\n",
    "        self.W_yo = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual LSTM.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        c_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            i_t = torch.sigmoid(self.W_ii(x_t) + self.W_hi(h_t))  # Input gate\n",
    "            f_t = torch.sigmoid(self.W_if(x_t) + self.W_hf(h_t))  # Forget gate\n",
    "            g_t = torch.tanh(self.W_ig(x_t) + self.W_hg(h_t))     # Cell gate\n",
    "            o_t = torch.sigmoid(self.W_io(x_t) + self.W_ho(h_t))  # Output gate\n",
    "            c_t = f_t * c_t + i_t * g_t                            # Cell state\n",
    "            h_t = o_t * torch.tanh(c_t)                            # Hidden state\n",
    "            y_t = self.W_yo(h_t)                                    # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "\n",
    "class GRUManual(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    Manual Gated Recurrent Unit (GRU)\n",
    "    =======================================\n",
    "\n",
    "    Architecture:\n",
    "        Input -> Update Gate (Linear Layers) -> Reset Gate (Linear Layers) -> GRU Cell -> Output (Linear)\n",
    "\n",
    "    Formula:\n",
    "        z_t = sigmoid(W_iz * x_t + W_hz * h_{t-1} + b_z)\n",
    "        r_t = sigmoid(W_ir * x_t + W_hr * h_{t-1} + b_r)\n",
    "        n_t = tanh(W_in * x_t + W_hn * (r_t * h_{t-1}) + b_n)\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
    "        y_t = W_ho * h_t + b_y\n",
    "\n",
    "    Description:\n",
    "        This class manually implements a GRU cell without using PyTorch's built-in GRU modules.\n",
    "        It includes update and reset gates to control the flow of information.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
    "        super(GRUManual, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Update gate\n",
    "        self.W_iz = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Reset gate\n",
    "        self.W_ir = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # New gate\n",
    "        self.W_in = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.W_hn = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Output layer\n",
    "        self.W_ho = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual GRU.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        x = indices_to_one_hot(x, self.vocab_size)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        h_t = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            z_t = torch.sigmoid(self.W_iz(x_t) + self.W_hz(h_t))  # Update gate\n",
    "            r_t = torch.sigmoid(self.W_ir(x_t) + self.W_hr(h_t))  # Reset gate\n",
    "            n_t = torch.tanh(self.W_in(x_t) + self.W_hn(r_t * h_t))  # New gate\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_t  # Hidden state\n",
    "            y_t = self.W_ho(h_t)  # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "\n",
    "# ================================\n",
    "# Built-In Implementations\n",
    "# ================================\n",
    "\n",
    "class RNNBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    Built-In Recurrent Neural Network (RNN)\n",
    "    =======================================\n",
    "\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.RNN] -> [Linear Output]\n",
    "\n",
    "    Formula:\n",
    "        h_t = RNN(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "\n",
    "    Description:\n",
    "        This class leverages PyTorch's built-in `nn.RNN` module along with an embedding layer.\n",
    "        It simplifies the model definition by encapsulating the recurrent operations within `nn.RNN`.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in RNN.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "\n",
    "class LSTMBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ==========================================\n",
    "    Built-In Long Short-Term Memory (LSTM)\n",
    "    ==========================================\n",
    "\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.LSTM] -> [Linear Output]\n",
    "\n",
    "    Formula:\n",
    "        (h_t, c_t) = LSTM(x_t, (h_{t-1}, c_{t-1}))\n",
    "        y_t = W_ho * h_t + b_o\n",
    "\n",
    "    Description:\n",
    "        This class utilizes PyTorch's built-in `nn.LSTM` module along with an embedding layer.\n",
    "        It abstracts the complexities of LSTM gates, providing a streamlined interface for recurrent operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in LSTM.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # output: [batch, seq, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "\n",
    "class GRUBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ======================================\n",
    "    Built-In Gated Recurrent Unit (GRU)\n",
    "    ======================================\n",
    "\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.GRU] -> [Linear Output]\n",
    "\n",
    "    Formula:\n",
    "        h_t = GRU(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "\n",
    "    Description:\n",
    "        This class employs PyTorch's built-in `nn.GRU` module along with an embedding layer.\n",
    "        It encapsulates the GRU's gating mechanisms, providing an efficient implementation of recurrent operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(GRUBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in GRU.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, hidden = self.gru(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output\n",
    "\n",
    "# ================================\n",
    "# Training and Evaluation\n",
    "# ================================\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        dataloader (DataLoader): DataLoader for the training data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimization algorithm.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader for the validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# ================================\n",
    "# Main Execution\n",
    "# ================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    VOCAB_SIZE = 16\n",
    "    EMBEDDING_DIM = 8\n",
    "    HIDDEN_DIM = 32\n",
    "    OUTPUT_DIM = 10\n",
    "    SEQ_LENGTH = 50  # Length of the input sequence before the delimiter\n",
    "    NUM_SAMPLES = 10000\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 0.001\n",
    "    LOOKBACK = 3\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Generate dataset\n",
    "    # dataset = generate_dataset(NUM_SAMPLES, SEQ_LENGTH, VOCAB_SIZE)\n",
    "    dataset = generate_dataset_mod_sum_prev_k(NUM_SAMPLES, SEQ_LENGTH, VOCAB_SIZE, OUTPUT_DIM, LOOKBACK)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "    # Initialize models\n",
    "    # Manual Implementations\n",
    "    rnn_manual = RNNManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    lstm_manual = LSTMManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    gru_manual = GRUManual(vocab_size = VOCAB_SIZE, hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "\n",
    "    # Built-In Implementations\n",
    "    rnn_builtin = RNNBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                             hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    lstm_builtin = LSTMBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                               hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "    gru_builtin = GRUBuiltIn(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM,\n",
    "                             hidden_dim = HIDDEN_DIM, output_dim = OUTPUT_DIM).to(device)\n",
    "\n",
    "    # Define loss function\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = lambda x, y: torch.nn.functional.cross_entropy(x.permute(0, 2, 1), y)\n",
    "\n",
    "    # Define optimizers\n",
    "    optimizers = {\n",
    "        'RNN_Manual': optim.Adam(rnn_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'LSTM_Manual': optim.Adam(lstm_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'GRU_Manual': optim.Adam(gru_manual.parameters(), lr = LEARNING_RATE),\n",
    "        'RNN_BuiltIn': optim.Adam(rnn_builtin.parameters(), lr = LEARNING_RATE),\n",
    "        'LSTM_BuiltIn': optim.Adam(lstm_builtin.parameters(), lr = LEARNING_RATE),\n",
    "        'GRU_BuiltIn': optim.Adam(gru_builtin.parameters(), lr = LEARNING_RATE),\n",
    "    }\n",
    "\n",
    "    # Initialize loss tracking\n",
    "    loss_history = {\n",
    "        'RNN_Manual': {'train': [], 'val': []},\n",
    "        'LSTM_Manual': {'train': [], 'val': []},\n",
    "        'GRU_Manual': {'train': [], 'val': []},\n",
    "        'RNN_BuiltIn': {'train': [], 'val': []},\n",
    "        'LSTM_BuiltIn': {'train': [], 'val': []},\n",
    "        'GRU_BuiltIn': {'train': [], 'val': []},\n",
    "    }\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "\n",
    "        # Train RNN Manual\n",
    "        train_loss = train(rnn_manual, train_loader, criterion, optimizers['RNN_Manual'])\n",
    "        val_loss = evaluate(rnn_manual, val_loader, criterion)\n",
    "        loss_history['RNN_Manual']['train'].append(train_loss)\n",
    "        loss_history['RNN_Manual']['val'].append(val_loss)\n",
    "        print(f'RNN Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train LSTM Manual\n",
    "        train_loss = train(lstm_manual, train_loader, criterion, optimizers['LSTM_Manual'])\n",
    "        val_loss = evaluate(lstm_manual, val_loader, criterion)\n",
    "        loss_history['LSTM_Manual']['train'].append(train_loss)\n",
    "        loss_history['LSTM_Manual']['val'].append(val_loss)\n",
    "        print(f'LSTM Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train GRU Manual\n",
    "        train_loss = train(gru_manual, train_loader, criterion, optimizers['GRU_Manual'])\n",
    "        val_loss = evaluate(gru_manual, val_loader, criterion)\n",
    "        loss_history['GRU_Manual']['train'].append(train_loss)\n",
    "        loss_history['GRU_Manual']['val'].append(val_loss)\n",
    "        print(f'GRU Manual | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train RNN Built-In\n",
    "        train_loss = train(rnn_builtin, train_loader, criterion, optimizers['RNN_BuiltIn'])\n",
    "        val_loss = evaluate(rnn_builtin, val_loader, criterion)\n",
    "        loss_history['RNN_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['RNN_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'RNN Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train LSTM Built-In\n",
    "        train_loss = train(lstm_builtin, train_loader, criterion, optimizers['LSTM_BuiltIn'])\n",
    "        val_loss = evaluate(lstm_builtin, val_loader, criterion)\n",
    "        loss_history['LSTM_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['LSTM_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'LSTM Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Train GRU Built-In\n",
    "        train_loss = train(gru_builtin, train_loader, criterion, optimizers['GRU_BuiltIn'])\n",
    "        val_loss = evaluate(gru_builtin, val_loader, criterion)\n",
    "        loss_history['GRU_BuiltIn']['train'].append(train_loss)\n",
    "        loss_history['GRU_BuiltIn']['val'].append(val_loss)\n",
    "        print(f'GRU Built-In | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the loss curves\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    for model_name in loss_history:\n",
    "        plt.plot(loss_history[model_name]['train'], label=f'{model_name} Train')\n",
    "        plt.plot(loss_history[model_name]['val'], label=f'{model_name} Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses for RNN, LSTM, and GRU (Manual and Built-In)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Display final validation losses\n",
    "    print(\"\\nFinal Validation Losses:\")\n",
    "    for model_name in loss_history:\n",
    "        final_val_loss = loss_history[model_name]['val'][-1]\n",
    "        print(f'{model_name}: {final_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_in = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 15]])\n",
    "print(test_in)\n",
    "print(torch.tensor(generate_answer_mod_sum_prev_k(test_in[0], OUTPUT_DIM, LOOKBACK)))\n",
    "with torch.no_grad():\n",
    "    for model in [rnn_manual, lstm_manual, gru_manual, rnn_builtin, lstm_builtin, gru_builtin]:\n",
    "        test_out = model(test_in)\n",
    "        values, indices = test_out.max(dim=-1)\n",
    "        print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# various loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀의 경우\n",
    "\n",
    "1. MSELoss\n",
    "\n",
    "분류의 경우\n",
    "\n",
    "1. (Raw) Logit\n",
    "2. Softmax / Log Softmax의 결과값\n",
    "3. 객관식 정답 자체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSELoss\n",
    "\n",
    "- 산식: $\\frac{1}{N} \\sum (y_i - y^{pred}_i)^2$\n",
    "- pred ( $y^{pred}_i$ ): (*shape, ), raw logit\n",
    "- target ( $y_i$): (*shape,), real number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrosssEntropyLoss\n",
    "\n",
    "- 산식 : $-\\frac{1}{N} \\sum log(\\frac{exp(x_{i, y_i})}{\\sum_j exp(x_{i, j})})$ = $-\\frac{1}{N} \\sum log(softmax(X))$\n",
    "  - n: number of samples\n",
    "  - $x_{i, j}$: Logit for class j of sample i\n",
    "  - $y_i$: i번째 샘플의 정답\n",
    "- pred : ($x_{i, j}$) : (*shape, number_of_classes)\n",
    "- target ($y_i$) : (*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# input: (batch_size, number_of_classes)\n",
    "# batch_size = 3, number_of_classes: 4\n",
    "input = torch.tensor([[1.2, 10.5, 2.1, 0.9],\n",
    "                      [1.0, 2.0, 0.1, 0.3],\n",
    "                      [20.1, -3.1, 2.5, 1.0],])\n",
    "# output: (batch_size,)\n",
    "output = torch.tensor([2, 1, 0])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(input, output)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLLLoss\n",
    "\n",
    "- 산식 : $-\\frac{1}{N} \\sum log(p_{i, y_i})$\n",
    "  - n: number of samples\n",
    "  - $p_{i, j}$: i번째 sample의 답이 j일 확률\n",
    "  - $y_i$: i번째 sample의 정답\n",
    "- pred : ($p_{i, j}$) : (*shape, number_of_classes)\n",
    "- target ($y_i$) : (*shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BCELoss: binary일 때만 쓰고, input이 sigmoid이길 기대\n",
    "BCEWithLogitsLoss: input이 raw logit이길 기대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 연습만이 살 길 --- 완전하게 실습\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN vari 기반 이름->국가\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import string\n",
    "'''\n",
    "def generate_data(n_samples=10000, x_amplitude = 1.5, noise_amplitude = 1, batch_size = 32):\n",
    "    # 표준정규분포(평0, 표준편차1)인 난수샘플 n_samples개의 1차원텐서 생성 : 표준편차 x_amplitude 배\n",
    "    X = torch.randn(n_samples, 1) * x_amplitude\n",
    "    noise = torch.randn(n_samples, 1) * noise_amplitude # noise 생성\n",
    "    y = 3*X + 4 + noise # 생성값 y = 3x + 4 + (노이즈)첨가\n",
    "    \n",
    "    dataset = TensorDataset(X, y) # X,y를 하나의 dataset으로 병합\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "'''\n",
    "# 데이터 로더\n",
    "class BaseDataLoader:\n",
    "    def __init__(self, dataset, train_ratio, valid_ratio, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.train_ratio = train_ratio\n",
    "        self.valid_ratio = valid_ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.prepare_data_loaders(self.dataset)\n",
    "    \n",
    "    def prepare_data_loaders(self, dataset):\n",
    "            total_size = len(dataset)\n",
    "            train_size = int(self.train_ratio * total_size)\n",
    "            valid_size = int(self.valid_ratio * total_size)\n",
    "            test_size = total_size - train_size - valid_size\n",
    "\n",
    "            train_set, valid_set, test_set = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "            train_loader = DataLoader(train_set, batch_size=self.batch_size, shuffle=True)\n",
    "            valid_loader = DataLoader(valid_set, batch_size=self.batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)# test set 전체 배치로\n",
    "            return train_loader, valid_loader, test_loader\n",
    "\n",
    "class CustomDataLoader(BaseDataLoader):\n",
    "    def __init__(self, dataset_class, train_ratio, valid_ratio, batch_size):\n",
    "        # 데이터셋 클래스를 사용하여 데이터셋 생성\n",
    "        self.dataset = dataset_class().create_dataset()  # 클래스 인스턴스화 및 데이터셋 생성\n",
    "        super().__init__(self.dataset, train_ratio, valid_ratio, batch_size)\n",
    "\n",
    "# 데이터 셋\n",
    "class CustomDataset1:\n",
    "    def create_dataset(self):\n",
    "        '''여기 필요한 dataset 함수를 가져다 끼자'''\n",
    "        return self.generate_classification(sample_n=10000)\n",
    "    \n",
    "    def create_dataset1(self):\n",
    "        features = torch.randn(1000, 10)  # 1000개의 샘플, 10개의 피처\n",
    "        labels = torch.randint(0, 2, (1000,))  # 이진 분류 레이블\n",
    "        self.plot_data(features, labels)\n",
    "        dataset = TensorDataset(features, labels)\n",
    "        return dataset\n",
    "    \n",
    "    def generate_classification(self, sample_n=10000): # x + y < 1 classification data\n",
    "        x = torch.rand(sample_n, 1)\n",
    "        y = torch.rand(sample_n, 1)\n",
    "        features = torch.cat((x, y), dim=1)\n",
    "        labels = (x + y < 1).int()\n",
    "        self.plot_data(features, labels)\n",
    "        dataset = TensorDataset(features, labels)\n",
    "        return dataset\n",
    "    \n",
    "    def plot_data(self, features, labels):\n",
    "        x = features[:, 0]\n",
    "        y = features[:, 1]  # 두 번째 특성 (Tensor)\n",
    "        labels = labels.squeeze()\n",
    "        \n",
    "        plt.scatter(x, y, c=labels, cmap='viridis', alpha=0.5)\n",
    "        boundary_x = torch.linspace(0, 1, 100)  # 0부터 1까지 100개의 값을 생성\n",
    "        boundary_y = 1 - boundary_x\n",
    "        \n",
    "        plt.plot(boundary_x, boundary_y, 'r--', label='x + y = 1')\n",
    "        \n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.title('Scatter Plot of Features with Decision Boundary')\n",
    "        plt.colorbar(label='Labels')\n",
    "        plt.show()\n",
    "\n",
    "class Name_nationality_Dataset2: # 이름 -> 국가\n",
    "    def __init__(self):\n",
    "        self.unknown_char, self.padding = '?','_'\n",
    "        self.all_letters = string.ascii_lowercase + self.unknown_char + self.padding\n",
    "        self.category_names,self.all_categories = {},[]\n",
    "        self.max_word_length = 10\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        '''여기 필요한 dataset 함수를 가져다 끼자'''\n",
    "        return self.Preprocessing_name()\n",
    "    \n",
    "    def Preprocessing_name(self):\n",
    "        files = glob.glob('./project_reference/data/names/*.txt')\n",
    "        # print(f'{len(files)} fiels',*files,sep='\\n')\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                words = f.read().strip().split('\\n')\n",
    "            category = file.split('\\\\')[-1].split('.')[0]\n",
    "            # category = os.path.splitext(os.path.basename(file))[0]\n",
    "            self.all_categories.append(category)\n",
    "            \n",
    "            words = [word for word in words if len(word) <= (self.max_word_length)]\n",
    "            self.category_names[category] = words\n",
    "\n",
    "        for category, words in self.category_names.items():\n",
    "            x = [self.word_to_tensor(word) for word in words]\n",
    "            y = [self.category_to_tensor(self.all_categories.index(category), len(self.all_categories)) for word in words]\n",
    "        \n",
    "        x = torch.stack(x)\n",
    "        y = torch.stack(y)\n",
    "        print(x.shape, y.shape)\n",
    "        dataset = TensorDataset(x, y)\n",
    "        return dataset\n",
    "    \n",
    "    def category_to_tensor(self, idx, N):\n",
    "        return torch.tensor([(i==idx)/1 for i in range(N)], dtype=torch.long)\n",
    "\n",
    "    def input_padding_and_unknown_char_handling(self, word):\n",
    "        word = ''.join(char if char in string.ascii_lowercase else self.unknown_char for char in word.lower())\n",
    "        return word + '_'*(10 - len(word)) if len(word)!=10 else word\n",
    "\n",
    "    def word_to_tensor(self, word):\n",
    "        word = self.input_padding_and_unknown_char_handling(word)\n",
    "        res = torch.zeros(len(word), len(self.all_letters))\n",
    "        for idx, letter in enumerate(word):\n",
    "            res[idx][self.all_letters.find(letter)] = 1\n",
    "        # print(word, res, res.shape)\n",
    "        return res #.squeeze(dim = 1) # shape(10,28)    \n",
    "    \n",
    "\n",
    "    \n",
    "def plot_loss_history(loss_history):\n",
    "    plt.plot(range(1, len(loss_history)+1), loss_history)\n",
    "    plt.show()\n",
    "    \n",
    "# 모델정의\n",
    "class Feedforward(nn.Module): # -x+y < 1 classification module\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        i2h_out = torch.relu(self.i2h(input))\n",
    "        h2h_out = torch.relu(self.h2h(i2h_out))\n",
    "        h2o_out = self.h2o(h2h_out)\n",
    "        return h2o_out\n",
    "    \n",
    "class Feedforward_1(nn.Module): # 이름 -> 국가\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Feedforward_1, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim = 1) # 출력값을 확률분포로 변환\n",
    "        self.optimizer = optim.Adam # 다양한 이동 평균(학습률)을 적용하여 손실함수 최적화 # 모델의 파라미터(가중치)를 업데이트하여 손실을 최소화하는 역할\n",
    "\n",
    "    def forward(self, input):\n",
    "        i2h_out = torch.relu(self.i2h(input))\n",
    "        h2h_out = torch.relu(self.h2h(i2h_out))\n",
    "        h2o_out = self.h2o(h2h_out)\n",
    "        h2o_out = self.softmax(h2o_out)\n",
    "        return h2o_out\n",
    "\n",
    "class RecurrentNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RecurrentNeuralNetwork, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        print(batch_size, seq_length, _)\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size).to(x.device) # 각 시간 단계(t)에서 히든 상태는 이전 시간 단계의 히든 상태와 현재 입력을 통해 갱신\n",
    "        outputs = []\n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            h_t = torch.tanh(self.i2h(x_t) + self.h2h(h_t))  # [batch_size, hidden_dim] # hidden state 업데이트\n",
    "            o_t = self.h2o(h_t)  # [batch_size, output_dim]\n",
    "            outputs.append(o_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs\n",
    "    '''\n",
    "        def forward(self, x, hidden):\n",
    "        # x: (batch_size, max_length, len(alphabets))\n",
    "        hidden = F.tanh(self.i2h(x) + self.h2h(hidden)) # hidden: (batch_size, hidden_size)\n",
    "        if self.batch_first:\n",
    "            output = self.h2o(hidden)\n",
    "            output = F.log_softmax(output, dim = -1)\n",
    "        else:\n",
    "            output = F.log_softmax(self.h2o(hidden), dim = 0)\n",
    "        # output.shape: batch_size, output_size\n",
    "\n",
    "        return output, hidden'''\n",
    "        \n",
    "# 모델 학습\n",
    "def train_model(model, train_loader, loss_function, optimizer, ):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()                                       # 이전 배치에서 계산된 기울기 초기화\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        total += torch.numel(targets)\n",
    "        \n",
    "        correct += (predicted == targets.squeeze(1)).sum().item()\n",
    "        # plot_evaluate_results(inputs, targets, predicted)\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = epoch_loss / len(train_loader)\n",
    "    return average_loss, accuracy\n",
    "\n",
    "# 모델 검증, 평가\n",
    "def evaluate_model(model, valid_loader, loss_function):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total += torch.numel(targets)\n",
    "            \n",
    "            correct += (predicted == targets.squeeze(1)).sum().item()            \n",
    "            # plot_evaluate_results(inputs, targets, predicted)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = epoch_loss / len(valid_loader)\n",
    "    return average_loss, accuracy\n",
    "\n",
    "# 모델 실행 및 test\n",
    "def test_model(model, test_loader, criterion, label_encoder=None):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():  # gradient 계산을 하지 않도록 설정\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data  # 입력 데이터와 실제 레이블 가져오기\n",
    "            outputs = model(inputs)  # 모델을 통해 예측값 계산\n",
    "            loss = criterion(outputs, labels)  # 손실값 계산\n",
    "            test_loss += loss.item()  # 손실값 누적\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)  # 가장 높은 확률의 클래스를 예측\n",
    "            total += labels.size(0)  # 전체 데이터 수\n",
    "            correct += (predicted == labels).sum().item()  # 맞춘 개수 누적\n",
    "\n",
    "            predictions.extend(predicted.cpu().numpy())  # 예측값 저장\n",
    "            real_labels.extend(labels.cpu().numpy())  # 실제 레이블 저장\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)  # 평균 손실값 계산\n",
    "    accuracy = 100 * correct / total  # 정확도 계산\n",
    "\n",
    "    # 예측값을 레이블로 역변환 (옵션)\n",
    "    if label_encoder:\n",
    "        predictions = label_encoder.inverse_transform(predictions)\n",
    "        real_labels = label_encoder.inverse_transform(real_labels)\n",
    "\n",
    "    return avg_loss, accuracy, predictions, real_labels  # 테스트 결과 반환\n",
    "\n",
    "# 검증 결과시각화\n",
    "# def plot_evaluate_results(inputs, targets, predicted):\n",
    "#     inputs = inputs.cpu().numpy()  # GPU에서 CPU로 데이터 이동 및 numpy 변환\n",
    "#     targets = targets.cpu().numpy()\n",
    "#     predicted = predicted.cpu().numpy()\n",
    "\n",
    "#     fig, axes = plt.subplots(1, figsize=(15, 5))  # 샘플 갯수만큼 서브플롯 생성\n",
    "#     for i, ax in enumerate(axes):\n",
    "#         ax.imshow(inputs[i].reshape(28, 28), cmap='gray')  # 입력 데이터를 28x28로 가정\n",
    "#         ax.set_title(f'True: {targets[i]}, Pred: {predicted[i]}')\n",
    "#         ax.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "def main():\n",
    "    # 하이퍼파라미터\n",
    "    pass\n",
    "\n",
    "# 모델 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    DATA_SET = {\n",
    "        'FNN' : CustomDataset1,\n",
    "        'RNN' : 1,  \n",
    "        'model3' : '클래스.dataset3', \n",
    "        'name_country': Name_nationality_Dataset2 # 이름 -> 국가\n",
    "    }\n",
    "    MODEL_SET = {\n",
    "        'FNN' : Feedforward_1,\n",
    "        'RNN' : RecurrentNeuralNetwork,\n",
    "        'model3' : '클래스.dataset3', \n",
    "    }\n",
    "    # Hyperparameters\n",
    "    NUM_SAMPLES = 100000\n",
    "    BATCH_SIZE = 32\n",
    "    TRAIN_RATIO, VALID_RATIO = 0.7, 0.3\n",
    "    \n",
    "    VOCAB_SIZE = 28 # 2, 28\n",
    "    HIDDEN_DIM = 32\n",
    "    OUTPUT_DIM = 18 # 2, 18\n",
    "    \n",
    "    SEQ_LENGTH = 50  # Length of the input sequence before the delimiter\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Device configuration print # cuda/cpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    # 모델 초기화\n",
    "    fnn = MODEL_SET['FNN'](input_size=VOCAB_SIZE,\n",
    "                           hidden_size=HIDDEN_DIM, output_size=OUTPUT_DIM).to(device)\n",
    "    rnn = MODEL_SET['RNN'](input_size=VOCAB_SIZE,\n",
    "                           hidden_size=HIDDEN_DIM, output_size=OUTPUT_DIM).to(device)\n",
    "    # 옵티마이저\n",
    "    optimizers = {\n",
    "        \"FNN\" : optim.Adam(fnn.parameters(), lr = LEARNING_RATE),\n",
    "        \"RNN\" : optim.Adam(rnn.parameters(), lr = LEARNING_RATE), }\n",
    "    # criterion(loss function) 정의\n",
    "    loss_function = {\n",
    "        'cross_entropy': lambda x, y: torch.nn.functional.cross_entropy(x, y.squeeze().long()),\n",
    "        'CrossEntropyLoss': nn.CrossEntropyLoss(),\n",
    "        'MSELoss': nn.MSELoss(),\n",
    "        'NLLLoss': nn.NLLLoss(), } # 모델의 예측과 실제 값 간의 차이를 측정 : 다중 클래스 분류문제에서 모델성능평가에 사용되는 중요 손실함수\n",
    "    loss_history = {\n",
    "        'FNN_model': {'train': [], 'valid': []},\n",
    "        'RNN_model': {'train': [], 'valid': []}, }\n",
    "    accuracy_history = {\n",
    "        'FNN_model': {'train': [], 'valid': []},\n",
    "        'RNN_model': {'train': [], 'valid': []}, }\n",
    "    \n",
    "    \n",
    "    # CustomDataLoader 객체 생성, 데이터셋 클래스를 전달\n",
    "    data_set = CustomDataLoader(DATA_SET['name_country'],\n",
    "                     train_ratio=TRAIN_RATIO, valid_ratio=VALID_RATIO,\n",
    "                     batch_size=BATCH_SIZE)\n",
    "    train_loader = data_set.train_loader\n",
    "    valid_loader = data_set.valid_loader\n",
    "    test_loader = data_set.test_loader\n",
    "    \n",
    "    # 학습시작\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "        # train FNN\n",
    "        train_loss, train_accuracy = train_model(fnn, train_loader, loss_function['NLLLoss'], optimizers[\"FNN\"])\n",
    "        valid_loss, valid_accuracy  = evaluate_model(fnn, valid_loader, loss_function['NLLLoss'])\n",
    "        loss_history['FNN_model']['train'].append(train_loss)\n",
    "        loss_history['FNN_model']['valid'].append(valid_loss)\n",
    "        accuracy_history['FNN_model']['train'].append(train_accuracy)\n",
    "        accuracy_history['FNN_model']['valid'].append(valid_accuracy)\n",
    "        print(f'| FNN | Train Loss: {train_loss:.5f} | Valid Loss: {valid_loss:.5f} \\\n",
    "            | FNN | Train Accuracy: {train_accuracy:.3f}% | Valid Accuracy: {valid_accuracy:.3f}%')\n",
    "        # train RNN\n",
    "        train_loss, train_accuracy = train_model(rnn, train_loader, loss_function['NLLLoss'], optimizers[\"RNN\"])\n",
    "        valid_loss, valid_accuracy  = evaluate_model(rnn, valid_loader, loss_function['NLLLoss'])\n",
    "        loss_history['RNN_model']['train'].append(train_loss)\n",
    "        loss_history['RNN_model']['valid'].append(valid_loss)\n",
    "        accuracy_history['RNN_model']['train'].append(train_accuracy)\n",
    "        accuracy_history['RNN_model']['valid'].append(valid_accuracy)\n",
    "        print(f'| RNN | Train Loss: {train_loss:.5f} | Valid Loss: {valid_loss:.5f} \\\n",
    "            | RNN | Train Accuracy: {train_accuracy:.3f}% | Valid Accuracy: {valid_accuracy:.3f}%')\n",
    "    \n",
    "    # 결과 시각화\n",
    "    plt.figure(figsize=(12,5))\n",
    "    # 손실 그래프\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for model in loss_history:\n",
    "        plt.plot(loss_history[model]['train'], label=f'{model} train')\n",
    "        plt.plot(loss_history[model]['valid'], label=f'{model} valid')\n",
    "        \n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend(); plt.grid(True)\n",
    "    # 정확도 그래프\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for model in accuracy_history:\n",
    "        plt.plot(accuracy_history[model]['train'], label=f'{model} train')\n",
    "        plt.plot(accuracy_history[model]['valid'], label=f'{model} valid')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracies')\n",
    "    plt.legend(); plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 손실 계산\n",
    "    print(\"validation loss\")\n",
    "    for model in loss_history:\n",
    "        x= loss_history[model]['valid'][-1]\n",
    "        print(f'{model}: {x:.5f}')\n",
    "        plot_loss_history(loss_history[model]['valid'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 모듈 도전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 의존성 라이브러리 & 유틸리티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import string\n",
    "import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BaseDataset 상속클래스: 공통적으로 사용할 기능을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data_path, batch_size, shuffle, valid_ratio, test_ratio, \n",
    "                 num_workers, ran_split=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        batch_size (int): 배치 크기\n",
    "        shuffle (bool): 데이터 셔플 여부\n",
    "        valid_ratio (float): 검증 데이터 비율\n",
    "        test_split (float): 테스트 데이터 비율\n",
    "        num_workers (int): DataLoader에서 사용할 워커 수\n",
    "        random_split (bool): 랜덤으로 데이터 분할 여부\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.valid_ratio = valid_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.ran_split = ran_split\n",
    "    \n",
    "    def load_data(self, data_path): # 상속하고 덮어써서 데이터 로딩메서드 구현\n",
    "        raise NotImplementedError(\"데이터 로딩 로직은 자식 클래스에서 구현해야 합니다.\")\n",
    "    \n",
    "    def create_dataloader(self, dataset):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size,\n",
    "                          shuffle=self.shuffle, num_workers=self.num_workers)\n",
    "        \n",
    "    def preprocess(self, data):\n",
    "        # 상속하고 덮어써서  데이터 전처리메서드 구현\n",
    "        return data\n",
    "    \n",
    "    def preprocess_inference(self, single_data): # 단일데이터 전처리 추론\n",
    "        processed_data = self.preprocess(single_data) # 덮어쓴 전처리 메서드 호출\n",
    "        # 여기서 모델 예측을 호출하는 부분은 추상화해 둠 (추론에 필요한 로직)\n",
    "        # 예시: label = self.model(processed_data)\n",
    "        # 가상의 예측 값 반환 (실제 구현에 맞게 수정해야 함)\n",
    "        label = self.get_label_from_prediction(processed_data)\n",
    "        return label\n",
    "\n",
    "    def split_data(self, data):\n",
    "        total_len = len(data)\n",
    "        if self.valid_ratio + self.test_ratio >= 1:\n",
    "            raise ValueError(\"Split ratio sum cannot be >= 1\")\n",
    "        \n",
    "        test_len = int(total_len * self.test_ratio)\n",
    "        val_len = int(total_len * self.valid_ratio)\n",
    "        train_len = total_len - val_len - test_len\n",
    "        \n",
    "        if self.shuffle:\n",
    "            random.shuffle(data)\n",
    "        \n",
    "        train_data = data[:train_len]\n",
    "        val_data = data[train_len:train_len + val_len]\n",
    "        test_data = data[train_len + val_len:train_len + val_len + test_len]\n",
    "        return train_data, val_data, test_data\n",
    "\n",
    "    def get_label_from_prediction(self, processed_data):\n",
    "        # 예측값에서 레이블을 반환하는 메서드 (임시 구현).\n",
    "        # 실제 모델 예측을 사용하는 로직으로 대체\n",
    "        return \"PredictedLabel\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 자식 데이터클래스 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPNameDataset(BaseDataset):\n",
    "    def __init__(self, data_path, batch_size, shuffle, valid_ratio, test_ratio, \n",
    "                 num_workers, ran_split=True):\n",
    "        super().__init__(data_path, batch_size, shuffle, valid_ratio, test_ratio, num_workers, ran_split)\n",
    "        self.all_categories = []  # 카테고리 이름을 저장할 리스트\n",
    "        self.category_names = {}  # 카테고리와 해당 단어를 저장할 딕셔너리\n",
    "        self.alphabets = []\n",
    "        \n",
    "        self.train_dataloader, self.valid_dataloader, self.test_dataloader = self.preprocess()\n",
    "        \n",
    "        # self.train_tensor, self.val_tensor, self.test_tensor = self.split_data(self.data)  # 전처리 된 데이터 분할\n",
    "        \n",
    "        # self.train_loader = self.create_dataloader(self.train_tensor)  # 데이터로더 생성\n",
    "        # self.val_loader = self.create_dataloader(self.val_data)  # 검증 데이터로더 생성\n",
    "        # self.test_loader = self.create_dataloader(self.test_tensor)\n",
    "        \n",
    "    def load_data(self):\n",
    "        data = []\n",
    "        file_pattern = '*.txt'\n",
    "        data_path = os.path.join(self.data_path, file_pattern)  # ./data/raw/names/*.txt\n",
    "        files = glob.glob(data_path)\n",
    "        \n",
    "        for file in files:\n",
    "            category = file.split('\\\\')[-1].split('.')[0]\n",
    "            with open(file) as f:\n",
    "                words = f.read().strip().split('\\n')\n",
    "        \n",
    "            self.all_categories.append(category)\n",
    "            self.category_names[category] = words\n",
    "        \n",
    "            data = [[word, category] for word in words if len(word) <= max_word_length]\n",
    "        print(f\"NLPNameDataset.load_data | Raw Data from {self.data_path}: {self.all_categories} | data sample : {self.category_names[category][:10]}\")\n",
    "        return data, self.all_categories # [[word, category], ...]\n",
    "\n",
    "    def preprocess(self):\n",
    "        data, languages = self.load_data()\n",
    "        alphabets = self.determine_alphabets(data)\n",
    "        \n",
    "        for idx, elem in enumerate(data):\n",
    "            tmp = []\n",
    "            for char in elem[0]:\n",
    "                if char.lower() in alphabets:\n",
    "                    tmp.append(char.lower())\n",
    "                else:\n",
    "                    tmp.append(OOV)\n",
    "            # print('alphabets', alphabets)\n",
    "            data[idx][0] = self.word2tensor(tmp, max_word_length, alphabets)\n",
    "            data[idx][1] = languages.index(data[idx][1])\n",
    "\n",
    "        x = [e[0] for e in data]\n",
    "        y = [self.category_encoding(e[1], len(self.all_categories)) for e in data]\n",
    "\n",
    "        train_x, valid_x, test_x = self.split_data(x)\n",
    "        train_y, valid_y, test_y = self.split_data(y)\n",
    "        print(\"데이터셋 preprocess train_x[0]\", train_x[0])\n",
    "        train_x = torch.stack(train_x)\n",
    "        train_y = torch.stack(train_y)\n",
    "        valid_x = torch.stack(valid_x)\n",
    "        valid_y = torch.stack(valid_y)\n",
    "        test_x = torch.stack(test_x)\n",
    "        test_y = torch.stack(test_y)\n",
    "\n",
    "        train_dataset = TensorDataset(train_x, train_y)\n",
    "        valid_dataset = TensorDataset(valid_x, valid_y)\n",
    "        test_dataset = TensorDataset(test_x, test_y)\n",
    "        print(f'데이터로더 클래스torch.stack(train_x).shape: {train_x.shape}')\n",
    "        print(f'데이터 클래스 torch.stack(train_y).shape: {train_y.shape}')\n",
    "        \n",
    "        train_dataloader = self.create_dataloader(train_dataset)\n",
    "        valid_dataloader = self.create_dataloader(valid_dataset)\n",
    "        test_dataloader = self.create_dataloader(test_dataset)\n",
    "        return train_dataloader, valid_dataloader, test_dataloader\n",
    "    \n",
    "    def category_encoding(self, label_index, num_classes):\n",
    "        return torch.eye(num_classes)[label_index]  # num_classes 크기의 단위 행렬에서 해당 인덱스를 추출\n",
    "    \n",
    "    def letter2tensor(self, char, alphabets):\n",
    "        res = [0 for _ in range(len(alphabets))]\n",
    "        if char in alphabets:\n",
    "            idx = alphabets.index(char)\n",
    "        else:\n",
    "            idx = alphabets.index(OOV)\n",
    "        res[idx] = 1\n",
    "        return torch.tensor(res)\n",
    "    \n",
    "    def word2tensor(self, word, max_word_length, alphabets):\n",
    "        # return torch.tensor with size (max_length, len(alphabets))\n",
    "        res = torch.zeros(max_word_length, len(alphabets))\n",
    "        \n",
    "        for idx, char in enumerate(word):\n",
    "            if idx < max_word_length:\n",
    "                res[idx] = self.letter2tensor(char, alphabets)\n",
    "        for i in range(max_word_length - len(word)):\n",
    "            res[len(word) + i] = self.letter2tensor(PAD, alphabets)\n",
    "        return res\n",
    "    \n",
    "    def determine_alphabets(self, data, pad=PAD, oov=OOV, threshold = 0.999): # oov pad 비식별 문자셋 추가\n",
    "        # data = list of [name, language_name]\n",
    "        lst = []\n",
    "        character_dict = defaultdict(int)\n",
    "        \n",
    "        for name, lang in data:\n",
    "            for char in name:\n",
    "                character_dict[char.lower()] += 1\n",
    "        for k, v in character_dict.items():\n",
    "            lst.append((k, v))\n",
    "        # (문자,빈도) 형태로 lst에 내림차순 정렬\n",
    "        lst = sorted(lst, key = lambda x:x[1], reverse = True)\n",
    "        total_count = sum(e[1] for e in lst)\n",
    "        # 빈도를 누적(s)하며, 전체 빈도 중에서 99.9% 이하의 문자를 선택하여 alphabets 리스트에 추가\n",
    "        s = 0 # 빈도 누적값 99.9%의문자가 선택되면 추가를 종료\n",
    "        alphabets = []\n",
    "        for k, v in lst:\n",
    "            s += v\n",
    "            if s < threshold * total_count:\n",
    "                alphabets.append(k)\n",
    "        \n",
    "        alphabets.append(pad)\n",
    "        alphabets.append(oov)\n",
    "        self.alphabets = alphabets\n",
    "        print(f'determine_alphabets: {len(alphabets)} 개 ', alphabets)\n",
    "        print('INPUT_DIM:',len(alphabets))\n",
    "        return alphabets\n",
    "    \n",
    "    def decode(self, one_hot_tensor):\n",
    "        \"\"\"원핫 인코딩된 텐서를 카테고리 레이블로 변환하는 메서드\"\"\"\n",
    "        # 가장 큰 값의 인덱스를 찾아서 카테고리 이름으로 변환\n",
    "        predicted_index = torch.argmax(one_hot_tensor, dim=1).item()\n",
    "        return self.all_categories[predicted_index]  # 카테고리 이름 반환\n",
    "    \n",
    "    def __len__(self): # 데이터의 총 샘플 수를 반환\n",
    "        return len(self.category_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.category_names[idx]\n",
    "    \n",
    "def indices_to_one_hot(tensor, vocab_size):\n",
    "    # 텐서가 정수형 인덱스가 아니면 타입 변환\n",
    "    if not tensor.dtype in [torch.int32, torch.int64]:\n",
    "        tensor = tensor.long()  # 정수형 타입으로 변환\n",
    "    # 마지막 차원을 제외한 입력 텐서의 모양을 가져옵니다.\n",
    "    # 이미 원-핫 인코딩된 경우 확인 (마지막 차원이 vocab_size인지 확인)\n",
    "    if tensor.ndim >= 2 and tensor.shape[-1] == vocab_size:\n",
    "        print(\"Tensor is already one-hot encoded.\")\n",
    "        return tensor.float()  # 그대로 반환\n",
    "    shape = tensor.shape[:-1]\n",
    "    print(\"Input shape:\", shape)\n",
    "    # (어휘 색인을 포함하는) 마지막 차원을 가져옵니다.\n",
    "    # last_dim = tensor.shape[-1]\n",
    "    # print(last_dim.shape)\n",
    "    # 마지막 차원을 제외하고 텐서를 평면화합니다.\n",
    "    tensor_flat = tensor.view(-1,)# last_dim)\n",
    "    print(\"Flattened tensor shape:\", tensor_flat.shape)\n",
    "    # 다음을 사용하여 원-핫 인코딩된 텐서를 생성합니다. shape (total_elements, vocab_size)\n",
    "    one_hot_flat = torch.nn.functional.one_hot(tensor_flat, num_classes=vocab_size)\n",
    "    print(\"One-hot encoded shape (flat):\", one_hot_flat.shape)\n",
    "    # 원-핫 인코딩된 텐서를 다시 원래 크기로 재구성합니다.\n",
    "    one_hot_tensor = one_hot_flat.view(*shape, vocab_size) # *shape, last_dim, vocab_size)\n",
    "    print(\"One-hot encoded shape (reshaped):\", one_hot_tensor.shape)\n",
    "    return one_hot_tensor.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CustomPreprocessing 개별 데이터 전처리 기능 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교육과정중 전처리 과정은 같은목적으로 모델만 돌려쓰며 대동소이하므로 그냥 dataset클래스에 합쳐버린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 PreprocessingDataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 모델아키텍처 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 이름 -> 국가 분류 모델 RNNManual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ===================================\n",
    "    Manual Recurrent Neural Network (RNN)\n",
    "    ===================================\n",
    "    Architecture:\n",
    "        Input -> Input-to-Hidden (Linear) -> Tanh Activation -> Hidden-to-Hidden (Linear) -> Output (Linear)\n",
    "    Formula:\n",
    "        h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b_h)\n",
    "        o_t = W_ho * h_t + b_o\n",
    "    설명:\n",
    "        이 클래스는 PyTorch의 내장 RNN 모듈을 사용하지 않고 간단한 RNN을 수동으로 구현합니다.\n",
    "        선형 레이어를 사용하여 숨겨진 상태와 출력을 계산합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RNNManual, self).__init__()\n",
    "        self.vocab_size = INPUT_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        \n",
    "        self.W_ih = nn.Linear(INPUT_DIM, HIDDEN_DIM)  # Input to hidden\n",
    "        self.W_hh = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)  # Hidden to hidden\n",
    "        self.W_ho = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)  # Hidden to output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual RNN.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        # x = indices_to_one_hot(x, self.vocab_size)\n",
    "        print('RNNManual x.shape',x.shape)\n",
    "        h_t = torch.zeros(HIDDEN_DIM).to(x.device) # BATCH_SIZE, 나머지 확인\n",
    "        outputs = []\n",
    "        for t in range(max_word_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            print('x_t.shape ', x_t.shape)\n",
    "            print('self.W_ih(x_t).shape ', self.W_ih(x_t).shape )\n",
    "            print('self.W_hh(h_t).shape ', self.W_hh(h_t).shape )\n",
    "            h_t = torch.tanh(self.W_ih(x_t) + self.W_hh(h_t))  # [batch_size, hidden_dim]\n",
    "            o_t = self.W_ho(h_t)  # [batch_size, output_dim]\n",
    "            outputs.append(o_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 이름 -> 국가 분류 모델 RNNBuiltIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    Built-In Recurrent Neural Network (RNN)\n",
    "    =======================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.RNN] -> [Linear Output]\n",
    "    Formula:\n",
    "        h_t = RNN(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.RNN` 모듈을 활용합니다.\n",
    "        'nn.RNN' 내에 반복 작업을 캡슐화하여 모델 정의를 단순화합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RNNBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(INPUT_DIM, EMBEDDING_DIM)\n",
    "        self.rnn = nn.RNN(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in RNN.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        print('RNNBuiltIn:', x.shape)\n",
    "        embedded = self.embedding(x.long())  # [batch_size, seq_length, embedding_dim]\n",
    "        print('embedded.shape:', embedded.shape)\n",
    "        output, hidden = self.rnn(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 이름 -> 국가 분류 모델 LSTMManual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMManual(nn.Module):\n",
    "    \"\"\"\n",
    "    ========================================\n",
    "    Manual Long Short-Term Memory (LSTM)\n",
    "    ========================================\n",
    "    Architecture:\n",
    "        Input -> Input Gates (Linear Layers) -> LSTM Cell -> Output (Linear)\n",
    "    Formula:\n",
    "        i_t = sigmoid(W_ii * x_t + W_hi * h_{t-1} + b_i)\n",
    "        f_t = sigmoid(W_if * x_t + W_hf * h_{t-1} + b_f)\n",
    "        g_t = tanh(W_ig * x_t + W_hg * h_{t-1} + b_g)\n",
    "        o_t = sigmoid(W_io * x_t + W_ho * h_{t-1} + b_o)\n",
    "        c_t = f_t * c_{t-1} + i_t * g_t\n",
    "        h_t = o_t * tanh(c_t)\n",
    "        y_t = W_ho * h_t + b_y\n",
    "    Description:\n",
    "        This class manually implements an LSTM cell without using PyTorch's built-in LSTM modules.\n",
    "        It includes input, forget, cell, and output gates to regulate information flow.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LSTMManual, self).__init__()\n",
    "        self.vocab_size = INPUT_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        # Input gate components\n",
    "        self.W_ii = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hi = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Forget gate components\n",
    "        self.W_if = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hf = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Cell gate components\n",
    "        self.W_ig = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hg = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Output gate components\n",
    "        self.W_io = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_ho = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Output layer\n",
    "        self.W_yo = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual LSTM.\n",
    "        1. 데이터 세트 생성: 임베딩 레이어를 사용하지 않고 복사 작업을 위한 합성 데이터 세트를 생성합니다.\n",
    "        우리는 각 모델 유형에 대해 두 가지 버전을 구현합니다.\n",
    "        - 수동 구현: `nn.Linear` 레이어를 사용하여 모델 방정식을 직접 구현합니다.\n",
    "        - 내장 구현: PyTorch의 내장 `nn.RNN`, `nn.LSTM` 및 `nn.GRU` 모듈을 활용합니다.\n",
    "        단순화된 모델 정의를 위한 임베딩 레이어가 있습니다.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        print('RNNManual x.shape',x.shape)\n",
    "        h_t = torch.zeros(BATCH_SIZE, self.hidden_dim).to(x.device)\n",
    "        c_t = torch.zeros(BATCH_SIZE, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(max_word_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            i_t = torch.sigmoid(self.W_ii(x_t) + self.W_hi(h_t))  # Input gate\n",
    "            f_t = torch.sigmoid(self.W_if(x_t) + self.W_hf(h_t))  # Forget gate\n",
    "            g_t = torch.tanh(self.W_ig(x_t) + self.W_hg(h_t))     # Cell gate\n",
    "            o_t = torch.sigmoid(self.W_io(x_t) + self.W_ho(h_t))  # Output gate\n",
    "            c_t = f_t * c_t + i_t * g_t                            # Cell state\n",
    "            h_t = o_t * torch.tanh(c_t)                            # Hidden state\n",
    "            y_t = self.W_yo(h_t)                                    # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 이름 -> 국가 분류 모델 LSTMBuiltIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ==========================================\n",
    "    Built-In Long Short-Term Memory (LSTM)\n",
    "    ==========================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.LSTM] -> [Linear Output]\n",
    "    Formula:\n",
    "        (h_t, c_t) = LSTM(x_t, (h_{t-1}, c_{t-1}))\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.LSTM` 모듈을 활용합니다.\n",
    "        LSTM 게이트의 복잡성을 추상화하여 반복 작업을 위한 간소화된 인터페이스를 제공합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LSTMBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(INPUT_DIM, EMBEDDING_DIM)\n",
    "        self.lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in LSTM.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # output: [batch, seq, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 이름 -> 국가 분류 모델 GRUManual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUManual(nn.Module):\n",
    "    \"\"\"\n",
    "    =======================================\n",
    "    수동 게이트 순환 장치(GRU)\n",
    "    =======================================\n",
    "    Architecture:\n",
    "        Input -> Update Gate (Linear Layers) -> Reset Gate (Linear Layers) -> GRU Cell -> Output (Linear)\n",
    "    공식:\n",
    "        z_t = sigmoid(W_iz * x_t + W_hz * h_{t-1} + b_z)\n",
    "        r_t = sigmoid(W_ir * x_t + W_hr * h_{t-1} + b_r)\n",
    "        n_t = tanh(W_in * x_t + W_hn * (r_t * h_{t-1}) + b_n)\n",
    "        h_t = (1 - z_t) * n_t + z_t * h_{t-1}\n",
    "        y_t = W_ho * h_t + b_y\n",
    "    설명:\n",
    "        이 클래스는 PyTorch의 내장 GRU 모듈을 사용하지 않고 GRU 셀을 수동으로 구현합니다.\n",
    "        여기에는 정보 흐름을 제어하는 ​​업데이트 및 재설정 게이트가 포함됩니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GRUManual, self).__init__()\n",
    "        self.vocab_size = INPUT_DIM\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        # Update gate\n",
    "        self.W_iz = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hz = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Reset gate\n",
    "        self.W_ir = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hr = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # New gate\n",
    "        self.W_in = nn.Linear(INPUT_DIM, HIDDEN_DIM)\n",
    "        self.W_hn = nn.Linear(HIDDEN_DIM, HIDDEN_DIM)\n",
    "        # Output layer\n",
    "        self.W_ho = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the manual GRU.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, vocab_size]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        print('RNNManual x.shape',x.shape)\n",
    "        h_t = torch.zeros(BATCH_SIZE, self.hidden_dim).to(x.device)\n",
    "        outputs = []\n",
    "        for t in range(max_word_length):\n",
    "            x_t = x[:, t, :]  # [batch_size, vocab_size]\n",
    "            z_t = torch.sigmoid(self.W_iz(x_t) + self.W_hz(h_t))  # Update gate\n",
    "            r_t = torch.sigmoid(self.W_ir(x_t) + self.W_hr(h_t))  # Reset gate\n",
    "            n_t = torch.tanh(self.W_in(x_t) + self.W_hn(r_t * h_t))  # New gate\n",
    "            h_t = (1 - z_t) * n_t + z_t * h_t  # Hidden state\n",
    "            y_t = self.W_ho(h_t)  # Output\n",
    "            outputs.append(y_t.unsqueeze(1))\n",
    "        outputs = torch.cat(outputs, dim=1)  # [batch_size, seq_length, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 이름 -> 국가 분류 모델 GRUBuiltIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUBuiltIn(nn.Module):\n",
    "    \"\"\"\n",
    "    ======================================\n",
    "    Built-In Gated Recurrent Unit (GRU)\n",
    "    ======================================\n",
    "    Architecture:\n",
    "        [Embedding] -> [nn.GRU] -> [Linear Output]\n",
    "    Formula:\n",
    "        h_t = GRU(x_t, h_{t-1})\n",
    "        y_t = W_ho * h_t + b_o\n",
    "    Description:\n",
    "        이 클래스는 임베딩 레이어와 함께 PyTorch의 내장 `nn.GRU` 모듈을 사용합니다.\n",
    "        이는 GRU의 게이팅 메커니즘을 캡슐화하여 반복 작업의 효율적인 구현을 제공합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GRUBuiltIn, self).__init__()\n",
    "        self.embedding = nn.Embedding(INPUT_DIM, EMBEDDING_DIM)\n",
    "        self.gru = nn.GRU(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the built-in GRU.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length]\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape [batch_size, seq_length, output_dim]\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_length, embedding_dim]\n",
    "        output, hidden = self.gru(embedded)  # output: [batch, seq, hidden], hidden: [1, batch, hidden]\n",
    "        output = self.fc(output)  # [batch, seq, output_dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n데이터로드 및 전처리\\n데이터셋 클래스 정의\\ndataloader구현\\ndata분할\\n그냥 1번에서 구현해보고 나중에 정리\\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "데이터로드 및 전처리\n",
    "데이터셋 클래스 정의\n",
    "dataloader구현\n",
    "data분할\n",
    "그냥 1번에서 구현해보고 나중에 정리\n",
    "'''\n",
    "# 데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 및 전처리 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        print('train_model | inputs.shape', inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        print('train_model | outputs.shape, targets.shape', outputs.shape, targets.shape)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    average_loss = epoch_loss / len(dataloader)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "            outputs = model(inputs)  # 모델 예측\n",
    "            loss = criterion(outputs, targets)  # 손실 계산\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, dim=1)\n",
    "            total_samples += torch.numel(targets)\n",
    "            \n",
    "            predicted = (outputs > threshold)  # 예측 클래스\n",
    "            correct_predictions += (predicted == targets.squeeze(1)).sum().item() # 정답과 비교 squeeze 필요없을듯한\n",
    "    \n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    average_loss = total_loss / len(valid_loader)\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론\n",
    "def inference(model, dataloader):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산을 하지 않도록 설정\n",
    "        for batch in dataloader:\n",
    "            inputs = batch  # 배치에서 입력만 사용\n",
    "            outputs = model(inputs)  # 모델 예측\n",
    "            predicted_classes = (outputs > evaluation_config['threshold']).float()  # 클래스 예측\n",
    "            predictions.append(predicted_classes)\n",
    "\n",
    "    return torch.cat(predictions)  # 예측 결과를 연결하여 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 예시\\nfrom torch.utils.data import Dataset, DataLoader\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\n\\n# 데이터셋 클래스 정의\\nclass CustomDataset(Dataset):\\n    def __init__(self, config):\\n        self.data = pd.read_csv(config[\\'data_path\\'])\\n        self.features = self.data[config[\\'input_features\\']].values\\n        self.labels = self.data[config[\\'target\\']].values\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        return self.features[idx], self.labels[idx]\\n\\n# 데이터셋 및 DataLoader 준비\\ndataset = CustomDataset(data_config[\\'dataset_1\\'])\\ndataloader = DataLoader(dataset, batch_size=data_config[\\'dataset_1\\'][\\'batch_size\\'], \\n                        shuffle=data_config[\\'dataset_1\\'][\\'shuffle\\'], \\n                        num_workers=data_config[\\'dataset_1\\'][\\'num_workers\\'])\\n\\n# 모델 정의\\nclass SimpleNN(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        super(SimpleNN, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x):\\n        x = torch.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n\\n# 모델 초기화\\nmodel = SimpleNN(input_dim=model_config[\\'SimpleNN\\'][\\'input_dim\\'], \\n                 hidden_dim=model_config[\\'SimpleNN\\'][\\'hidden_dim\\'], \\n                 output_dim=model_config[\\'SimpleNN\\'][\\'output_dim\\'])\\n\\n# 훈련 함수 정의\\ndef train_model(model, dataloader, config):\\n    model.train()\\n    criterion = nn.MSELoss() if config[\\'loss_function\\'] == \\'mse\\' else nn.CrossEntropyLoss()\\n    optimizer = optim.Adam(model.parameters(), lr=config[\\'learning_rate\\'])\\n\\n    for epoch in range(config[\\'num_epochs\\']):\\n        for inputs, labels in dataloader:\\n            optimizer.zero_grad()\\n            outputs = model(torch.FloatTensor(inputs))  # 입력 데이터를 텐서로 변환\\n            loss = criterion(outputs, torch.FloatTensor(labels))\\n            loss.backward()\\n            optimizer.step()\\n        print(f\\'Epoch [{epoch+1}/{config[\"num_epochs\"]}], Loss: {loss.item():.4f}\\')\\n\\n# 모델 훈련\\ntrain_model(model, dataloader, training_config)\\n\\n# 평가 함수 정의\\ndef evaluate_model(model, dataloader, config):\\n    model.eval()\\n    metrics = {metric: 0 for metric in config[\\'metrics\\']}\\n    \\n    with torch.no_grad():\\n        for inputs, labels in dataloader:\\n            outputs = model(torch.FloatTensor(inputs))\\n            if \\'mse\\' in config[\\'metrics\\']:\\n                metrics[\\'mse\\'] += nn.MSELoss()(outputs, torch.FloatTensor(labels)).item()\\n    \\n    metrics[\\'mse\\'] /= len(dataloader)\\n    print(f\\'Evaluation Metrics: {metrics}\\')\\n\\n# 모델 평가\\nevaluate_model(model, dataloader, evaluation_config)'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config = {\n",
    "    'dataset': 'NLPNameDataset', # LoaderDataset 지정\n",
    "    'NLPNameDataset': {\n",
    "        'DATA_PATH': './project_reference/data/raw/names/', # 데이터 파일 경로 설정\n",
    "        'BATCH_SIZE': 32,  # 배치 크기 설정\n",
    "        'VALID_RATIO': 0.15,\n",
    "        'TEST_RATIO': 0.15,\n",
    "        'SHUFFLE': True,  # 데이터 셔플 여부\n",
    "        'NUM_WORKERS': 2,  # DataLoader에 사용할 프로세스 수\n",
    "        'ran_split': True,\n",
    "        'max_word_length': 10,  # 이름의 최대 길이 (패딩 처리를 위한 기준)\n",
    "        'PAD': '[PAD]',  # 패딩에 사용할 토큰\n",
    "        'OOV': '[OOV]',  # OOV 토큰 (사전에 없는 단어 처리)\n",
    "        'input_type': 'name',  # 입력 피처 유형 (이름 데이터)\n",
    "        'target_type': 'country',  # 타겟 레이블 유형 (국가명)\n",
    "        'vocab': list(string.ascii_lowercase) + ['?', '_'],  # 알파벳, OOV, PAD 토큰 포함한 사전\n",
    "    },\n",
    "    'dataset_2': {\n",
    "        'data_path': '../data/dataset2.csv',\n",
    "        'batch_size': 64,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4,\n",
    "        'input_features': ['featureA', 'featureB'],\n",
    "        'target': 'target_label',\n",
    "    },\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    'models': [ # 실행될 모델 순번 지정\n",
    "        'RNNManual','RNNBuiltIn','LSTMManual','LSTMBuiltIn', 'GRUManual', 'GRUBuiltIn'\n",
    "        ],\n",
    "    'RNNManual': {\n",
    "        'EMBEDDING_DIM': 10,\n",
    "        'INPUT_DIM': 0,  # 입력 차원 \n",
    "        # dataconfig에서 변수를 받아 dataloader를 만든 뒤 input과 ouput이 정해지는 구조는 사전에 모든 파라미터를 정해놓고\n",
    "        # 한번에 관리하는 취지와 맞지않지만 시간초과...\n",
    "        'HIDDEN_DIM': 64,  # 은닉층 차원\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',  # 출력 차원\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # RNN의 층 수\n",
    "    },\n",
    "    'RNNBuiltIn': {\n",
    "        'EMBEDDING_DIM': 10,  # 임베딩 차원\n",
    "        'INPUT_DIM': 0,\n",
    "        'HIDDEN_DIM': 64,  # 여러 개의 은닉층 차원\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # 다층 RNN 사용\n",
    "    },\n",
    "    'LSTMManual': {\n",
    "        'EMBEDDING_DIM': 10,  # 임베딩 차원\n",
    "        'INPUT_DIM': 0,\n",
    "        'HIDDEN_DIM': 64,\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # LSTM의 층 수\n",
    "    },\n",
    "    'LSTMBuiltIn': {\n",
    "        'EMBEDDING_DIM': 10,  # 임베딩 차원\n",
    "        'INPUT_DIM': 0,\n",
    "        'HIDDEN_DIM': 64,  # 여러 개의 은닉층 차원0\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # 다층 LSTM 사용\n",
    "    },\n",
    "    'GRUManual': {\n",
    "        'EMBEDDING_DIM': 10,  # 임베딩 차원\n",
    "        'INPUT_DIM': 0,\n",
    "        'HIDDEN_DIM': 64,  # 은닉층 차원\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # GRU의 층 수\n",
    "    },\n",
    "    'GRUBuiltIn': {\n",
    "        'EMBEDDING_DIM': 10,  # 임베딩 차원\n",
    "        'INPUT_DIM': 0,\n",
    "        'HIDDEN_DIM': 64,  # 여러 개의 은닉층 차원\n",
    "        'OUTPUT_DIM': 'len(data_loader.all_categories)',\n",
    "        'activation': 'adam',  # 활성화 함수\n",
    "        'num_layers': 1,  # 다층 GRU 사용\n",
    "    },\n",
    "}\n",
    "\n",
    "training_config = {\n",
    "    'EPOCHS': 100,  # 에폭 수\n",
    "    'LEARNING_RATE': 0.001,  # 학습률\n",
    "    'optimizers': {   # 최적화 알고리즘 main에서 명시 & 대입\n",
    "        },\n",
    "    # 'loss_function': 'mse',  # 손실 함수\n",
    "    # 'weight_decay': 0.0001,  # 가중치 감소 (L2 정규화)\n",
    "    # 'scheduler': 'ReduceLROnPlateau',  # 학습률 스케줄러\n",
    "    # 'scheduler_patience': 5,  # 학습률 감소를 위한 기다리는 에폭 수\n",
    "    # 'scheduler_factor': 0.5,  # 학습률 감소 비율\n",
    "}\n",
    "evaluation_config = {\n",
    "    'metrics': ['accuracy', 'mse', 'precision', 'recall', 'f1_score'],  # 평가 지표\n",
    "    'validation_split': 0.2,  # 검증 데이터 비율\n",
    "    'threshold': 0.5,\n",
    "    # 'early_stopping': {\n",
    "    #     'patience': 10,  # 에포크 동안 개선이 없을 경우 중단\n",
    "    #     'min_delta': 0.001,  # 개선으로 간주될 최소 변화량\n",
    "    #     'monitor': 'val_loss'  # 기준이 되는 지표 (ex. 검증 손실)\n",
    "    # },\n",
    "    # 'save_best_model': True,  # 가장 좋은 모델을 저장할지 여부\n",
    "    # 'save_path': './models/best_model.pth',  # 모델 저장 경로\n",
    "    # 'threshold': 0.5,  # 분류 문제에서 예측 확률을 기준으로 하는 임계값\n",
    "    # 'confusion_matrix': True,  # 혼동 행렬을 계산할지 여부\n",
    "    # 'roc_curve': False,  # ROC 곡선 계산 여부\n",
    "    'num_workers': 2,  # 데이터 로딩 시 사용될 프로세스 수 (dataloader 파라미터)\n",
    "}\n",
    "\n",
    "''' 예시\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, config):\n",
    "        self.data = pd.read_csv(config['data_path'])\n",
    "        self.features = self.data[config['input_features']].values\n",
    "        self.labels = self.data[config['target']].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 데이터셋 및 DataLoader 준비\n",
    "dataset = CustomDataset(data_config['dataset_1'])\n",
    "dataloader = DataLoader(dataset, batch_size=data_config['dataset_1']['batch_size'], \n",
    "                        shuffle=data_config['dataset_1']['shuffle'], \n",
    "                        num_workers=data_config['dataset_1']['num_workers'])\n",
    "\n",
    "# 모델 정의\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleNN(input_dim=model_config['SimpleNN']['input_dim'], \n",
    "                 hidden_dim=model_config['SimpleNN']['hidden_dim'], \n",
    "                 output_dim=model_config['SimpleNN']['output_dim'])\n",
    "\n",
    "# 훈련 함수 정의\n",
    "def train_model(model, dataloader, config):\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss() if config['loss_function'] == 'mse' else nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.FloatTensor(inputs))  # 입력 데이터를 텐서로 변환\n",
    "            loss = criterion(outputs, torch.FloatTensor(labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{config[\"num_epochs\"]}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 훈련\n",
    "train_model(model, dataloader, training_config)\n",
    "\n",
    "# 평가 함수 정의\n",
    "def evaluate_model(model, dataloader, config):\n",
    "    model.eval()\n",
    "    metrics = {metric: 0 for metric in config['metrics']}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(torch.FloatTensor(inputs))\n",
    "            if 'mse' in config['metrics']:\n",
    "                metrics['mse'] += nn.MSELoss()(outputs, torch.FloatTensor(labels)).item()\n",
    "    \n",
    "    metrics['mse'] /= len(dataloader)\n",
    "    print(f'Evaluation Metrics: {metrics}')\n",
    "\n",
    "# 모델 평가\n",
    "evaluate_model(model, dataloader, evaluation_config)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_file(log_file_path, headers): # 헤더기록 log파일생성\n",
    "    with open(log_file_path, 'w') as f:\n",
    "        f.write(','.join(headers) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. __main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "NLPNameDataset.load_data | Raw Data from ./project_reference/data/raw/names/: ['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese'] | data sample : ['Nguyen', 'Tron', 'Le', 'Pham', 'Huynh', 'Hoang', 'Phan', 'Vu', 'Vo', 'Dang']\n",
      "determine_alphabets: 22 개  ['a', 'n', 'h', 'u', 'o', 'i', 'g', 't', 'c', 'e', 'l', 'd', 'm', 'p', 'v', 'y', 'r', 'b', 'q', 'k', '[PAD]', '[OOV]']\n",
      "INPUT_DIM: 22\n",
      "데이터셋 preprocess train_x[0] tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0.]])\n",
      "데이터로더 클래스torch.stack(train_x).shape: torch.Size([53, 10, 22])\n",
      "데이터 클래스 torch.stack(train_y).shape: torch.Size([53, 18])\n",
      "dataloader생성 분할 직후\n",
      "문자여 모델 순회하며 변수 초기화 하기 전\n",
      "RNNManual EMBEDDING_DIM 10\n",
      "RNNManual INPUT_DIM 0\n",
      "RNNManual HIDDEN_DIM 64\n",
      "RNNManual OUTPUT_DIM len(data_loader.all_categories)\n",
      "RNNManual activation adam\n",
      "RNNManual num_layers 1\n",
      "rnnmanual= RNNManual로 모델 초기화\n",
      "RNNBuiltIn EMBEDDING_DIM 10\n",
      "RNNBuiltIn INPUT_DIM 0\n",
      "RNNBuiltIn HIDDEN_DIM 64\n",
      "RNNBuiltIn OUTPUT_DIM len(data_loader.all_categories)\n",
      "RNNBuiltIn activation adam\n",
      "RNNBuiltIn num_layers 1\n",
      "rnnbuiltin= RNNBuiltIn로 모델 초기화\n",
      "LSTMManual EMBEDDING_DIM 10\n",
      "LSTMManual INPUT_DIM 0\n",
      "LSTMManual HIDDEN_DIM 64\n",
      "LSTMManual OUTPUT_DIM len(data_loader.all_categories)\n",
      "LSTMManual activation adam\n",
      "LSTMManual num_layers 1\n",
      "lstmmanual= LSTMManual로 모델 초기화\n",
      "LSTMBuiltIn EMBEDDING_DIM 10\n",
      "LSTMBuiltIn INPUT_DIM 0\n",
      "LSTMBuiltIn HIDDEN_DIM 64\n",
      "LSTMBuiltIn OUTPUT_DIM len(data_loader.all_categories)\n",
      "LSTMBuiltIn activation adam\n",
      "LSTMBuiltIn num_layers 1\n",
      "lstmbuiltin= LSTMBuiltIn로 모델 초기화\n",
      "GRUManual EMBEDDING_DIM 10\n",
      "GRUManual INPUT_DIM 0\n",
      "GRUManual HIDDEN_DIM 64\n",
      "GRUManual OUTPUT_DIM len(data_loader.all_categories)\n",
      "GRUManual activation adam\n",
      "GRUManual num_layers 1\n",
      "grumanual= GRUManual로 모델 초기화\n",
      "GRUBuiltIn EMBEDDING_DIM 10\n",
      "GRUBuiltIn INPUT_DIM 0\n",
      "GRUBuiltIn HIDDEN_DIM 64\n",
      "GRUBuiltIn OUTPUT_DIM len(data_loader.all_categories)\n",
      "GRUBuiltIn activation adam\n",
      "GRUBuiltIn num_layers 1\n",
      "grubuiltin= GRUBuiltIn로 모델 초기화\n",
      "변수명에 동적할당전\n",
      "에폭스 시작전\n",
      "\n",
      "Epoch 1/100\n",
      "모델 실행전\n",
      "RNNManual 해당모델 실행\n",
      "RNNManual <function <lambda> at 0x000001903ED374C0> Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM: 22 64 18\n",
      "train_model | inputs.shape torch.Size([32, 10, 22]) torch.Size([32, 18])\n",
      "RNNManual x.shape torch.Size([32, 10, 22])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "x_t.shape  torch.Size([32, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([32, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([32, 64])\n",
      "train_model | outputs.shape, targets.shape torch.Size([32, 10, 18]) torch.Size([32, 18])\n",
      "train_model | inputs.shape torch.Size([21, 10, 22]) torch.Size([21, 18])\n",
      "RNNManual x.shape torch.Size([21, 10, 22])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "x_t.shape  torch.Size([21, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([21, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([21, 64])\n",
      "train_model | outputs.shape, targets.shape torch.Size([21, 10, 18]) torch.Size([21, 18])\n",
      "RNNManual x.shape torch.Size([10, 10, 22])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "x_t.shape  torch.Size([10, 22])\n",
      "self.W_ih(x_t).shape  torch.Size([10, 64])\n",
      "self.W_hh(h_t).shape  torch.Size([10, 64])\n",
      "| RNNManual | Train Loss: 2.2980\n",
      "| Val Loss: 2.2724 | Val accuracy: 944.4444\n",
      "RNNBuiltIn 해당모델 실행\n",
      "RNNBuiltIn <function <lambda> at 0x000001903ED374C0> Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM: 22 64 18\n",
      "train_model | inputs.shape torch.Size([32, 10, 22]) torch.Size([32, 18])\n",
      "RNNBuiltIn: torch.Size([32, 10, 22])\n",
      "embedded.shape: torch.Size([32, 10, 22, 10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RNN: Expected input to be 2D or 3D, got 4D tensor instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[241], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(model, criterion , optimizer)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINPUT_DIM, HIDDEN_DIM, OUTPUT_DIM:\u001b[39m\u001b[38;5;124m'\u001b[39m, INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n\u001b[1;32m---> 98\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(current_model, val_loader, criterion)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loss_history:\n",
      "Cell \u001b[1;32mIn[192], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_model | inputs.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape, targets\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_model | outputs.shape, targets.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape, targets\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[239], line 32\u001b[0m, in \u001b[0;36mRNNBuiltIn.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x\u001b[38;5;241m.\u001b[39mlong())  \u001b[38;5;66;03m# [batch_size, seq_length, embedding_dim]\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedded.shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, embedded\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 32\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# output: [batch, seq, hidden], hidden: [1, batch, hidden]\u001b[39;00m\n\u001b[0;32m     33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)  \u001b[38;5;66;03m# [batch, seq, output_dim]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:561\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    559\u001b[0m batch_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    562\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    563\u001b[0m batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: RNN: Expected input to be 2D or 3D, got 4D tensor instead"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = data_config['dataset'] # 원하는 데이터셋 입력\n",
    "    models = model_config['models']\n",
    "    # data_config[dataset]에 있는 모든 키-값 쌍을 전역 변수로 선언 1\n",
    "    # for key, value in data_config[dataset].items(): # 비효율적인 뻘짓\n",
    "    #     globals()[key] = value\n",
    "    # data_config[dataset]에 있는 모든 키-값 쌍을 전역 변수로 선언 2\n",
    "    globals().update(data_config[dataset])\n",
    "    globals().update(training_config)\n",
    "    globals().update(evaluation_config)\n",
    "    DATA_SET = {\n",
    "        'NLPNameDataset': NLPNameDataset, # 이름 -> 국가\n",
    "        'dataset2': 'class'\n",
    "    }\n",
    "    MODEL_SET = {\n",
    "        'RNNManual': RNNManual,\n",
    "        'LSTMManual': LSTMManual,\n",
    "        'RNNBuiltIn': RNNBuiltIn,\n",
    "        'LSTMBuiltIn': LSTMBuiltIn,\n",
    "        'GRUManual': GRUManual,\n",
    "        'GRUBuiltIn': GRUBuiltIn,\n",
    "    }\n",
    "    LOSSFUNCTION = {\n",
    "        'MSELoss': nn.MSELoss(),  # 평균 제곱 오차 손실\n",
    "        'BCELoss': nn.BCELoss(),  # 이진 교차 엔트로피 손실\n",
    "        'CrossEntropyLoss': nn.CrossEntropyLoss(),  # 교차 엔트로피 손실\n",
    "        'CrossEntropyLoss_mk2': lambda x, y: torch.nn.functional.cross_entropy(x, y.long()),  # 교차 엔트로피 손실\n",
    "        'L1Loss': nn.L1Loss(),  # 평균 절대 오차 손실\n",
    "    }\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    # Generate dataset\n",
    "    data_loader = DATA_SET[dataset](data_path=DATA_PATH, batch_size=BATCH_SIZE, shuffle=SHUFFLE, valid_ratio=VALID_RATIO, test_ratio=TEST_RATIO, \n",
    "                 num_workers=NUM_WORKERS, ran_split=ran_split)\n",
    "    train_loader = data_loader.train_dataloader\n",
    "    val_loader = data_loader.valid_dataloader\n",
    "    test_loader = data_loader.test_dataloader\n",
    "    print('dataloader생성 분할 직후')\n",
    "    # Define loss function\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = LOSSFUNCTION['CrossEntropyLoss_mk2']\n",
    "    \n",
    "    # Initialize loss tracking\n",
    "    loss_history = {  # 손실기록 초기화\n",
    "    }\n",
    "    \"\"\"EX = {\n",
    "        'RNNManual': {'train': [], 'val': []},\n",
    "        'LSTMManual': {'train': [], 'val': []},\n",
    "        'GRUManual': {'train': [], 'val': []},\n",
    "        'RNNBuiltIn': {'train': [], 'val': []},\n",
    "        'LSTMBuiltIn': {'train': [], 'val': []},\n",
    "        'GRUBuiltIn': {'train': [], 'val': []}, }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize models\n",
    "    print('문자여 모델 순회하며 변수 초기화 하기 전')\n",
    "    for model in models: # 문자열 모델명 순회\n",
    "        for key, value in model_config[model].items():\n",
    "            print(model, key, value)\n",
    "            OUTPUT_DIM = all_targets = len(data_loader.all_categories)\n",
    "            INPUT_DIM = len(data_loader.alphabets)\n",
    "            # model_config 변수초기화\n",
    "            globals()[key] = value\n",
    "        # rnnmanual = RNNManual().to(device)\n",
    "        globals()[model.lower()] = MODEL_SET[model]().to(device)\n",
    "        print(f'{model.lower()}= {model}로 모델 초기화')\n",
    "    print('변수명에 동적할당전')\n",
    "    # Define optimizers\n",
    "    for model in models: # 변수명에 동적 할당?\n",
    "        model_var_name = model.lower()\n",
    "        training_config['optimizers'][model_var_name] = optim.Adam(globals()[model_var_name].parameters(), lr=LEARNING_RATE)\n",
    "    # optimizers = training_config['optimizer'] = {\n",
    "    #     'RNNManual': optim.Adam(RNNManual.parameters(), lr = LEARNING_RATE),\n",
    "    #     'LSTMManual': optim.Adam(LSTMManual.parameters(), lr = LEARNING_RATE),\n",
    "    #     'GRUManual': optim.Adam(GRUManual.parameters(), lr = LEARNING_RATE),\n",
    "    #     'RNNBuiltIn': optim.Adam(RNNBuiltIn.parameters(), lr = LEARNING_RATE),\n",
    "    #     'LSTMBuiltIn': optim.Adam(LSTMBuiltIn.parameters(), lr = LEARNING_RATE),\n",
    "    #     'GRUBuiltIn': optim.Adam(GRUBuiltIn.parameters(), lr = LEARNING_RATE),\n",
    "    # } # 직접 명시하는게 더 나을?나은게? 맞...나..\n",
    "    print('에폭스 시작전')\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
    "        # Train RNN Manual\n",
    "        # Train LSTM Manual\n",
    "        # Train RNN Built-In\n",
    "        # Train LSTM Built-In\n",
    "        # Train GRU Built-In\n",
    "        # Train GRU Built-In\n",
    "        print('모델 실행전')\n",
    "        for model in models:\n",
    "            print(model,'해당모델 실행')\n",
    "            current_model = globals()[model.lower()] # 초기화된 모델인스턴스 호출\n",
    "            optimizer = optimizers[model.lower()]\n",
    "            print(model, criterion , optimizer)\n",
    "            print('INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM:', INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "            train_loss = train_model(current_model, train_loader, criterion, optimizer)\n",
    "            val_loss, val_accuracy = evaluate_model(current_model, val_loader, criterion)\n",
    "        \n",
    "            if model not in loss_history:\n",
    "                loss_history[model] = {'train': {'average_loss': []},\n",
    "                                       'val': {'average_loss':[], 'accuracy': []}}\n",
    "        \n",
    "            loss_history[model]['train']['average_loss'].append(train_loss)\n",
    "            loss_history[model]['val']['average_loss'].append(val_loss)\n",
    "            loss_history[model]['val']['accuracy'].append(val_accuracy)\n",
    "            print(f'| {model} | Train Loss: {train_loss:.4f}\\n| Val Loss: {val_loss:.4f} | Val accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Display final validation losses\n",
    "    print(\"\\nFinal Validation Losses:\")\n",
    "    for model_name in loss_history:\n",
    "        final_val_loss = loss_history[model_name]['val'][-1]\n",
    "        print(f'{model_name}: {final_val_loss:.4f}')\n",
    "        plt.plot(range(1, len(loss_history[model_name]['val'])+1), loss_history[model_name]['val'])\n",
    "        plt.show()\n",
    "    \n",
    "    # Plotting the loss curves\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    for model_name in loss_history:\n",
    "        plt.plot(loss_history[model_name]['train']['average_loss'], label=f'{model_name} Train')\n",
    "        plt.plot(loss_history[model_name]['val']['average_loss'], label=f'{model_name} Val')\n",
    "        plt.plot(loss_history[model_name]['val']['accuracy'], label=f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses for RNN, LSTM, and GRU (Manual and Built-In)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메모순서\n",
    "- FNN RNN GRU 어탠션 트랜스포머\n",
    "\n",
    "- from torch.utils.data import random_split\n",
    "- y값이 어쩌고 일때 쓰면 안된다 성능에 영향 꽤 준다\n",
    "\n",
    "- various loss functions\n",
    "  - 회귀의 경우\n",
    "      1. MSELoss\n",
    "  - 분류의 경우\n",
    "      1. (Raw) Logit\n",
    "      2. Softmax / Log Softmax의 결과값\n",
    "      3. 객관식 정답 자체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([[ 0.3558, -0.3845],\n",
      "        [ 1.2965, -0.3279],\n",
      "        [ 1.0103, -0.2987],\n",
      "        [ 0.3558, -0.3845],\n",
      "        [ 1.0103, -0.2987]], grad_fn=<EmbeddingBackward0>) torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "embedd = torch.nn.Embedding( 3, 2)\n",
    "res = embedd(torch.tensor([1,0, 2,1 ,2]))\n",
    "\n",
    "print(torch.tensor([1,0, 2,1 ,2]).shape)\n",
    "print(res , res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Model):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        h_t = torch.tanh(self.i2h(x) + self.h2h(h_prev)) # x : batch_size, i # h_prev : b, h # self.i2h(x) : b, i # self.h2h(h_prev) : b, h\n",
    "        o_t = self.h2o(h_t)\n",
    "        return h_t\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(hidden_size)\n",
    "# rnn\n",
    "k=i\n",
    "l(t).shape = (*s, o)\n",
    "o=calss=2\n",
    "l(t)\n",
    "1. l(t)가 계산 가능한 조건\n",
    "2. l(t).shape\n",
    "t가 입력데이터\n",
    "l 이 리니어 o가 ouput이면 \n",
    "행렬곱을 거쳐 나와야하니\n",
    "shape는 (배치, k=i )\n",
    "아웃풋은 o 가 되고 \n",
    "\n",
    "shape = (4,3)\n",
    "t = torch.randn((100, 1), i)\n",
    "l = nn.Linear(k, o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
