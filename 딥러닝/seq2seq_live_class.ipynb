{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJG2_Md3gSM_",
        "outputId": "1bbc3942-7b62-4197-8dc1-0f926861c393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.5013, Accuracy: 24.93%\n",
            "Epoch [2/20], Loss: 1.4113, Accuracy: 26.85%\n",
            "Epoch [3/20], Loss: 1.3915, Accuracy: 29.99%\n",
            "Epoch [4/20], Loss: 1.3780, Accuracy: 32.15%\n",
            "Epoch [5/20], Loss: 1.3653, Accuracy: 35.10%\n",
            "Epoch [6/20], Loss: 1.3494, Accuracy: 40.28%\n",
            "Epoch [7/20], Loss: 1.3322, Accuracy: 42.46%\n",
            "Epoch [8/20], Loss: 1.3142, Accuracy: 44.23%\n",
            "Epoch [9/20], Loss: 1.2969, Accuracy: 45.42%\n",
            "Epoch [10/20], Loss: 1.2823, Accuracy: 46.66%\n",
            "Epoch [11/20], Loss: 1.2688, Accuracy: 47.96%\n",
            "Epoch [12/20], Loss: 1.2562, Accuracy: 48.78%\n",
            "Epoch [13/20], Loss: 1.2455, Accuracy: 48.81%\n",
            "Epoch [14/20], Loss: 1.2340, Accuracy: 49.80%\n",
            "Epoch [15/20], Loss: 1.2219, Accuracy: 50.47%\n",
            "Epoch [16/20], Loss: 1.2112, Accuracy: 50.44%\n",
            "Epoch [17/20], Loss: 1.2015, Accuracy: 50.55%\n",
            "Epoch [18/20], Loss: 1.1911, Accuracy: 51.23%\n",
            "Epoch [19/20], Loss: 1.1796, Accuracy: 51.89%\n",
            "Epoch [20/20], Loss: 1.1708, Accuracy: 52.49%\n",
            "Training Accuracy: 52.57%\n",
            "\n",
            "Sample Input Sequence:    [1, 1, 1, 2, 3, 1, 3, 1, 3, 1]\n",
            "Sample Target Sequence:   [1, 1, 1, 2, 3, 1, 3, 1, 3, 1]\n",
            "Predicted Sequence       :   [1, 1, 3, 3, 3, 1, 3, 1, 3, 1]\n",
            "tensor([1, 1, 1, 2, 3, 1, 3, 1, 3, 1]) tensor([1, 1, 1, 2, 3, 1, 3, 1, 3, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import json\n",
        "\n",
        "def generate_dataset(seq_length, num_samples, vocab_size): # vocab_size:\n",
        "    inputs = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
        "    outputs = inputs.clone()\n",
        "    return TensorDataset(inputs, outputs)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.linear = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        batch_size, seq_length = input_seq.size() # batch_size, seq_elngth\n",
        "        hidden = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "\n",
        "        for char_idx in range(seq_length):\n",
        "            x_t = nn.functional.one_hot(input_seq[:, char_idx], num_classes = self.linear.in_features).float()\n",
        "            hidden = self.activation(self.lineaㄴr(x_t) + hidden)\n",
        "        return hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # self.i2h = nn.Linear(input_size, hidden_size) # input -> hidden\n",
        "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, target_seq, hidden):\n",
        "        batch_size, seq_len = target_seq.size()\n",
        "        outputs = torch.zeros(batch_size, seq_len, self.output_size).to(device)\n",
        "\n",
        "        for char_idx in range(seq_len):\n",
        "            if char_idx == 0:\n",
        "                previous_y = torch.zeros(batch_size, self.input_size).to(device)\n",
        "            else:\n",
        "                y_prev = target_seq[:, char_idx - 1]\n",
        "                previous_y = nn.functional.one_hot(y_prev, self.input_size).to(device).float()\n",
        "            hidden = self.activation(self.linear1(previous_y) + hidden)\n",
        "            output = self.linear2(hidden)\n",
        "            outputs[:, char_idx, :] = output\n",
        "        return outputs\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, target_seq):\n",
        "        encoder_hidden = self.encoder(input_seq)\n",
        "        decoder_output = self.decoder(target_seq, encoder_hidden)\n",
        "        return decoder_output\n",
        "\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "    training_stats = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_accuracy': []\n",
        "    }\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, targets in dataloader:\n",
        "            # inputs.shape - batch_size, sequence_length\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs, targets)\n",
        "            outputs = outputs.view(-1, outputs.size(-1)) # batch_size * seq_length, output_size\n",
        "            targets = targets.view(-1) # batch_size * seq_len\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            # Accuracy 계산\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "            \n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        accuracy = correct / total\n",
        "        \n",
        "        # 성능 기록\n",
        "        training_stats['epoch'].append(epoch)\n",
        "        training_stats['train_loss'].append(avg_loss)\n",
        "        training_stats['train_accuracy'].append(accuracy)\n",
        "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # 학습 성능을 JSON 파일로 저장\n",
        "    with open('training_stats.json', 'w') as f:\n",
        "        json.dump(training_stats, f)\n",
        "    \n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"모델 평가 후 정확도 반환\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs, targets) # batch_size, seq_length, vocab_size\n",
        "\n",
        "            predicted = torch.argmax(outputs, dim = 2)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0) * targets.size(1)\n",
        "            \n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seq_length = 10\n",
        "    num_samples = 1000\n",
        "    vocab_size = 5 # Including a padding index if needed\n",
        "    hidden_size = 64\n",
        "    batch_size = 32\n",
        "    num_epochs = 20\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # print(f\"Using device: {device}\")\n",
        "\n",
        "    dataset = generate_dataset(seq_length, num_samples, vocab_size)\n",
        "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "    encoder = Encoder(input_size = vocab_size, hidden_size = hidden_size)\n",
        "    decoder = Decoder(input_size = vocab_size, hidden_size = hidden_size, output_size = vocab_size)\n",
        "    model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    train_model(model, dataloader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "    acc = evaluate_model(model, dataloader, device)\n",
        "    print(f\"Training Accuracy: {acc * 100:.2f}%\\n\")\n",
        "\n",
        "    # 모델 평가 결과도 저장\n",
        "    with open('evaluation_result.json', 'w') as f:\n",
        "        json.dump({'test_accuracy': acc}, f)\n",
        "    \n",
        "    # 테스트 샘플 예측 결과 확인\n",
        "    with torch.no_grad():\n",
        "        test_input, test_target = dataset[0]\n",
        "        test_input = test_input.unsqueeze(0).to(device)\n",
        "        test_target = test_target.unsqueeze(0).to(device)\n",
        "\n",
        "        output = model(test_input, test_target)\n",
        "        predicted = torch.argmax(output, dim = 2)\n",
        "        print(\"Sample Input Sequence:   \", test_input.squeeze().tolist())\n",
        "        print(\"Sample Target Sequence:  \", test_target.squeeze().tolist())\n",
        "        print(\"Predicted Sequence       :  \", predicted.squeeze().tolist())\n",
        "\n",
        "    for x, y in dataset:\n",
        "        print(x, y)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "t = torch.randn(3, 5)\n",
        "print(t)\n",
        "print(t>0)\n",
        "print(torch.sum(t > 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "RIfu2s3QmVs_",
        "outputId": "66fa87a7-227a-43e2-c213-e279635c70bd"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 7]' is invalid for input of size 150",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(input_seq)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(input_seq.shape)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(input_seq[:, 3])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# x_t = nn.functional.one_hot(input_seq[:, char_idx],\u001b[39;00m\n\u001b[0;32m     15\u001b[0m               \u001b[38;5;66;03m# num_classes = self.linear.in_features).float()\u001b[39;00m\n\u001b[0;32m     17\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 7]' is invalid for input of size 150"
          ]
        }
      ],
      "source": [
        "input_sequence = [torch.randint(1, 4, (5,)) for _ in range(4)]\n",
        "input_seq = torch.stack(input_sequence)\n",
        "\n",
        "# print(input_seq)\n",
        "# print(input_seq.shape)\n",
        "# print(input_seq[:, 3])\n",
        "# one_hot = nn.functional.one_hot(input_seq[:, 3], 3)\n",
        "# print(one_hot.shape)\n",
        "# print(one_hot)\n",
        "# 1 2 3\n",
        "# [1, 0, 0]\n",
        "# [0, 1, 0]\n",
        "# [0, 0, 1]\n",
        "# x_t = nn.functional.one_hot(input_seq[:, char_idx],\n",
        "              # num_classes = self.linear.in_features).float()\n",
        "\n",
        "t = torch.randn(10, 3, 5)\n",
        "print(t.view(-1, 7).shape)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
